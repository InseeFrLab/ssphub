---
title: Infolettre n¬∞15
description: |
  Infolettre de rentr√©e, __Septembre 2023__

# Date published
date: '2023-09-10'
number: 15

image: learning.png

authors:
  - Lino Galiana

tags:
  - infolettre

categories:
  - Infolettre
---

![](learning.png){width=50% fig-align="center"}

:::{.callout-tip}

__*Vous d√©sirez int√©grer la liste de diffusion ? L'inscription se fait [ici](https://framaforms.org/integration-reseau-des-data-scientists-1676407156).*__

:::


C'est la rentr√©e ! Comme les √©l√®ves
qui reviennent sur les bancs des √©coles,
les mod√®les de _machine learning_ ont p√©riodiquement
besoin de mettre √† jour leurs connaissances.

Cette _newsletter_ sera consacr√©e aux enjeux
du r√©-entrainement et de la sp√©cialisation
de mod√®les, une question
d'actualit√© suite √† la publication estivale de 
plusieurs grands mod√®les de langage (LLM) _open source_.


::: {.callout-note}
La premi√®re partie de cette _newsletter_
se concentrera sur les
enjeux principaux. La suite sera plus technique 
et √©voquera plus en d√©tail certains mod√®les
et les m√©thodes de r√©-entrainement.  
:::

# Enjeux associ√©s au r√©entrainement des mod√®les de langage

## Un entra√Ænement _ex nihilo_ hors de port√©e

Si de nombreuses t√¢ches de mod√©lisation ne n√©cessitent pas
des mod√®les tr√®s sophistiqu√©s, deux domaines de
recherche - √† savoir le traitement naturel du langage (NLP) et l'analyse d'image -
ont connu ces derni√®res ann√©es des innovations
importantes gr√¢ce
√† des [r√©seaux de neurones](https://www.cnil.fr/fr/definition/reseau-de-neurones-artificiels-artificial-neural-network) √† 
l'architecture de plus en plus complexe. 

Pour √™tre en mesure d'entra√Æner
un mod√®le complexe, du type [grand mod√®le de langage (LLM)](https://fr.wikipedia.org/wiki/Grand_mod%C3%A8le_de_langage), il faut, _a minima_,
disposer des intrants suivants:

- Un __immense volume de donn√©es d√©structur√©es__. La constitution de
ces corpus implique le moissonnage en masse
de ressources en ligne, ce qui n'est pas sans poser des
[enjeux juridiques de propri√©t√© intellectuelle](https://www.deeplearning.ai/the-batch/time-to-update-copyright-for-generative-ai/) qui ne sont pas encore r√©solus. 
La r√©cup√©ration de ces donn√©es n√©cessite
une bonne connaissance de la 
structure et la nature des donn√©es n√©cessaires pour entrainer un mod√®le ;
- Des __ressources informatiques hors du commun__. Les investissements importants
pour les cartes graphiques (GPU), une [mati√®re premi√®re en p√©nurie](https://www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.html) et les
co√ªts courants associ√©s (√©lectricit√©, maintenance...) font qu'une poign√©e d'acteurs du num√©rique
disposent des moyens financiers ad√©quats pour entra√Æner un mod√®le _ex nihilo_.
Cet article de [Forbes](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/) 
√©voque des montants de l'ordre de plusieurs dizaines voire centaines de millions de dollars.
- Des __experts__ aux comp√©tences √† l'intersection entre la recherche en math√©matique et informatique
ainsi que des sp√©cialistes en _data engineering_ actuellement en p√©nurie sur le march√© du travail. 

La combinaison de ces facteurs rend difficile, si ce n'est impossible,
l'entrainement _ex nihilo_ de tels mod√®les par la majorit√©
des acteurs de la donn√©e. Seule une poign√©e de centres de recherche
diposent des ressources permettant d'entra√Æner _ex nihilo_ ce type de mod√®les.


## La r√©utilisation en pratique

N√©anmoins, les besoins d'utilisation de ces mod√®les d√©passent le cercle
des acteurs en mesure de les entra√Æner. 
En effet, les grands mod√®les de langage sont g√©n√©ralement entra√Æn√©s
sur des corpus g√©n√©riques de langage naturel principalement issus d'internet (@tbl-corpus-falcon).
Cela les rend capables de comprendre les interactions avec des utilisateurs, notamment leurs instructions (_prompt_)
et d'interagir avec eux de mani√®re assez naturelle.

N√©anmoins, pour des t√¢ches tr√®s sp√©cialis√©es ou alors face √† des corpus
particuliers, ces mod√®les g√©n√©riques peuvent n√©cessiter d'√™tre sp√©cialis√©s pour obtenir de meilleures performances.
Plusieurs types de techniques, de [complexit√© graduelle](https://www.deeplearning.ai/the-batch/tips-for-taking-advantage-of-open-large-language-models/),
ont ainsi √©merg√© pour √™tre en mesure de r√©utiliser et am√©liorer
un mod√®le pr√©-entra√Æn√©.


Un frein √† la r√©utilisation massive de mod√®les pr√©-entra√Æn√©s est
la nature propri√©taire de certains mod√®les, dont les conditions
de r√©utilisation peuvent √™tre limitantes. Pour
cette raison, l'√©mergence de mod√®les _open source_, dont la
structure est plus transparente
et dont les conditions de r√©utilisation
sur des infrastructures internes
sont moins restrictives, est devenu ces derniers mois
un enjeu important dans l'√©cosyst√®me de la donn√©e. 

Les discussions sur l'ouverture des mod√®les s'inscrivent dans le contexte d'un affrontement important entre deux visions
du mod√®le
√©conomique du secteur num√©rique : si le co-cr√©ateur d'`OpenAI` 
a pu affirmer, pour justifier l'absence de transparence scientifique sur les mod√®les d'`OpenAI` ["_[on openly sharing research,] we were wrong_"](https://www.theverge.com/2023/3/15/23640180/openai-gpt-4-launch-closed-research-ilya-sutskever-interview),
un [m√©mo interne de Google](https://www.theverge.com/2023/7/10/23790132/google-memo-moat-ai-leak-demis-hassabis)
d√©fendait quant √† lui l'id√©e que les mod√®les _open source_ sont 
amen√©s √† prendre le dessus, car ils
peuvent b√©n√©ficier √† plus grande √©chelle du travail d'experts et de retours d'utilisateurs.

La publication cet √©t√© de deux
mod√®les _open source_ ([`LLaMA-2`](https://ai.meta.com/llama/) par Meta et [`Falcon`](https://falconllm.tii.ae/) par l'Institut de l'innovation technologique d'Abu Dhabi)
ouvre de nouvelles perspectives pour une r√©utilisation
de mod√®les dans une infrastructure interne, √† condition de
disposer des ressources computationnelles suffisantes
et d'une strat√©gie adapt√©e de r√©-apprentissage. 

Pour aller plus loin sur ce sujet, la suite de cette _newsletter_
√©voque des d√©tails plus techniques.
La _masterclass_ sur le
sujet du _fine-tuning_ que nous organisons avec `datascientest` ([plus d'informations üëáÔ∏è](#mc-datascientest))
permettra √©galement d'approfondir cette question. 

::: {.callout-note collapse="true"}
## D√©rouler pour en savoir plus sur le corpus d'entra√Ænement de `Falcon`

: _[Corpus d'entra√Ænement de Falcon 180B](https://huggingface.co/tiiuae/falcon-180B)_{#tbl-corpus-falcon}

  | Source de donn√©es | Proportion dans le corpus 
  |-----------|------------|
  | `RefinedWeb-English` (_webscraping_)	| 75%	|
  | `RefinedWeb-Europe`	(_webscraping_) | 7%	|
  | Livres |	6%	|	
  | Sites de conversations (`Reddit`, `StackOverflow`, `HackerNews`...) |	5%	|
  | Code (`Github`...)	| 5% |
  | Documents techniques (`arXiv`, `PubMed`...) |	2%	| 

:::


# Actualit√©s des mod√®les de langage

## De nouveaux grands mod√®les de langage (LLM)

Pas de vacances pour les principaux acteurs de la _data science_ !
Ce champ de recherche appliqu√©e continue √† conna√Ætre
une actualit√© dense avec la publication, cet √©t√©, 
de deux mod√®les importants:

- [`LLaMA-2`](https://ai.meta.com/llama/) par Meta, disponible en versions 7B, 13B et 70B c'est-√†-dire, respectivement, 7, 13 et 70 milliards de param√®tres. Dans le domaine des LLM actuels, c'est donc un mod√®le plut√¥t minimaliste (`GPT-3` comportait 175 milliards de param√®tres, `GPT-4` en comporterait 1.7 trillions soit 1700 milliards). Le site web `LeBonLLM` propose, d√©j√†, des exemples de [_fine tuning_ de `LLaMa`](https://www.lebonllm.fr/entrainer-son-llm-avec-llama-1-et-llama-2/)
 ;
- [`Falcon`](https://falconllm.tii.ae/) par l'Institut d'Innovation et de Technologie d'Abu Dhabi. [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) avait d√©j√† connu, cet √©t√©, un engouement important en se pla√ßant en t√™te des r√©utilisations sur `HuggingFace`. [Falcon 180B](https://huggingface.co/tiiuae/falcon-180B), sorti il y a quelques jours, rapproche les performances de celles des mod√®les propri√©taires.

Ces deux mod√®les permettent d'envisager des r√©utilisations apr√®s un r√©-entrainement sur des jeux de donn√©es _ad hoc_ pour am√©liorer leurs performances (technique du _fine tuning_). 
Ceci est possible gr√¢ce √† leurs licences permissives de r√©utilisation. Celle de [`Falcon`](https://falconllm.tii.ae/)
est assez standard puisqu'il s'agit d'une [Apache 2.0](https://fr.wikipedia.org/wiki/Licence_Apache). 
Celle de [`LLaMA-2`](https://ai.meta.com/llama/) est quant √† elle moins traditionnelle. 
La r√©utilisation est libre,
y compris √† des fins commerciale, sauf pour les gros acteurs du num√©rique, globalement les concurrents
de `Meta` :

> 2. Additional Commercial Terms. If, on the Llama 2 version release date, the 
monthly active users of the products or services made available by or for Licensee, 
or Licensee's affiliates, is greater than 700 million monthly active users in the 
preceding calendar month, you must request a license from Meta, which Meta may 
grant to you in its sole discretion, and you are not authorized to exercise any of the 
rights under this Agreement unless or until Meta otherwise expressly grants you 
such rights.
>
> [Licence de LLaMa-2 sur `Github`](https://github.com/facebookresearch/llama/blob/d7e2e37e163981fd674ea2a633fac2014550898d/LICENSE#L65-L71)



## Le r√©entrainement des mod√®les

L'ouverture de ces mod√®les laisse envisager des r√©utilisations
sur de nouveaux jeux de donn√©es dans des infrastructures internes. 
Cet √©t√©, [Andrew Ng](https://www.deeplearning.ai/the-batch/tips-for-taking-advantage-of-open-large-language-models/), 
dans sa _newsletter_,
est revenu sur les m√©thodes pour affiner les performances d'un mod√®le
sur des donn√©es qu'il n'a pas rencontr√©es dans son corpus d'entra√Ænement. 

- La technique la plus simple est d'__affiner les instructions__ (_prompt_) fournies √†
un mod√®le. Pour faire l'analogie avec l'apprentissage humain, pour obtenir une r√©ponse
mieux cibl√©e √† une question, il est souvent n√©cessaire de reformuler une question. 
Par exemple, lors d'une interaction avec une IA assistante de code,
il peut √™tre utile de guider un LLM avec une instruction
_"as a data scientist"_ ;
- __Fournir quelques exemples √† un mod√®le__ (_few shot learning_). De m√™me qu'avec les humains, fournir
un petit nombre d'exemples peut suffire √† un mod√®le, par un raisonnement inductif, 
√† comprendre et r√©pondre de mani√®re juste √† son instructeur. 
Selon la difficult√© de la t√¢che √† 
mettre en oeuvre, il peut suffire de tr√®s peu de cas pour sp√©cialiser un mod√®le en modifiant les derni√®res
couches du r√©seau de neurone. 
- __Fournir de nouvelles sources √† un mod√®le avant de l'interroger__ (_retrieval augmented generation_). 
Cette technique consiste √† enrichir la base de connaissance d'un mod√®le pr√©-entra√Æn√© avec un nouveau corpus
puis l'interroger sur la m√™me th√©matique afin, par exemple, d'obtenir une synth√®se
ou alors une information contenue dans les documents
mais n√©cessitant un temps d'extraction non n√©gligeable √† un humain. Dans cette approche,
il ne s'agit pas de r√©entrainer le mod√®le pour mettre √† jour ses param√®tres mais de lui donner plus
de contexte pour am√©liorer la pertinence des pr√©dictions ou r√©ponses du mod√®le. Pour continuer l'analogie
avec l'apprentissage humain, cette technique se rapproche d'une situation o√π un humain assimile
une bibliographie pour rentrer dans un nouveau sujet. 
- __R√©entrainement par sp√©cialisation__ (_fine tuning_) pour affiner le mod√®le sur une t√¢che donn√©e. 
Il s'agit d'une approche qui peut ressembler √† l'apprentissage d'une nouvelle langue pour un humain: 
√† partir d'un certain stock de connaissances ant√©rieures (une langue natale), on accumule
en s√©rie des exemples bien choisis pour am√©liorer la compr√©hension d'une autre langue. 
Cette approche permet une √©conomie de ressources puisqu'elle consiste √† sp√©cialiser un mod√®le
g√©n√©raliste mais n√©cessite que la nature du probl√®me pour lequel est r√©-entrain√© un mod√®le
ressemble √† celle pour lequel le mod√®le a √©t√© entra√Æn√©. De m√™me qu'essayer de transposer des r√®gles
d'une langue latine aidera peu √† apprendre le japonais, sp√©cialiser un mod√®le d'analyse
d'image pour une t√¢che de classification de donn√©es textuelles sera inefficace. 

Le _fine tuning_ est ainsi une solution int√©ressante √† condition d'avoir test√© si des
approches plus simples n'apportent pas d√©j√† des solutions satisfaisantes.

De plus, pour √™tre en mesure de _fine tuner_ un mod√®le, outre 
l'acc√®s √† des ressources computationnelles
cons√©quentes (mais tout de m√™me moindres qu'un entra√Ænement _ex nihilo_),
beaucoup de m√©thodes n√©cessitent de disposer 
de donn√©es labellis√©es, c'est-√†-dire impliquent de poss√©der une base de donn√©es 
permettant de juger de la qualit√© des pr√©dictions du mod√®le.
Pour pallier cette absence, il est possible
de mettre en oeuvre un processus humain d'annotation et fournir au mod√®le ces √©valuations pour l'amener
√† s'am√©liorer (technique nomm√©e **_[reinforcement learning from human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)_**).

Comme l'√©voque encore Andrew Ng, l'acc√®s √† des mod√®les
pr√©-entra√Æn√©s change le [cycle de d√©veloppement des projets utilisateurs d'IA](https://www.deeplearning.ai/the-batch/building-machine-learning-systems-is-more-debugging-than-development/).
Ces projets ne consistent plus, comme les logiciels classiques,
√† d√©velopper en amont des sp√©cifications puis d√©ployer un mod√®le correspondant √† celles-ci
mais, au contraire, √† commencer par mettre en oeuvre rapidement un premier mod√®le dont le comportement
sera √©valu√© et am√©lior√© en continu par des m√©thodes comme l'apprentissage par renforcement. 


# Le reste de l'actualit√©

- La commission d'acc√®s aux documents administratifs (CADA) a rendu un [avis](https://www.cada.fr/20230314)
sur le sujet de la publication en _open source_ des mod√®les d'apprentissage entra√Æn√©s par l'administration fran√ßaise ;
- Une association fran√ßaise a √©merg√© pour rassembler des ressources et offrir un espace communautaire autour des LLM
francophones: [Le Bon LLM](https://www.lebonllm.fr/) ;
- La qualit√© des r√©ponses de `ChatGPT` pourrait s'√™tre [d√©grad√©e](https://www.deeplearning.ai/the-batch/chatgpts-behavior-change-over-time/) au cours du temps ; 
- Le _Financial Times_ pr√©sente de mani√®re tr√®s p√©dagogique le [fonctionnement des LLM](https://ig.ft.com/generative-ai/) ;
- Meta publie [`Nougat`](https://facebookresearch.github.io/nougat/), un mod√®le tr√®s efficace de num√©risation de PDF ;
- Le risque de disparition 
d'emplois d'acteurs et sc√©naristes du fait des IA g√©n√©ratrices est l'un des
facteurs √† l'origine de la gr√®ve √† Hollywood ([voir _Le Monde_](https://www.lemonde.fr/economie/article/2023/07/14/greve-a-hollywood-les-acteurs-craignent-d-etre-remplaces-par-des-machines_6181893_3234.html)) ;
- Le mod√®le _open source_ de g√©n√©ration d'images `StableDiffusion` sort une [version XL (`sdxl`)](https://stability.ai/stable-diffusion)  
am√©liorant encore la qualit√© des productions (interface [ici](https://clipdrop.co/fr/stable-diffusion));
- OpenAI int√®gre une nouvelle version de son g√©n√©rateur d‚Äôimage (`Dall-E 3`) dans `ChatGPT Plus` ([annonce](https://openai.com/dall-e-3)) ;
- La derni√®re revue de presse de la [Gazette BlueHats #24](https://code.gouv.fr/fr/bluehats/bluehats_24/#revue-de-presse) recense de nombreux articles sur l'IA ;
- Un [site interactif](https://unehistoireduconflitpolitique.fr/) riche de nombreuses data visualisations est associ√© au nouvel ouvrage de Thomas Piketty et Julia Cag√©, _Une histoire du conflit politique_.

# √âv√©nements

## Masterclass _datascientest_ sur le _fine-tuning_ {#mc-datascientest}

![](../infolettre_11/datascientest.png){width=20% fig-align="center"}


Notre cycle de _masterclass_ organis√©es en lien avec _datascientest_
continue ! 

Apr√®s avoir explor√© en d√©tail les th√©matiques du traitement
automatique du langage et de l'analyse d'image, nous progressons
dans notre parcours avec le sujet du _fine tuning_. 

Au programme: 

- R√©utilisation de _transformers_ (`BERT`) et de LLM (`LLaMA`)
avec les librairies d'`HuggingFace` ;
- _Fine tuning_ de ces mod√®les.

__Rendez-vous le 28 septembre de 10h √† 12h__ !
[Inscription ici](https://framaforms.org/inscription-aux-masterclass-datascientest-1695194241)

## Autres √©v√©nements

Quelques √©v√©nements ou informations int√©ressantes :

- [Hackathon velib](https://blog.velib-metropole.fr/hackathon/?utm_source=sendgrid.com&utm_medium=email&utm_campaign=website), fermeture des inscriptions le 29 septembre ;
- [Hackathon de l'ONU](https://unstats.un.org/bigdata/events/2023/un-datathon/): fermeture des inscriptions √† la fin du mois ;
- [Prix du jeune statisticien de l'IAOS](https://mailings.isi-web.org/?na=v&nk=15226-43b0d8c9de&id=563) : article √† envoyer avant le 10 f√©vrier 2024

Les personnes int√©ress√©es par former une √©quipe pour les _hackathons_ peuvent contacter <ssphub-contact@insee.fr>. 

## Rejoindre le salon Tchap `SSP Hub`

Pour √©changer autour des activit√©s du r√©seau et, plus largement, discuter entre pairs des sujets _data science_,
il existe un __salon `SSP Hub`__ dans la messagerie s√©curis√©e
de l‚Äô√âtat `Tchap`.
Celui-ci r√©unit plus de 250 personnes et permet des √©changes plus directs, plus fr√©quents et plus informels que la liste de diffusion mail.

Si vous avez un compte sur `Tchap`, vous pouvez rejoindre ce salon en cherchant celui-ci par son nom __`¬´ SSP Hub ¬ª`__. 
En cas de probl√®me pour le rejoindre, n‚Äôh√©sitez pas √† envoyer un mail √† <ssphub-contact@insee.fr>.