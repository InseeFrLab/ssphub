---
title: "Le plongement lexical, ou comment apprendre √† lire √† un ordinateur"
author: Lucas Malherbe

# Summary for listings and search engines
description: |
  Introduction aux m√©thodes de traitement du langage naturel


# Date published
date: '2022-09-19T00:00:00Z'

# Featured image
image: featured.jpg


categories:
  - Insee
  - NLP
---


Avec le d√©veloppement de la collecte automatis√©e d'information num√©rique, les donn√©es textuelles sont devenues omnipr√©sentes, que ce soit sous la forme d‚Äôe-mails, de r√©ponses √† des enqu√™tes, d'articles de presse ou encore de commentaires sur les r√©seaux sociaux. 
Ces donn√©es peuvent √™tre une source tr√®s riche d‚Äôinformations mobilisable par les statisticiens, pour peu qu'ils parviennent √† en faire un traitement statistique. Ainsi, une probl√©matique r√©currente dans la statistique publique consiste √† classer des informations formul√©es en langage courant (professions, noms de produits, noms de communes, etc.) dans des nomenclatures
standardis√©es (PCS[^1], NAF[^2], COG[^cog]...). 

[^1]: La nomenclature `PCS` (professions et cat√©gories socioprofessionnelles)
sert √† la codification des professions dans le recensement et les enqu√™tes aupr√®s des m√©nages. 
Elle permet ainsi de classer un ensemble de professions dans une m√™me cat√©gorie. Par exemple,
dans sa derni√®re version ([PCS 2020](https://www.insee.fr/fr/information/6205305)),
la cat√©gorie des _"Professions lib√©rales de sant√©"_ (31A) regroupe diverses professions m√©dicales: m√©decins lib√©raux, dentistes, psychologues, v√©terinaires, pharmaciens lib√©raux... Une description plus
compl√®te de cette nomenclature et de son historique est disponible sur [le site de l'Insee](https://www.insee.fr/fr/information/6208292)

[^2]: La `NAF` (nomenclature d'activit√©s fran√ßaise), est une nomenclature des activit√©s √©conomiques productives,
principalement √©labor√©e pour faciliter l'organisation de l'information √©conomique et sociale. Il s'agit d'une
typologie facilitant la repr√©sentation de l'√©conomie sous forme de secteurs. Par exemple, au sein de l'industrie
manufacturi√®re (section C), la NAF distingue les industries alimentaires de l'industrie de l'habillement
ou de l'industrie automobile.
Une description plus
compl√®te de cette nomenclature et de son historique est disponible sur [le site de l'Insee](https://www.insee.fr/fr/information/2406147)

[^cog]: Le Code Officiel G√©ographique est le r√©f√©rentiel permettant de relier des adresses, des noms de communes ou encore des noms de collectivit√©s locales √† un identifiant unique. Pour plus d'informations, voir le [site de l'Insee](https://www.insee.fr/fr/information/5230987)

Or, le traitement des donn√©es textuelles pose une difficult√© particuli√®re: __le langage naturel n'a pas de sens pour un ordinateur !__ Un ordinateur ne travaille qu'avec des nombres, et ne peut pas manipuler directement des mots, des expressions ou des phrases. C'est pourquoi de multiples m√©thodes ont √©t√© d√©velopp√©es au cours des derni√®res d√©cennies pour proposer des solutions g√©n√©riques permettant de traiter des corpus de donn√©es textuelles √† la fois peu structur√©s et h√©t√©rog√®nes. Cet ensemble de m√©thodes de traitement automatis√© du langage, plus connues sous l'acronyme `NLP` (_natural langage processing_) constituent encore aujourd'hui un champ de recherche particuli√®rement actif. 

Ce billet de blog n'a pas l'ambition de proposer un aper√ßu des m√©thodes de NLP, mais simplement de pr√©senter deux m√©thodes fr√©quemment utilis√©es pour transformer l'information textuelle pour la rendre compr√©hensible et utilisable par une machine:

*   le [sac de mots (_bag of words_)](#bag-of-words),
*   le [plongement lexical (_word embedding_)](#embedding).

## Traiter un texte comme une information num√©rique : les approches possibles

### L'approche _bag of words_ {#bag-of-words}

Le principe du _bag of words_ est qu'on peut d√©crire un document comme
un dictionnaire de mots (un _sac de mots_) dans lequel on pioche plus ou moins fr√©quemment
un terme en fonction de son nombre d'occurrences. 

La mani√®re la plus simple de transformer des phrases ou des
libell√©s textuels en une information num√©rique est de passer
par un objet que l‚Äôon appelle la __matrice document-terme__.
L‚Äôid√©e est de compter le nombre de fois o√π les
mots (les termes, en colonne) sont pr√©sents dans chaque phrase ou libell√© (le document, en ligne).
Cette matrice fournit alors une repr√©sentation __num√©rique__ des donn√©es textuelles.

Consid√©rons un corpus constitu√© des trois phrases suivantes :

*   _"La pratique du tricot et du crochet_"
*   _"Transmettre la passion du timbre"_
*   _"Vivre de sa passion"_

La matrice document-terme associ√©e √† ce corpus est la suivante :


|                                     | crochet | de | du | et | la | passion | pratique | sa | timbre | transmettre | tricot | vivre |
| ----------------------------------- | :-------: | :--: | :--: | :--: | :--: | :-------: | :--------: | :--: | :------: | :-----------: | :------: | :-----: |
| La pratique du tricot et du crochet | 1       | 0  | 2  | 1  | 1  | 0       | 1        | 0  | 0      | 0           | 1      | 0     |
| Transmettre sa passion du timbre    | 0       | 0  | 1  | 0  | 0  | 1       | 0        | 1  | 1      | 1           | 0      | 0     |
| Vivre de sa passion                 | 0       | 1  | 0  | 0  | 0  | 1       | 0        | 1  | 0      | 0           | 0      | 1     |

__Mission accomplie !__ üéâ
Chaque phrase du corpus est associ√©e √† un vecteur num√©rique.

Il est maintenant possible de manipuler cette matrice comme des donn√©es tabulaires classiques. Par exemple, on pourrait appliquer l‚Äôun des algorithmes usuels de classification (r√©gression logistique, for√™t al√©atoire, _gradient boosting_, etc.) pour classer ces phrases dans des cat√©gories.

L'approche _bag-of-words_ r√©pond donc au besoin initial de transformer les donn√©es pour les rendre manipulables par une machine, en repr√©sentant les donn√©es textuelles sous la forme d'une matrice document-terme. Cette approche pr√©sente n√©anmoins une limite: elle traite tous les termes de fa√ßon ind√©pendante et ne restitue pas la proximit√© de certains termes. Par exemple, rien dans la matrice document-terme de l'exemple pr√©c√©dent n'indique que les termes _'tricot"_ et _"crochet"_ rel√®vent du m√™me champ lexical. Un autre type de repr√©sentation plus complexe et plus riche constitue souvent comme une meilleure option : le plongement lexical.

### Le plongement lexical {#embedding}

Le plongement lexical (_word embedding_ en anglais)
consiste √† projeter l'ensemble des termes qui apparaissent dans le corpus dans un espace num√©rique √† $n$ dimensions. Chaque mot est repr√©sent√© par un vecteur de taille fixe (comprenant $n$ nombres),
de fa√ßon √† ce que deux mots dont le sens est proche poss√®dent des repr√©sentations num√©riques proches. Ainsi les mots ¬´ chat ¬ª et ¬´ chaton ¬ª devraient avoir des vecteurs de plongement assez similaires, eux-m√™mes √©galement assez proches de celui du mot ¬´ chien ¬ª et plus √©loign√©s de la repr√©sentation du mot ¬´ maison ¬ª.

![Illustration du word embedding](word_embedding.png)

<div style="text-align: center"> Illustration du plongement lexical. Source : Post de blog <a href="https://medium.com/@hari4om/word-embedding-d816f643140" target="_blank">Word Embedding : Basics</a></div>  &nbsp;


Chacune des $n$ composantes va encoder des informations diff√©rentes, comme le fait d‚Äô√™tre un √™tre vivant ou un objet, le genre, l‚Äô√¢ge, le niveau d‚Äôabstraction, etc. C'est pour cette raison que des termes appartenant au m√™me champ lexical auront des repr√©sentations num√©riquement proches. En pratique, les vecteurs de plongement ont des dizaines voire des centaines de composantes et il est impossible d‚Äôassocier √† chacune une interpr√©tation univoque : toutes les notions s‚Äôentrem√™lent, mais chaque composante a un r√¥le √† jouer.

Le plongement lexical poss√®de deux avantages par rapport √† l‚Äôapproche _bag of words_. D'une part, il fournit une repr√©sentation dense des termes, qui est plus adapt√©e aux algorithmes d‚Äôapprentissage statistique que la repr√©sentation creuse (matrice contenant beaucoup de z√©ros) de l‚Äôapproche _bag of words_. D'autre part, les op√©rations math√©matiques ont un sens sur les vecteurs du plongement. C'est l√† la magie du plongement lexical: __il devient possible de faire des math√©matiques avec les mots__. Ainsi par exemple, les vecteurs r√©sultant de la diff√©rence entre les repr√©sentations des mots ¬´ femme ¬ª et ¬´ homme ¬ª d‚Äôune part, et des mots ¬´ reine ¬ª et ¬´ roi ¬ª d‚Äôautre part, devraient √™tre proches, car conceptuellement ces couples de mots sont r√©gis par la m√™me relation : un changement de genre.

Cette formule, souvent r√©sum√©e sous la forme, 

$$\text{king} - \text{man} + \text{woman} ‚âà \text{queen}$$

a assur√© le succ√®s des _embeddings_, car elle permet √† une machine d'appr√©hender les relations logiques entre les mots.

Jusqu‚Äôici, nous avons parl√© du plongement de mots, mais comment obtenir le plongement d‚Äôun libell√© textuel ? Une possibilit√© est de consid√©rer tous les mots qui composent le libell√© et de calculer la moyenne de leurs vecteurs de plongement.

## Construction d‚Äôun plongement lexical

Un plongement lexical se construit en parcourant un grand corpus de textes et en rep√©rant les mots qui apparaissent souvent dans le m√™me contexte. L'ensemble des articles `Wikipedia` est un des corpus de pr√©dilection des personnes ayant construit des plongements
lexicaux. Il comporte en effet des phrases compl√®tes, contrairement √† des informations issues de commentaires de r√©seaux sociaux, 
et propose des rapprochements int√©ressants entre des personnes, des lieux, etc.

Le contexte d‚Äôun mot est d√©fini par une fen√™tre de taille fixe autour de ce mot. La taille de la fen√™tre est un param√®tre de la construction de l‚Äô_embedding_. Le corpus fournit un grand ensemble d‚Äôexemples mots-contexte, qui peuvent servir √† entra√Æner un r√©seau de neurones.

Plus pr√©cis√©ment, il existe deux approches :

*   _Continuous bag of words_ (CBOW), o√π le mod√®le est entra√Æn√© √† pr√©dire un mot √† partir de son contexte ;
*   _Skip-gram_, o√π le mod√®le tente de pr√©dire le contexte √† partir d‚Äôun seul mot.

![Illustration de la diff√©rence entre les approches CBOW et Skip-gram](CBOW_Skipgram_training.png)

<div style="text-align: center"> Illustration de la diff√©rence entre les approches CBOW et Skip-gram. Source : Anwarvic sur  <a href="https://stackoverflow.com/questions/57507056/difference-between-max-length-of-word-ngrams-and-size-of-context-window" target="_blank">StackOverflow</a></div>  &nbsp;

### Algorithmes c√©l√®bres

La m√©thode de construction d‚Äôun plongement lexical pr√©sent√©e ci-dessus est celle de l‚Äôalgorithme [`Word2Vec`](https://fr.wikipedia.org/wiki/Word2vec).
Il s‚Äôagit d‚Äôun mod√®le _open-source_ d√©velopp√© par une √©quipe de `Google` en 2013.
`Word2Vec` a √©t√© le pionnier en termes de mod√®les de plongement lexical.

Le mod√®le [`GloVe`](https://nlp.stanford.edu/projects/glove/) constitue un autre exemple[^3]. D√©velopp√© en 2014 √† Stanford,
ce mod√®le ne repose pas sur des r√©seaux de neurones mais sur la construction d‚Äôune grande matrice de co-occurrences de mots. Pour chaque mot, il s‚Äôagit de calculer les fr√©quences d‚Äôapparition des autres mots dans une fen√™tre de taille fixe autour de lui. La matrice de co-occurrences obtenue est ensuite factoris√©e par une d√©composition en valeurs singuli√®res.
Il est √©galement possible de produire des plongements de mots √† partir du [mod√®le de langage `BERT`](https://jalammar.github.io/illustrated-bert/), d√©velopp√© par `Google` en 2019, dont il existe des d√©clinaisons dans diff√©rentes langues, notamment en Fran√ßais (les
mod√®les [`CamemBERT`](https://camembert-model.fr/) ou [`FlauBERT`](https://github.com/getalp/Flaubert))

[^3]: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation

Enfin, le mod√®le [`FastText`](https://fasttext.cc/), d√©velopp√© en 2016 par une √©quipe de `Facebook`, fonctionne de fa√ßon similaire √† `Word2Vec` mais se distingue particuli√®rement sur deux points :

*   En plus des mots eux-m√™mes, le mod√®le apprend des repr√©sentations pour les n-grams de caract√®res (sous-s√©quences de caract√®res de taille \\(n\\), par exemple _¬´ tar ¬ª_, _¬´ art ¬ª_ et _¬´ rte ¬ª_ sont les trigrammes du mot _¬´ tarte ¬ª_), ce qui le rend notamment robuste aux variations d‚Äôorthographe ;
*   Le mod√®le a √©t√© optimis√© pour que son entra√Ænement soit particuli√®rement rapide.

A `l‚ÄôInsee`, plusieurs mod√®les de classification de libell√©s textuels dans des nomenclatures reposent sur l‚Äôalgorithme de plongement lexical [`FastText`](https://fasttext.cc/). 

![Illustration du mod√®le fastText](fasttext.png)

 <div style="text-align: center"> Illustration du fonctionnement du mod√®le fastText sur un libell√© de profession</div>  &nbsp;

### Comment utiliser ces mod√®les en pratique ? 

Collecter √† nouveau les donn√©es ayant servi √† entrainer un mod√®le puis le
r√©-entra√Æner implique √©norm√©ment de ressources, ce qui est co√ªteux
en temps et peu √©cologique[^4].

[^4]: Strubell, Ganesh, and McCallum (2019)
estiment que l‚Äôentra√Ænement d‚Äôun mod√®le √† l‚Äô√©tat de l‚Äôart dans le domaine du NLP
n√©cessite autant d‚Äô√©nergie que ce que consommeraient cinq voitures, en moyenne,
au cours de l‚Äôensemble de leur cycle de vie.

En `Python`, plusieurs librairies proposent les mod√®les `Word2Vec`, `GloVe`, `BERT` ou `FastText`. 
Le [package `gensim`](https://radimrehurek.com/gensim/)
les met toutes en ≈ìuvre √† l'exception de `BERT`. Ce dernier est disponible
sur [`HuggingFace`](https://huggingface.co/docs/transformers/model_doc/bert), la principale plateforme
de mise √† 
disposition de mod√®les pr√©-entra√Æn√©s. Il est ainsi possible d'utiliser `BERT` avec
les librairies [`PyTorch`](https://pytorch.org/hub/huggingface_pytorch-transformers/) ou [`Keras`](https://keras.io/examples/nlp/text_extraction_with_bert/). 
Chacun des mod√®les pr√©sent√©s poss√®de √©galement son package d√©di√©, g√©n√©ralement d√©velopp√© par l'√©quipe de recherche
ayant entra√Æn√© le mod√®le.

En `R`, il faut utiliser les packages `word2vec`, `text2vec` (pour le mod√®le GloVe) et `fastTextR`.


## Bonus : le plongement lexical en version ludique

Le r√©sultat d‚Äôun plongement lexical peut avoir de nombreux usages.
Il rend notamment possible le calcul de la proximit√© entre deux mots quelconques.

Une mani√®re de proc√©der est de calculer la similarit√© cosinus entre les vecteurs de plongement des deux mots. Plus pr√©cis√©ment, la similarit√© entre deux mots de repr√©sentations vectorielles \\(u\\) et \\(v\\) est d√©finie comme le cosinus de leur angle \\( \theta \\) : $$cos(\theta) = \frac{u \cdot v}{\lVert u\rVert \lVert v\rVert}$$

![Illustration de la similarit√© cosinus](similarite_cosinus.png)

 <div style="text-align: center"> Illustration de la similarit√© cosinus en deux dimensions</div>  &nbsp;

Le calcul de la proximit√© entre les mots est √† la base du jeu [cemantix](https://cemantix.certitudes.org/).
Le principe est proche du jeu `Wordle` mais s'en distingue sur un point : il y a certes un mot √† trouver chaque jour et il s‚Äôagit de faire des propositions de mots mais le jeu r√©pond en donnant la proximit√© entre les mots propos√©s et le mot du jour. Ainsi, au fil des propositions, on a une vision de plus en plus pr√©cise du champ lexical associ√© au mot myst√®re, jusqu‚Äô√† finalement le trouver.
