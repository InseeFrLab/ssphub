[
  {
    "objectID": "project/2022_satellites/index.html",
    "href": "project/2022_satellites/index.html",
    "title": "Utilisation des images satellites pour la statistique publique",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\n\n\nDétail du projet\nFin 2022, un groupe de travail expérimental sur l’utilisation des données satellites pour la statistique publique a été lancé. Cette initiative visait à améliorer l’organisation des enquêtes cartographiques dans les départements d’outre-mer, particulièrement en Guyane et à Mayotte. Ces enquêtes servent à mettre à jour le répertoire individus / logement (RIL) chaque année, et facilitent l’identification en amont des enquêtes de nouvelles zones d’habitations spontanées ou temporaires. Des algorithmes appliqués à des photographies satellites permettent d’identifier de telles zones, qui à une année d’exercice donnée nécessiteront plus d’enquêteurs qu’à l’exercice précédent.  Le projet est présenté plus précisément ici.\n\n\nActeurs\nInsee (DG et Direction interrégionale Réunion-Mayotte)\n\n\nRésultats du projet\nLes résultats du projet sont présentés plus précisément sur le site dédié. La phase d’expérimentation s’est achevée avec:  - La transmission de statistiques d’évolution de bâti par îlot à Mayotte. Ces statistiques ont servi à l’organisation de l’enquête cartographique complémentaire 2024 à Mayotte ;  - Le développement d’une application de visualisation intégrant des fonds de carte construits à partir de tuiles Pleiades et les bâtiments détectés sur ces tuiles avec nos algorithmes, ainsi que des couches de détection de changement ;  - La création dupackage Python de traitement d’images satellites Astrovision\n\n\nCode du projet\n- https://github.com/InseeFrLab/astrovision : package Python pour travailler avec des données satellites:  - https://inseefrlab.github.io/satellite-images-webapp/: code de l’application de visualisation  - https://github.com/InseeFrLab/satellite-images-preprocess: code de pré-processing des données  - https://github.com/InseeFrLab/satellite-images-train : code pour l’entraînement des modèles  - https://github.com/InseeFrLab/satellite-images-inference : code pour l’inférence\n\n\n\n\n\nDocuments relatifs au projet sur les données satellites à l’Insee\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Article JMS 2025\n            \n\n            \n              Description : Utilisation des images satellites pour améliorer le repérage des logements à Mayotte.\n\n            \n\n            \n            \n              \n                Maëlys Bernard, Raya Berova et Thomas Faria\n                25 nov. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Utilisation des images satellites pour améliorer le repérage des logements dans les départements d'outre-mer\n            \n\n            \n              Description : Slides pour le séminaire de Méthodologie Statistique & Sciences des données de la DMCSI - La terre vue d’en haut : images satellites, images aériennes et statistique publique.\n\n            \n\n            \n            \n              \n                Raya Berova et Thomas Faria\n                3 juil. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Conference on New Techniques and Technologies for official Statistics (NTTS 2025)\n            \n\n            \n              Description : Slide deck for the NTTS 2025 conference.\n\n            \n\n            \n            \n              \n                Raya Berova, Thomas Faria et Clément Guillo\n                12 mars 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            AIML4OS Workshop on Earth Observation\n            \n\n            \n              Description : Slideshow presentation of the project Detecting changes in buildings in French overseas departments for Work Package 7,\nArtificial Intelligence and Machine Learning for Official Statistics, organized by Eurostat.\n\n            \n\n            \n            \n              \n                Raya Berova, Thomas Faria et Clément Guillo\n                13 nov. 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Séminaire à la Dirag\n            \n\n            \n              Description : Ce séminaire marque la fin des travaux d’expérimentation. À cette occasion, nous présenterons l’intégralité de la chaîne de traitement, de la récupération des images jusqu’à l’application d’aide à la décision, en mettant l’accent sur les difficultés rencontrées. La poursuite de ces travaux dépendra des besoins réels de l’Insee et nécessitera des compétences variées et une technicité élevée pour maintenir et/ou améliorer la chaîne de traitement.\n\n            \n\n            \n            \n              \n                Raya Berova, Gaëtan Carrere, Thomas Faria, Clément Guillo et Tom Seimandi\n                28 mai 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Document de travail\n            \n\n            \n              Description : Document retraçant les avancées et résultats obtenus sur les travaux exploratoires utilisant des images satellitaires\n\n            \n\n            \n            \n              \n                Raya Berova, Gaëtan Carrere, Thomas Faria, Clément Guillo et Tom Seimandi\n                16 mai 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Application web\n            \n\n            \n              Description : Application web à destination des agents facilitant l'utilisation des résultats fournis par le modèle de segmentation.\n\n            \n\n            \n            \n              \n                \n                23 oct. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Documentation\n            \n\n            \n              Description : Ici est rassemblée toute la documentation technique sur le travail effectué au cours de l'expérimentation sur les données satellite\n\n            \n\n            \n            \n              \n                Raya Berova, Gaëtan Carrere, Thomas Faria, Clément Guillo et Tom Seimandi\n                23 oct. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_Appariement/index.html",
    "href": "project/2022_Appariement/index.html",
    "title": "Comparaison des méthodes d’appariement et apport du machine learning",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nComparaison des méthodes d’appariement et apport du machine learning\n\n\n\n\nDétail du projet\nLe programme Resil vise à construire un système de répertoires d’individus, de ménages et de locaux d’habitation, durable et évolutif, mis à jour à partir de sources administratives diverses. Il nécessite l’agrégation de plusieurs sources de données sans identifiant direct commun.  Le but de l’expérimentation est de tester et de comparer différentes méthodes d’appariements afin de dégager des recommandations pour les travaux nécessaires à la construction des répertoires. Celles-ci seront fondées sur des critères de performance (qualité de l’appariement) mais aussi sur des considérations opérationnelles (facilité de déploiement, temps de calcul, etc.). L’objectif est notamment d’évaluer l’apport et les contraintes des méthodes probabilistes ainsi que du machine learning dans les tâches d’appariement. Ce travail s’accompagnera d’une réflexion sur la normalisation préalable des données et l’évaluation des résultats d’un appariement.\n\n\nActeurs\nInsee (DSDS)\n\n\nRésultats du projet\nRésultats présentés aux journées de la méthodologique statistique 2022:  - Méthodologie d’appariement de données individuelles ;  - Probabilistes ou déterministes, des méthodes d’appariements au banc d’essai du programme RéSIL ;  - Impact du nettoyage des données sur la qualité d’un appariement   Écriture d’un document de travail:  - Les appariements : finalités, pratiques et enjeux de qualité, juillet 2024   En production :  - ??\n\n\nCode du projet"
  },
  {
    "objectID": "project/2021_Extraction_CS/index.html",
    "href": "project/2021_Extraction_CS/index.html",
    "title": "Extraction automatique du tableau des filiales et participations des comptes sociaux des entreprises",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nExtraction automatique du tableau des filiales et participations des comptes sociaux des entreprises\n\n\n\n\nDétail du projet\nUne expérimentation autour de l’extraction automatisée du tableau des filiales et participations des comptes sociaux des entreprises a été démarrée en 2022 à la suite d’un stage dans la division PTGU. Ce tableau est exploité à la main au cours des opérations de profilage des entreprises, ce qui constitue un travail fastidieux. L’expérimentation a été menée avec la Banque de France qui est intéressé par le même cas d’usage que l’Insee, avec deux objectifs principaux :   - développer un prototype d’application permettant à des utilisateurs de récupérer un tableau des filiales et participations automatiquement pour un numéro Siren et une année donnés ;  - comparer les performances de différentes méthodes d’extraction automatique de tableaux, basées d’une part sur des outils open-source et d’autre part sur des solutions commerciales.\n\n\nActeurs\nInsee (DSE), Inpi, Banque de France\n\n\nRésultats du projet\n- Une API expérimentale et une interface expérimentale ont été mises en place pour répondre au besoin métier.  - Le projet a par ailleurs été présenté le 27 juin 2024 à un séminaire interne Insee slides \n\n\nCode du projet\n- https://github.com/InseeFrLab/ca-document-querier/ : wrapper Python autour de l’API entreprises de l’Inpi  - https://github.com/InseeFrLab/extraction-comptes-sociaux : dépôt « core » où se trouvent les éléments pour la détection de page/extraction de tableaux  - https://github.com/InseeFrLab/extract-table-ui : dépôt de code de l’interface expérimentale  - https://github.com/InseeFrLab/extraction-comptes-sociaux-llm : code source de l’architecture globale, comprenant des microservices conteneurisés et orchestrés par Kubernetes. Il combine des appels à des API externes (INPI), le traitement de PDF, et l’utilisation de modèles de langage (LLM) pour l’analyse et l’extraction de données."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projets innovants du SSP",
    "section": "",
    "text": "Vous pouvez trouver ici les projets innovants portés par des membres du SSP.\n\n\n\n\n\n\n\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\n\nPython\n\npackage\n\ndeep learning\n\nimages satellites\n\nen production\n\n\n\nUtiliser les images satellites pour améliorer le recensement de la population dans les territoire ultra-marins\n\n\n\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\nProjets\n\ncodification automatique\n\nextraction de données\n\ndonnées de caisse\n\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\n\nPython\n\ncodification automatique\n\nfasttest\n\npackage\n\nen production\n\nMLFlow\n\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtraction automatique du tableau des filiales et participations des comptes sociaux des entreprises\n\n\n\nPython\n\nextraction de données\n\nAPI\n\nMachine learning\n\nen production\n\n\n\nExtraire les informations de tableaux de comptes sociaux, en particulier des tableaux des filiales et participations, contenus dans des images scannées mises à disposition…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\n\nProjets\n\nPython\n\ncodification automatique\n\nfasttext\n\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparaison des méthodes d’appariement et apport du machine learning\n\n\n\nappariement\n\ndonnées administratives\n\nResil\n\n\n\nTester et comparer différentes méthodes d’appariements afin de dégager des recommandations pour les travaux nécessaires à la construction des répertoires, notamment dans le…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\nPython\n\ncodification automatique\n\ndonnées de caisse\n\nCOICOP\n\nIPC\n\nconjoncture\n\nproduction\n\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "infolettre/infolettre_old/index.html",
    "href": "infolettre/infolettre_old/index.html",
    "title": "Archive des infolettres et lettres Big Data",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit"
  },
  {
    "objectID": "infolettre/infolettre_old/index.html#les-anciennes-infolettres",
    "href": "infolettre/infolettre_old/index.html#les-anciennes-infolettres",
    "title": "Archive des infolettres et lettres Big Data",
    "section": "Les anciennes infolettres",
    "text": "Les anciennes infolettres\nLes infolettres avant 2022, dont le format était différent, sont archivées ici.\n\nInfolettre n°7, décembre 2022\nInfolettre n°6, novembre 2022\nInfolettre n°5, juin 2022\nInfolettre n°4, mai 2022\nInfolettre n°3, avril 2022\nInfolettre n°2, avril 2022\nInfolettre n°1, avril 2022"
  },
  {
    "objectID": "infolettre/infolettre_old/index.html#lettres-big-data",
    "href": "infolettre/infolettre_old/index.html#lettres-big-data",
    "title": "Archive des infolettres et lettres Big Data",
    "section": "Lettres big data",
    "text": "Lettres big data\nEt même encore avant avant les infolettres 👵👴, les lettres Big Data sont archivées ici.\n\nLettre Big Data n°12, mai 2022\nLettre Big Data n°11, octobre 2021\nLettre Big Data n°10, décembre 2020\nLettre Big Data n°9, décembre 2019\nLettre Big Data n°8, mai 2019\nLettre Big Data n°7, avril 2018\nLettre Big Data n°6, novembre 2017\nLettre Big Data n°5, mars 2017\nLettre Big Data n°4, septembre 2016\nLettre Big Data n°3, juin 2016\nLettre Big Data n°2, mars 2016\nLettre Big Data n°1, décembre 2015"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html",
    "href": "infolettre/infolettre_19/index.html",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "",
    "text": "À l’occasion de cette dix-neuvième infolettre, on a discuté, réfléchi et pensé au sens de la vie. On a décidé de revoir le schéma de l’infolettre selon plusieurs critères :\n\nune périodicité mensuelle “garantie”, quitte à avoir des infolettres moins denses que d’autres ;\nsimplifier la rédaction et le contenu ;\ncontinuer de parler à toutes les personnes intéressées par la science des données, quel que soit son niveau d’expertise ;\ncontribuer à mettre en avant plus facilement des personnes ou des projets au sein du réseau.\n\nNouvelle structure, moins longue, plus ramassée, vous aurez plus de travail pour creuser plus loin les sujets ! L’idée est ainsi d’insérer au moins dans chaque veille :\n\nune datavisualisation\nles actualités du réseau et une veille\nune interview de quelqu’un ou d’une équipe selon un canevas à peu près stable.\n\nBonne lecture 📔 !"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#la-troisième-journée-du-réseau-1-décembre---la-tréso-malakoff",
    "href": "infolettre/infolettre_19/index.html#la-troisième-journée-du-réseau-1-décembre---la-tréso-malakoff",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "La troisième journée du réseau 📅 1 décembre - La Tréso (Malakoff)",
    "text": "La troisième journée du réseau 📅 1 décembre - La Tréso (Malakoff)\nRéservez votre 1er décembre ! Pour la troisième année consécutive, le SSPLab organise la journée du réseau pour rassembler les data-scientists de la statistique publique. Au menu : présentation de projets innovants, retour d’expérience et moments d’échanges informels (autrement appelés “pots” 🎉).\nComme les années précédentes, l’événement sera en présentiel et à distance pour permettre à tous de participer. Les détails seront publiés sur le site du réseau et si jamais vous voulez déjà vous inscrire alors que l’agenda n’est pas finalisé, c’est possible ici.\n\n👉️ Ajouter cet événement à votre agenda Outlook"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#le-site-du-réseau-évolue",
    "href": "infolettre/infolettre_19/index.html#le-site-du-réseau-évolue",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Le site du réseau évolue",
    "text": "Le site du réseau évolue\nL’inscription à la liste de diffusion a été revue et utilise maintenant Grist. Pour s’inscrire à la liste de diffusion, c’est par ici. Une fois inscrit, vous pouvez créer un compte sur Grist et vous connecter directement sur l’annuaire pour mettre à jour vos données, demander votre désinscription en cochant la case “Supprimez mon compte”.\nPar ailleurs, le site du réseau devrait évoluer dans les prochaines semaines. Il va s’étoffer pour présenter plus de projets en cours et permettre ainsi à tout un chacun de savoir qu’un projet existe et pouvoir échanger entre pairs. Si vous souhaitez valoriser un projet, n’hésitez pas à nous le faire savoir !"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#ia",
    "href": "infolettre/infolettre_19/index.html#ia",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "IA",
    "text": "IA\nComme toujours, une flopée d’articles a été publiée sur l’IA : le nouveau modèle d’OpenAI (GPT-5) a été déployé cet été, l’usage de l’IA se développe, des craintes se font entendre sur l’existence d’une bulle financière et, avec l’augmentation de son utilisation, de plus en plus de failles de sécurité liées sont découvertes. Un petit florilège rapide, non exhaustif :\n\nEn France, l’IA est de plus en plus utilisée par les entreprises d’après une étude de l’Insee. En 2024 ainsi, une entreprise sur dix utilise l’IA, et ce phénomène concerne particulièrement 33% des grandes entreprises et 42% de celles de l’information. L’usage de l’IA augmente de 4 points par rapport à 2023. L’IA est par ailleurs légèrement moins utilisée par les entreprises en France que dans l’Europe, où 13% des entreprises disent utiliser l’IA en 2024.\nSur l’impact de l’IA, notamment sur le travail et la productivité, de nombreuses études continuent d’être publiées. Petit disclaimer, la technologie évolue encore très vite : depuis son arrivée il y a moins de trois ans, les bugs relevés au début ne sont plus du tout d’actualité aujourd’hui : les images sont de bien meilleure qualité, des RAG ont été mis en place … Face à un domaine aussi changeant, les résultats des études varient donc encore beaucoup.\n\nCeci étant dit, les études montrent globalement que l’IA permettrait d’améliorer l’efficacité des travailleurs, particulièrement des non-experts, et réduit les inégalités de performance, même si les résultats sont contrastés. Selon cette étude, l’IA commence à avoir un impact négatif sur l’emploi, quand celle-ci estime à l’inverse que les gains de productivité pour les développeurs sont sur-estimés.\nL’usage de l’IA serait particulièrement efficace pour effectuer des tâches moyennement rares, l’humain restant plus efficace sur les tâches courantes (cf. par exemple ce papier). Par ailleurs, sur les tâches complexes ou rares, l’IA serait largement moins efficace que l’humain et produirait des résultats de qualité moindre (cf. ce papier).\n\nConcernant la technologie en soit, des chercheurs ont réussi, à partir d’un petit modèle d’IA générative, à classifier du texte aussi efficacement qu’avec un gros modèle et nécessitant bien moins de données. Pour ce faire, ils ont utilisé un modèle de régression pénalisée (type Lasso/Ridge) sur la représentation numérique sous-jacente du texte. Plus de détails dans leur article.\nDe nombreux articles font craindre l’existence d’une bulle financière autour de l’IA.\n\nEdward Zitron, un publiciste britannique, auteur et podcasteur, rappelle sur son blog toutes les raisons pour laquelle une bulle existerait actuellement sur l’IA. Il rappelle notamment que les 560Md$ investis par les GAFAM dans l’IA n’ont généré que très peu de bénéfices et que le seul gagnant est Nvidia. Comme le dit le proverbe, “Pendant la ruée vers l’or, ce ne sont pas les chercheurs d’or qui se sont le plus enrichis, mais les vendeurs de pelles et de pioches”.\nL’adoption de l’IA par les entreprises prendrait par ailleurs plus de temps qu’anticipé et n’aurait pas des rendements aussi rapides qu’espéré.\nPlus généralement, des articles, comme cet article de Forbes, rappellent que l’IA reste très utilisée aujourd’hui et que, même si aujourd’hui des investissements sont fait vers des projets peu productifs, l’adoption de nouvelles technologies prend du temps. Les articles citent beaucoup l’exemple d’internet, et de la bulle du début des années 2000 : les attentes du marché étaient trop hautes par rapport à tout le travail qu’il restait à faire, et cela n’empêche pas que aujourd’hui, 25 ans après cette bulle, les investissements dans le réseau internet ont permis de changer la société.\n\nEnfin, avec l’augmentation de son utilisation, la sécurité de la technologie est un enjeu qui est de plus en plus discuté, au-delà du détournement à des fins illégales qui attend toute innovation numérique :\n\nDes données confidentielles de Microsoft ont fuité après le piratage d’agents Copilot. Les hackeurs ont ainsi reçu par mail des extraits des contacts et des ventes de Microsoft.\nSelon le rapport d’Anthropic sur les menaces liées à l’IA, cette technologie a notamment été détournée pour :\n\ns’assurer des postes bien payés pour des Nord-Coréens, qui leur ont permis de rapatrier les capitaux au pays ;\nmassifier les fraudes aux données personnelles ;\nautomatiser les attaques par ransomware."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#parquet",
    "href": "infolettre/infolettre_19/index.html#parquet",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Parquet",
    "text": "Parquet\n\nLe site Hyperparam permet d’afficher très rapidement des données Parquet volumineuses sur son explorateur web très rapidement (en moins de 500ms). Pour la tuyauterie, tout est expliqué sur ce blog."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#kubernetes",
    "href": "infolettre/infolettre_19/index.html#kubernetes",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Kubernetes",
    "text": "Kubernetes\n\nComment détecter facilement des pods Kubernetes peu actifs et les désactiver? Un début de processus a été publié sur Devops.dev."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#nouveautés",
    "href": "infolettre/infolettre_19/index.html#nouveautés",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Nouveautés",
    "text": "Nouveautés\n\nUne nouvelle version des notebooks Observable est disponible en pré-production, avec un kit open source pour générer des notebooks et des sites statiques et une application pour Mac pour éditer ses notebooks en local, intégrant de manière plus fluide les apports de l’IA. Plus de détails par ici et une galerie d’exemples de sites.\nL’université allemande de Hanovre a publié une base d’embedding des entités d’Openstreetmap directement utilisable pour entraîner des modèles de machine learning.\nSelon une étude de Posit, le meilleur modèle d’IA pour aider à coder en Python serait ceux d’OpenAI (o3-mini, o4-mini) ou d’Anthropic (Claude Sonnet 4)."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#fun",
    "href": "infolettre/infolettre_19/index.html#fun",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Fun",
    "text": "Fun\n\nVous vous êtes déjà demandé comment résoudre un SUTOM avec les dépendances de Python ? Non ? Et bien quelqu’un a trouvé le moyen de résoudre des Sudoku et des Motus grâce à cela ! Tout est expliqué ici\nAvez-vous déjà vu une intelligence artificielle jouer au Loup-Garou ? Des étudiants de l’ENSAE se sont amusés ont étudié quelles IA étaient meilleures au jeu du Loup-Garou. Ce jeu nécessite en effet de mentir, de convaincre, et d’adapter sa stratégie pour survivre (pour les villageois) ou tuer tous les villageois (pour les loup-garous). A la fin, GPT-5 gagne dans 97 % des 60 matchs joués, contre 15% pour GPT-OSS-120b.\n\n\n\n\n\n\n\nAstuce\n\n\n\nVous voyez d’autres sujets d’actualité intéressants ? N’hésitez pas à les partager sur le groupe Tchap 💬 directement !"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#première-interview-avec-nicolas-qui-travaille-à-linsee-ssplab",
    "href": "infolettre/infolettre_19/index.html#première-interview-avec-nicolas-qui-travaille-à-linsee-ssplab",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Première interview avec Nicolas, qui travaille à l’Insee (SSPLab)",
    "text": "Première interview avec Nicolas, qui travaille à l’Insee (SSPLab)\n\n\n\nPeux-tu te présenter?\nDe formation ingénieur, j’ai travaillé huit ans dans l’administration publique avec un parcours que certains ont dit plutôt atypique (ce n’est pas totalement mon avis 🙃). J’ai notamment travaillé au sein de la DG Trésor et à la Commission européenne, sur des sujets de prévision de finances publiques, de négociations européennes et de suivi de la Banque centrale européenne. Le traitement de la donnée n’a pas été jusque-là au centre de mes postes mais l’importance des outils et de traitement plus robuste aurait facilité la vie à certains moments. J’arrive donc à l’Insee pour la première fois mais je suis content de “découvrir la maison”.  A l’Insee, je travaille au sein du SSPLab, le laboratoire de l’innovation en data sciences de l’Insee. L’équipe est chargée de faire de la veille et d’épauler les équipes métiers dans leurs projets. J’ai tout particulièrement l’honneur de succéder à Lino au poste d’animateur du SSPHub, big up à lui pour tout ce qu’il a fait ces trois dernières années !\n\n\nPeux-tu donner un conseil que tu aurais aimé recevoir en lien avec la data ?\nQuestion difficile, étant donné que je n’ai pas mené beaucoup de projets data jusqu’ici. Mais j’ai remarqué que l’ordinateur avait rarement tort, ce qui est assez frustrant, et qu’être persévérant était nécessaire. Avoir un code propre aussi, quand on n’est même pas capable de comprendre ce qu’on a codé soi-même en rentrant de vacances, cela donne une leçon pour la suite. J’aurai sûrement plus d’exemples d’ici un an ou deux 😉\n\n\nAs-tu un projet qui a particulièrement marché, et pourquoi a-t-il marché ? A l’inverse, as-tu un projet qui n’a pas marché et pourquoi ?\nJ’avais codé des petits programmes pour m’aider dans mon travail quotidien, sans rapport direct avec la donnée. Ce qui a aidé dans les deux cas c’est que le besoin métier était bien défini et bien compris, puisque j’étais à la fois le métier et le développeur. Cependant, ces programmes faisaient partie d’un shadow IT qui n’a pas dû me survivre bien longtemps. J’ai quand même réussi à pousser un programme qui faisait du publipostage jusqu’à sa mise en production. À ma surprise, c’est la phase de déploiement qui a été bien plus longue que la phase de développement : cela a dû me prendre quelques jours de code contre plusieurs semaines avant le déploiement.\n\n\nDans quel domaine le service public pourrait être aidé par une utilisation plus importante des données ?\nUn usage plus important de la donnée serait fort bénéfique pour la simplification des démarches pour les citoyens, permettre de diminuer le non-recours aux subventions et différents dispositifs publics tout en renforçant la qualité des données échangées entre administrations.\n\n\nLa dernière découverte technologique qui t’a marqué\nJ’avais un peu laissé de côté R Markdown depuis huit ans. En revenant dans le domaine de la data, je découvre à quel point ce milieu a évolué grâce à Posit, Quarto et l’apport de tous ces projets open source, auxquels des gens contribuent souvent naturellement. L’open source, c’est un peu comme les pompiers ou le secours en mer : ce sont des bénévoles qui font tenir et maintiennent des outils extraordinaires. Et plus récemment encore avec ma prise de poste à l’Insee, la découverte d’Onyxia, de duckDB et de Parquet m’ont aussi marqués."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#linterview-à-laquelle-vous-avez-échappé",
    "href": "infolettre/infolettre_19/index.html#linterview-à-laquelle-vous-avez-échappé",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "L’interview à laquelle vous avez échappé",
    "text": "L’interview à laquelle vous avez échappé\nQuelle est la plus-value de l’IA par rapport à un travail humain ? J’ai comparé les questions que j’avais trouvées avec des idées produites par une IA. Voici les questions qu’il m’a proposées, que j’ai proposé de ne pas suivre. Remarquez, ces questions sont le résultat de deux ou trois échanges avec l’IA, je n’y ai pas passé beaucoup plus de temps.\n\nVoici une proposition de questions adaptées à des data scientists ou des managers de data scientists travaillant dans l’administration publique :\n\nFormations et influences “Quelle formation ou influence majeure a façonné votre approche des données et de la gestion dans l’administration ?”\nExpériences professionnelles “Quelles ont été les étapes clés de votre carrière dans l’administration qui ont influencé votre façon de gérer ou d’analyser les données ?”\nMotivations et passions “Qu’est-ce qui vous motive le plus dans l’analyse ou la gestion des données au sein de l’administration publique ?”\nRéalisations majeures “Quels sont vos principaux accomplissements en matière de gestion ou d’analyse de données dans votre rôle actuel ?”\nDéveloppement personnel “Quels défis spécifiques aux données avez-vous rencontrés dans le secteur public et comment les avez-vous surmontés ?”\nProjets actuels “Sur quels projets liés aux données travaillez-vous actuellement, et qu’est-ce qui les rend particulièrement intéressants ou innovants ?”\nRegrets ou aspirations “Y a-t-il des projets ou réalisations en lien avec les données que vous auriez aimé accomplir dans votre carrière mais qui ne se sont pas concrétisés ?”\nVision future “Comment voyez-vous l’évolution de l’importance des données dans l’administration publique dans les années à venir, et quel rôle aimeriez-vous y jouer ?”\nConseils aux jeunes générations “Quel conseil donneriez-vous à un jeune professionnel qui souhaite devenir data scientist ou manager de data scientists dans le secteur public ?”\nPhilosophie personnelle “Si vous deviez partager une philosophie ou un credo concernant la gestion ou l’analyse des données dans l’administration, quelle serait-elle ?”\n\nCes questions devraient aider à explorer les nuances spécifiques du travail des data scientists et de leurs managers dans le secteur public. Si vous souhaitez ajuster davantage ou ajouter des thèmes spécifiques, n’hésitez pas à me le faire savoir !"
  },
  {
    "objectID": "infolettre/infolettre_17/index.html",
    "href": "infolettre/infolettre_17/index.html",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "",
    "text": "En ce début d’année 2024, nous prenons un peu de recul sur l’année écoulée et vous présentons les avancées marquantes de l’année 2023 dans le domaine de la data science. Ces avancées concernent l’intelligence artificielle générative, au cœur des débats médiatisés, mais aussi plusieurs développements technologiques très utiles pour l’analyse et la diffusion de bases de données. Des éléments plus techniques, qui ne sont pas nécessaires à la compréhension globale, sont présents dans des encadrés déroulables ou sont référencés dans les sections “Pour en savoir plus”.\nLa fin de la newsletter est consacrée à quelques annonces sur les prochains événements communautaires du réseau."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html#panorama-des-avancées-ayant-eu-lieu-en-2023-dans-le-domaine-de-la-data-science",
    "href": "infolettre/infolettre_17/index.html#panorama-des-avancées-ayant-eu-lieu-en-2023-dans-le-domaine-de-la-data-science",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "Panorama des avancées ayant eu lieu en 2023 dans le domaine de la data science",
    "text": "Panorama des avancées ayant eu lieu en 2023 dans le domaine de la data science\n\nLes IA génératives toujours au coeur des débats\nDans la continuité de la sortie de ChatGPT en décembre 2022, les IA génératives ont continué en 2023 à focaliser une part importante de l’attention portée à la data science. Outre la publication de GPT-4 en mars (modèle embarqué dans la version Pro de ChatGPT), de nombreux grands modèles de langage (LLM) généralistes ont été publiés cette année : Llama-2 (Meta), Mixtral 7B (Mistral), Falcon 180B (Technology Innovation Institute), PaLM 2 (Google)…\nCes publications ont mis en avant le caractère stratégique de la mise à disposition de modèles open source. La récupération et la structuration de corpus massifs, l’entraînement de modèles intégrant des milliards de paramètres et l’évaluation ex post de ceux-ci est à la portée d’un nombre restreint d’acteurs. La publication en open source de modèles et de codes sources est dès lors indispensable pour, entre autres, être en mesure d’évaluer la pertinence scientifique des modèles ou permettre aux acteurs n’ayant pas ces moyens techniques et humains de pouvoir tout de même réutiliser ces modèles sur leurs propres données.\nNéanmoins, malgré l’ouverture progressive de modèles, notamment par le biais d’une mise à disposition sur la plateforme Hugging Face, des contraintes limitent encore la réutilisation de ces modèles dans des infrastructures internes. Ces réseaux de neurones étant très gourmands en calculs du fait de leur architecture complexe (des milliards de paramètres pour les grands modèles de langage), afin d’obtenir une réponse du modèle il est généralement nécessaire d’effectuer les calculs par le biais de cartes graphiques (GPU), celles-ci permettant plus de parallélisme dans les calculs que les processeurs (CPU). Cependant, comme les GPU sont plus consommatrices d’énergie et plus coûteuses à l’achat du fait notamment d’une demande excédant l’offre, l’accès à cette ressource est limité pour de nombreux acteurs. Le retour à des modèles plus légers, pouvant être exploités depuis des architectures informatiques plus accessibles, constitue l’un des défis de l’année 2024.\nLes débats concernant les droits d’exploitation commerciale d’informations collectées sur internet ont été nombreux en 2023. Après les plaintes médiatisées de Getty Images (envers Stability AI), d’un collectif d’auteurs célèbres (envers OpenAI), la grève des acteurs à Hollywood contre l’exploitation de leur image par des IA et des scénaristes contre l’utilisation des générateurs de texte, c’est maintenant le New York Times qui a déposé en décembre 2023 une plainte envers OpenAI auprès de la Cour Fédérale de Manhattan. A partir d’exemples, le journal américain met en avant le degré de confiance élevé que ChatGPT attribue aux informations issues des articles du quotidien, sans pour autant en citer la provenance, ni compenser financièrement le journal. Cela entraînerait un préjudice commercial dû à la réduction du trafic sur le site du New York Times. A contrario, le journal met en avant l’effet négatif sur son image que peuvent avoir des hallucinations attribuées au quotidien. Cette plainte fait suite à l’échec des négociations entre les deux acteurs au cours de l’année 2023. Ces actualités relatives aux droits d’auteurs interviennent dans un contexte où il s’agit de l’un des principaux axes d’intervention de l’“Artificial Intelligence Act” européen (voir Infolettre #16).\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nUn article du Washington Post sur le corpus d’entraînement des LLM ;\nUn article du Financial Times qui présente de manière très pédagogique la manière dont fonctionnent les LLM ;\nUn tutoriel sur les LLM par Hugging Face ;\nUn article du New York Times sur la pénurie de GPU ;\nUn article du site spécialisé The Verge sur les dernières évolutions de la plainte de Getty Images ;\nUn article de Courrier International sur la plainte d’un collectif d’auteurs envers OpenAI ;\nNew York Times vs OpenAI par le Monde et le New York Times ;\nLes chartes relatives au contenu produit par des IA génératives du gouvernement britannique et d’une vingtaine de médias français recensés par l’INA.\n\n\n\n\n\nDes avancées scientifiques en arrière plan\nLa tendance des LLM à l’hallucination, c’est-à-dire à la production de contenu plausible par sa forme mais factuellement faux, a été l’objet d’inquiétudes quant à la véracité des informations pouvant être mises en avant par les IA amenées à occuper une place croissante dans la diffusion de contenu. Pour faire face à ce défi, au cours de l’année 2023, les équipes de conception des LLM ont utilisé de manière croissante la technique du RAG (Retrieval Augmentated Generation). Celle-ci consiste, pour les modélisateurs, à cadrer le comportement du LLM en faisant en sorte qu’il privilégie des informations issues d’un corpus adapté spécialement à une tâche. Pour faire l’analogie avec l’apprentissage humain, les modèles où le RAG intervient peuvent être comparés à des étudiants préparant une dissertation s’appuyant sur un dossier préparé par les évaluateurs. Ce contexte pourra être utilisé pour construire une réponse argumentée et des exemples plus pertinents.\nLe succès des IA de discussion comme ChatGPT est intrinsèquement lié au travail humain qui a été mis en œuvre pour évaluer la pertinence des réponses proposées par le modèle afin de régulièrement mettre à jour le comportement du modèle. Sur une question donnée, l’humain évalue la réponse la plus pertinente faite par l’IA. A partir d’un volume suffisant d’évaluations, l’algorithme pourra, dans les prochaines situations similaires, faire un choix plus pertinent s’il apprend de ses erreurs. La technique ayant gagné cette année en popularité pour cette tâche est la DPO (Direct Preference Optimization) qui vise à simplifier l’intégration des retours humains dans le cycle de réentraînement d’un modèle. Cette problématique de supervision et d’amélioration continue d’un modèle dépasse d’ailleurs le cadre des modèles de langage : afin de s’assurer que les algorithmes ne perdent pas en qualité, l’évaluation humaine par le biais, par exemple, de campagnes de labellisation ou de retours des utilisateurs, est un enjeu important dans le cycle de vie de tout modèle mis en production.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLa vidéo “State of GPT” par Andrew Karpathy ;\nUn tutoriel sur le RAG par Hugging Face ;\nLe blog présentant la technique du Reinforcement learning with human feedbacks par OpenAI ;\nL’article académique présentant la DPO et un tutoriel d’Hugging Face ;\nUn article de blog d’Andrew Ng sur la DPO.\n\n\n\n\n\nLes bases de données vectorielles gagnent en popularité\nLe langage de programmation Python est le point d’entrée de référence dans le domaine de la data science, mais pour des besoins plus spécialisés, des logiciels dédiés viennent s’y intégrer.\nC’est le cas notamment des bases de données vectorielles comme ChromaDB. Ces bases facilitent la recherche de similarité entre documents textuels en exploitant des transformations de ceux-ci en vecteurs numériques (technique des embeddings).\nPar exemple, dans l’image ci-dessous, une base de données vectorielle pourra évaluer la similarité entre les termes en utilisant des techniques d’algèbre linéaire de manière plus efficace que ne le permettrait Python. Ce dernier est en effet un langage informatique généraliste, moins performant que des logiciels spécialisés pour faire de la recherche de similarité dans des corpus massifs. Dans une chaîne de production exploitant ce type de technique, Python servira de point d’entrée et déléguera ensuite les calculs complexes à la base de données vectorielle.\n\n\n\nUn exemple d’embedding et de rapprochement de textes. Chaque bande représente une dimension latente de notre langage. Il est possible de rapprocher les termes à partir de celles-ci. Source: Financial Times.\n\n\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLe framework LangChain pour construire par le biais de Python des applications utilisant des LLM : création d’une interface pour poser des questions à un LLM, transformation de la question en vecteur numérique par le biais d’une base vectorielle comme ChromaDB, interrogation du LLM, renvoi à l’utilisateur d’une réponse… ;\nUn tutoriel de realpython.com sur ChromaDB."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html#diffuser-des-données-au-format-parquet-pour-simplifier-leur-usage",
    "href": "infolettre/infolettre_17/index.html#diffuser-des-données-au-format-parquet-pour-simplifier-leur-usage",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "Diffuser des données au format Parquet pour simplifier leur usage",
    "text": "Diffuser des données au format Parquet pour simplifier leur usage\nDans le domaine de la diffusion des données open data, l’Insee a expérimenté le format Parquet à deux reprises pendant l’année 2023. En premier lieu, pour la diffusion des données du Répertoire Electoral Unique. Plus récemment, ce sont les données détaillées du recensement de la population qui ont été diffusées dans ce format, accompagnées d’un guide d’utilisation mis en ligne sur le blog du SSP Hub (plus de détails dans l’infolettre #16).\n\n\nQuelques exemples de retours sur la publication des données détaillées du recensement au format Parquet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe format de données Parquet est très intéressant pour les data scientists intéressés par le traitement de données volumineuses. Il permet des gains de performance importants par rapport au CSV sans être dépendant d’un logiciel propriétaire (contrairement aux formats .sas7bdat, dbase, etc.). Par exemple, les données détaillées du recensement diffusées par l’Insee pèsent 450Mo au format Parquet contre 5Go au format CSV. Des outils de traitement optimisés existent pour faciliter l’utilisation de ce format. Parmi ceux-ci, cette année a été marquée par la montée en puissance de DuckDB. Il s’agit d’un logiciel qui est utilisable par le biais des principaux langages maîtrisés par les data scientists : Python, R, JavaScript ou directement en ligne de commande.\nSa capacité à gérer de grandes quantités de données en faisant des requêtes SQL optimisées sur un fichier au format Parquet rend DuckDB particulièrement approprié pour le traitement de données volumineuses (voir les éléments plus techniques, ci-dessous). Avec ce logiciel, les données du recensement peuvent être lues en quelques secondes alors qu’il fallait plusieurs dizaines de minutes dans les précédents formats. Pour des utilisateurs de l’écosystème de l’open data ou pour des organisations dont le patrimoine de données prend plus la forme de fichiers que de bases de données PostGreSQL, DuckDB est une opportunité technologique permettant de valoriser des données dont le traitement et la diffusion nécessitait jusqu’à présent des ressources computationnelles importantes.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\nRessources techniques:\n\nUn notebook  sur DuckDB issu d’une formation de l’Insee donnée à la BCEAO ;\nUne fiche sur Arrow, l’écosystème sous-jacent à DuckDB, dans la documentation collaborative utilitR ;\nLe post de blog sur la librairie  Polars, une approche alternative à DuckDB ;\nL’explorateur de données du SSPCloud qui repose sur DuckDB.\n\nDonnées diffusées par la statistique publique au format Parquet :\n\nLes données du Répertoire Electoral Unique ;\nLe guide d’utilisation des données du recensement de la population au format Parquet sous forme de billet de blog. Voir aussi l’infolettre #16 ;\nLes données de la délinquance enregistrée par la police et gendarmerie nationales publiées par le service statistique ministériel de la sécurité intérieur publiées au format Parquet.\n\nSur le format Parquet :\n\nUn article sur le format Parquet dans le Courrier des stats n°9 écrit par Alexis Dondon et Pierre Lamarche ;\nLe blog d’Eric Mauvière qui présente une série d’articles sur le format Parquet ;\nLa présentation de Romain Lesur sur le sujet pour l’atelier Modernisation of Official Statistics de l’UNECE.\n\n\n\n\n\n\n\n\n\nAstuceDes éléments plus techniques sur la gestion de la volumétrie des données\n\n\n\n\n\nIl existe principalement deux approches pour stocker, organiser et mettre à disposition des jeux de données structurés sous forme tabulaire : les fichiers et les bases de données relationnelles.\nLes bases de données relèvent d’une approche systémique. Un système de gestion de base de données (SGBD) est un logiciel qui gère à la fois le stockage d’un ensemble de données reliées, permet de mettre à jour celles-ci (ajout ou suppression d’informations, modification des caractéristiques d’une table…) et qui gère également les modalités d’accès à la donnée (type de requête, utilisateurs ayant les droits en lecture ou en écriture…). L’un des logiciels les plus connus dans le domaine est PostgreSQL.\nD’un autre côté, le stockage de données tabulaires sous forme de fichiers offre une approche plus décentralisée et flexible. Par rapport à des bases de données, les fichiers sont plus faciles à créer, partager et stocker et ne nécessitent pas systématiquement des logiciels spécialisés pour leur manipulation. Le stockage sous la forme de fichier consiste à organiser l’information présente dans un jeu de données dans des fichiers, de manière brute. Ces données peuvent être analysées sans recourir à un logiciel spécialisé. Même dans le cadre de formats propriétaires, comme le .xlsx ou .sas7bdat, le fait d’avoir une certaine forme de standardisation rend possible, même si ce n’est jamais parfaitement fiable, de lire ces données avec un autre logiciel que celui prévu initialement.\nLa logique de la base de données est donc très différente de celle du fichier. Par rapport à une base de données, l’approche des fichiers présente plusieurs avantages, à condition de privilégier des formats libres.\nEn premier lieu, les fichiers sont moins adhérents à un logiciel gestionnaire. Une transition d’un logiciel de traitement vers un autre n’implique pas de changer la source brute. En outre, alors que le traitement des bases de données nécessite l’intermédiation du logiciel de gestion adapté, les utilisateurs de Python ou R peuvent utiliser des fichiers à partir d’une librairie, donc un système beaucoup plus léger, qui sait comment transformer la donnée pour la retravailler depuis Python ou R.\nPour ces raisons, entre autres, il est plus pratique pour des utilisateurs finaux de données d’avoir accès à des fichiers plutôt qu’à des bases de données, à condition d’avoir les ressources computationnelles suffisantes pour pouvoir traiter ces fichiers.\nNéanmoins, cette condition d’accès à des ressources computationnelles suffisantes peut représenter une contrainte limitante dans un environnement où les données sont de volume croissant. Dans les environnements où la volumétrie des données était importante, les bases de données ont connu une certaine popularité puisqu’elles permettaient de gérer efficacement de grandes quantités de données. Comme, de plus, les bases de données offraient une gestion plus fine et fiable des droits d’accès et d’écriture sur les bases que ne le permettaient des fichiers, cette approche a pu connaître une certaine popularité.\nLe développement conjoint de formats de stockages orientés objets (comme le protocole S3, utilisé par les systèmes cloud modernes à l’image du SSPCloud) et d’outils de traitement efficaces comme DuckDB permet d’associer les avantages de ce dernier à ceux d’un système cohérent de fichiers partagés (lecture/écriture optimisées, dissociation des utilisateurs pouvant lire et écrire un fichier…).\nTechniquement, DuckDB fonctionne de manière optimale avec des fichiers au format Parquet. Ce format de données, orienté colonne, permet en effet d’optimiser des traitements classiques des data scientists : sélectionner seulement certaines colonnes d’un jeu de données, regrouper des données pour faire des calculs d’agrégats, etc.\n\n\n\nUne illustration du principe du stockage orienté colonne (Source: Michael Berk)\n\n\nPar exemple, dans le schéma ci-dessus, si on ne s’intéresse qu’aux dates enregistrées, il suffit de ne prendre que le bloc de données ad hoc. Il n’est pas nécessaire de lire tout le fichier pour ne garder que les dates, comme ce serait le cas avec un format CSV."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html#de-la-data-science-depuis-un-navigateur",
    "href": "infolettre/infolettre_17/index.html#de-la-data-science-depuis-un-navigateur",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "De la data science depuis un navigateur",
    "text": "De la data science depuis un navigateur\nLe gain de popularité de DuckDB au cours de l’année 2023 s’explique en partie grâce à sa version web qui permet d’exécuter des traitements de données par le biais de navigateurs web, sans avoir à installer de logiciel spécialisé comme Python  ou . C’est une approche typique du web assembly qui consiste à mettre à disposition des logiciels de calculs scientifiques par le biais d’un simple navigateur grâce à Javascript , qui est disponible sur tout navigateur. Cette approche est intéressante pour les institutions proposant des data visualisations car elle peut permettre de mettre en oeuvre des manipulations de données complexes directement depuis la source brute par le biais du navigateur, donc sans recourir à des serveurs externes hébergés dans des cloud.\n\n\n\n\n\n\nNoteFaire du  depuis le navigateur avec webR\n\n\n\n\n\nIl est maintenant possible de faire du R directement depuis un navigateur web grâce à webR, une librairie développée par Posit en 2023 et qui porte la grammaire R dans le navigateur sans recourir à Shiny (exemple à tester 👇️).\nL’idée est que le code d’analyse de données est en R mais qu’en arrière plan c’est du Javascript qui servira à l’exécution. La librairie est encore jeune mais celle-ci est prometteuse pour faciliter la transition entre du code d’analyse de données en R et une application interactive.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUn exemple plus complexe, utilisant le package ggplot2:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLa documentation officielle de WebR ;\nUn post sur le web assembly par ThinkR ;\nL’extension quarto-webR qui permet d’encapsuler du code webR dans un site web statique construit avec Quarto ;\nUne démonstration de WebR sur Observable."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html#prochains-événements-du-réseau",
    "href": "infolettre/infolettre_17/index.html#prochains-événements-du-réseau",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "Prochains événements du réseau",
    "text": "Prochains événements du réseau\n\n“La dataviz pour donner du sens aux données et communiquer un message” par Eric Mauvière (📅 29 février, 15h-16h)\nLe 29 février (15h - 16h), Eric Mauvière nous fera une présentation, avec de nombreux exemples issus de la statistique publique, de la manière dont une visualisation de données peut être construite pour transmettre un message clair aux lecteurs. Cette présentation permettra d’évoquer les bonnes pratiques et les outils simples pour construire des visualisations de données faciles à lire et à comprendre afin de rendre le message intelligible, efficace et utile à un large spectre de publics.\nCet événement aura lieu en visio sur Zoom, il est ouvert à tous les membres du réseau. Pour les agents de la DG de l’Insee, une retransmission en salle 4-C-458 est organisée pour assister à la présentation puis échanger à l’issue de celle-ci.\nÉric Mauvière est statisticien, passé par la diffusion et les études régionales de l’Insee. Il a ensuite créé le logiciel cartographique web Géoclip utilisé, par exemple, par le site https://statistiques-locales.insee.fr/. Plus récemment, il a participé à la conception du site https://vizagreste.agriculture.gouv.fr/, portail de visualisation de données du service statistique ministériel du ministère de l’Agriculture et de la Souveraineté alimentaire. Depuis 3 ans, au sein d’Icem7, il participe à la diffusion de connaissances sur les problématiques de dataviz et forme sur mesure en sémiologie graphique et analyse de données.\n\nInvitation outlook\nLien Zoom\n\n\n\nMasterclass datascientest\nDe nouvelles masterclass en partenariat avec datascientest seront organisées prochainement. Un questionnaire pour recenser les besoins, similaire à celui proposé l’an dernier, sera transmis prochainement pour évaluer le contenu à prioriser.\n\n\nProgramme 10%\nLa saison 3 du programme 10% démarre prochainement. Ce programme, issu des recommandations du rapport de l’Inspection Générale de l’Insee et de la DINUM, permet à des experts de la donnée de l’administration de monter en compétence en consacrant jusqu’à 10 % de leur temps de travail à des projets transversaux, à des formations ou encore à des moments informels qui favorisent l’échange entre pairs.\nLe premier événement de la saison aura lieu le 11 mars 2024 au Lieu de la Transformation Publique (Paris XV). L’objectif de cette journée est de définir collectivement les projets mutualisables pouvant entraîner des collaborations entre data scientists de différentes administrations pendant l’année 2024.\nPlus d’informations à venir sur le site du programme.\n\nInscription sur Eventbrite.\n\n\n\nAutoformation de découverte à Python\nDepuis quelques semaines, une nouvelle formation est apparue au catalogue de formation de l’Insee et sur le portail de formation du SSPCloud : une auto-formation de découverte de Python, construite et mentorée par les équipes innovation de l’Insee.\nChaque chapitre de formation, disponible sous la forme de notebooks Jupyter, peut être ouvert en un clic à partir du catalogue de formation du SSPCloud. Ces ressources sont disponibles en continu, au-delà des périodes délimitées pour la formation.\nDes cycles de 6 semaines pendant lesquels les participants peuvent bénéficier d’un mentorat ont vocation à être organisés régulièrement. Les premiers formés selon cette modalité ont commencé leur apprentissage en ce début d’année 2024.\nCe système de mentorat prend deux formes :\n\ntout au long de la formation : vous pouvez poser toutes vos questions sur le canal Tchap dédié ; les mentors de la formation répondent rapidement, afin que vous ne restiez jamais bloqué ;\nponctuellement : une visio est réalisée si vous le désirez avec 1-2 mentors afin de pouvoir discuter plus en détail des problèmes que vous pouvez rencontrer.\n\nCe système de mentorat a l’avantage à la fois de favoriser une pédagogie par la pratique continue qui a fait ses preuves pour l’apprentissage des langages de programmation, tout en laissant à chacun la possibilité d’avancer à son rythme.\nLes dates des prochains cycles de mentorat seront prochainement communiquées. Il est néanmoins possible, en attendant, d’explorer les ressources disponibles sur le SSPCloud ainsi que rejoindre le canal Tchap SSPy - Formation \"Initiation à Python\" afin d’échanger sur celle-ci (poser des questions de compréhension, comprendre un bug, assister les collègues bloqués, etc.)."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html",
    "href": "infolettre/infolettre_15/index.html",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "",
    "text": "C’est la rentrée ! Comme les élèves qui reviennent sur les bancs des écoles, les modèles de machine learning ont périodiquement besoin de mettre à jour leurs connaissances.\nCette newsletter sera consacrée aux enjeux du ré-entrainement et de la spécialisation de modèles, une question d’actualité suite à la publication estivale de plusieurs grands modèles de langage (LLM) open source."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#un-entraînement-ex-nihilo-hors-de-portée",
    "href": "infolettre/infolettre_15/index.html#un-entraînement-ex-nihilo-hors-de-portée",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Un entraînement ex nihilo hors de portée",
    "text": "Un entraînement ex nihilo hors de portée\nSi de nombreuses tâches de modélisation ne nécessitent pas des modèles très sophistiqués, deux domaines de recherche - à savoir le traitement naturel du langage (NLP) et l’analyse d’image - ont connu ces dernières années des innovations importantes grâce à des réseaux de neurones à l’architecture de plus en plus complexe.\nPour être en mesure d’entraîner un modèle complexe, du type grand modèle de langage (LLM), il faut, a minima, disposer des intrants suivants:\n\nUn immense volume de données déstructurées. La constitution de ces corpus implique le moissonnage en masse de ressources en ligne, ce qui n’est pas sans poser des enjeux juridiques de propriété intellectuelle qui ne sont pas encore résolus. La récupération de ces données nécessite une bonne connaissance de la structure et la nature des données nécessaires pour entrainer un modèle ;\nDes ressources informatiques hors du commun. Les investissements importants pour les cartes graphiques (GPU), une matière première en pénurie et les coûts courants associés (électricité, maintenance…) font qu’une poignée d’acteurs du numérique disposent des moyens financiers adéquats pour entraîner un modèle ex nihilo. Cet article de Forbes évoque des montants de l’ordre de plusieurs dizaines voire centaines de millions de dollars.\nDes experts aux compétences à l’intersection entre la recherche en mathématique et informatique ainsi que des spécialistes en data engineering actuellement en pénurie sur le marché du travail.\n\nLa combinaison de ces facteurs rend difficile, si ce n’est impossible, l’entrainement ex nihilo de tels modèles par la majorité des acteurs de la donnée. Seule une poignée de centres de recherche diposent des ressources permettant d’entraîner ex nihilo ce type de modèles."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#la-réutilisation-en-pratique",
    "href": "infolettre/infolettre_15/index.html#la-réutilisation-en-pratique",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "La réutilisation en pratique",
    "text": "La réutilisation en pratique\nNéanmoins, les besoins d’utilisation de ces modèles dépassent le cercle des acteurs en mesure de les entraîner. En effet, les grands modèles de langage sont généralement entraînés sur des corpus génériques de langage naturel principalement issus d’internet (Table 1). Cela les rend capables de comprendre les interactions avec des utilisateurs, notamment leurs instructions (prompt) et d’interagir avec eux de manière assez naturelle.\nNéanmoins, pour des tâches très spécialisées ou alors face à des corpus particuliers, ces modèles génériques peuvent nécessiter d’être spécialisés pour obtenir de meilleures performances. Plusieurs types de techniques, de complexité graduelle, ont ainsi émergé pour être en mesure de réutiliser et améliorer un modèle pré-entraîné.\nUn frein à la réutilisation massive de modèles pré-entraînés est la nature propriétaire de certains modèles, dont les conditions de réutilisation peuvent être limitantes. Pour cette raison, l’émergence de modèles open source, dont la structure est plus transparente et dont les conditions de réutilisation sur des infrastructures internes sont moins restrictives, est devenu ces derniers mois un enjeu important dans l’écosystème de la donnée.\nLes discussions sur l’ouverture des modèles s’inscrivent dans le contexte d’un affrontement important entre deux visions du modèle économique du secteur numérique : si le co-créateur d’OpenAI a pu affirmer, pour justifier l’absence de transparence scientifique sur les modèles d’OpenAI “[on openly sharing research,] we were wrong”, un mémo interne de Google défendait quant à lui l’idée que les modèles open source sont amenés à prendre le dessus, car ils peuvent bénéficier à plus grande échelle du travail d’experts et de retours d’utilisateurs.\nLa publication cet été de deux modèles open source (LLaMA-2 par Meta et Falcon par l’Institut de l’innovation technologique d’Abu Dhabi) ouvre de nouvelles perspectives pour une réutilisation de modèles dans une infrastructure interne, à condition de disposer des ressources computationnelles suffisantes et d’une stratégie adaptée de ré-apprentissage.\nPour aller plus loin sur ce sujet, la suite de cette newsletter évoque des détails plus techniques. La masterclass sur le sujet du fine-tuning que nous organisons avec datascientest (plus d’informations 👇️) permettra également d’approfondir cette question.\n\n\n\n\n\n\nNoteDérouler pour en savoir plus sur le corpus d’entraînement de Falcon\n\n\n\n\n\n\n\n\nTable 1: Corpus d’entraînement de Falcon 180B\n\n\n\n\n\n\n\n\n\nSource de données\nProportion dans le corpus\n\n\n\n\nRefinedWeb-English (webscraping)\n75%\n\n\nRefinedWeb-Europe (webscraping)\n7%\n\n\nLivres\n6%\n\n\nSites de conversations (Reddit, StackOverflow, HackerNews…)\n5%\n\n\nCode (Github…)\n5%\n\n\nDocuments techniques (arXiv, PubMed…)\n2%"
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#de-nouveaux-grands-modèles-de-langage-llm",
    "href": "infolettre/infolettre_15/index.html#de-nouveaux-grands-modèles-de-langage-llm",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "De nouveaux grands modèles de langage (LLM)",
    "text": "De nouveaux grands modèles de langage (LLM)\nPas de vacances pour les principaux acteurs de la data science ! Ce champ de recherche appliquée continue à connaître une actualité dense avec la publication, cet été, de deux modèles importants:\n\nLLaMA-2 par Meta, disponible en versions 7B, 13B et 70B c’est-à-dire, respectivement, 7, 13 et 70 milliards de paramètres. Dans le domaine des LLM actuels, c’est donc un modèle plutôt minimaliste (GPT-3 comportait 175 milliards de paramètres, GPT-4 en comporterait 1.7 trillions soit 1700 milliards). Le site web LeBonLLM propose, déjà, des exemples de fine tuning de LLaMa ;\nFalcon par l’Institut d’Innovation et de Technologie d’Abu Dhabi. Falcon-40B avait déjà connu, cet été, un engouement important en se plaçant en tête des réutilisations sur HuggingFace. Falcon 180B, sorti il y a quelques jours, rapproche les performances de celles des modèles propriétaires.\n\nCes deux modèles permettent d’envisager des réutilisations après un ré-entrainement sur des jeux de données ad hoc pour améliorer leurs performances (technique du fine tuning). Ceci est possible grâce à leurs licences permissives de réutilisation. Celle de Falcon est assez standard puisqu’il s’agit d’une Apache 2.0. Celle de LLaMA-2 est quant à elle moins traditionnelle. La réutilisation est libre, y compris à des fins commerciale, sauf pour les gros acteurs du numérique, globalement les concurrents de Meta :\n\n\nAdditional Commercial Terms. If, on the Llama 2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n\nLicence de LLaMa-2 sur Github"
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#le-réentrainement-des-modèles",
    "href": "infolettre/infolettre_15/index.html#le-réentrainement-des-modèles",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Le réentrainement des modèles",
    "text": "Le réentrainement des modèles\nL’ouverture de ces modèles laisse envisager des réutilisations sur de nouveaux jeux de données dans des infrastructures internes. Cet été, Andrew Ng, dans sa newsletter, est revenu sur les méthodes pour affiner les performances d’un modèle sur des données qu’il n’a pas rencontrées dans son corpus d’entraînement.\n\nLa technique la plus simple est d’affiner les instructions (prompt) fournies à un modèle. Pour faire l’analogie avec l’apprentissage humain, pour obtenir une réponse mieux ciblée à une question, il est souvent nécessaire de reformuler une question. Par exemple, lors d’une interaction avec une IA assistante de code, il peut être utile de guider un LLM avec une instruction “as a data scientist” ;\nFournir quelques exemples à un modèle (few shot learning). De même qu’avec les humains, fournir un petit nombre d’exemples peut suffire à un modèle, par un raisonnement inductif, à comprendre et répondre de manière juste à son instructeur. Selon la difficulté de la tâche à mettre en oeuvre, il peut suffire de très peu de cas pour spécialiser un modèle en modifiant les dernières couches du réseau de neurone.\nFournir de nouvelles sources à un modèle avant de l’interroger (retrieval augmented generation). Cette technique consiste à enrichir la base de connaissance d’un modèle pré-entraîné avec un nouveau corpus puis l’interroger sur la même thématique afin, par exemple, d’obtenir une synthèse ou alors une information contenue dans les documents mais nécessitant un temps d’extraction non négligeable à un humain. Dans cette approche, il ne s’agit pas de réentrainer le modèle pour mettre à jour ses paramètres mais de lui donner plus de contexte pour améliorer la pertinence des prédictions ou réponses du modèle. Pour continuer l’analogie avec l’apprentissage humain, cette technique se rapproche d’une situation où un humain assimile une bibliographie pour rentrer dans un nouveau sujet.\nRéentrainement par spécialisation (fine tuning) pour affiner le modèle sur une tâche donnée. Il s’agit d’une approche qui peut ressembler à l’apprentissage d’une nouvelle langue pour un humain: à partir d’un certain stock de connaissances antérieures (une langue natale), on accumule en série des exemples bien choisis pour améliorer la compréhension d’une autre langue. Cette approche permet une économie de ressources puisqu’elle consiste à spécialiser un modèle généraliste mais nécessite que la nature du problème pour lequel est ré-entrainé un modèle ressemble à celle pour lequel le modèle a été entraîné. De même qu’essayer de transposer des règles d’une langue latine aidera peu à apprendre le japonais, spécialiser un modèle d’analyse d’image pour une tâche de classification de données textuelles sera inefficace.\n\nLe fine tuning est ainsi une solution intéressante à condition d’avoir testé si des approches plus simples n’apportent pas déjà des solutions satisfaisantes.\nDe plus, pour être en mesure de fine tuner un modèle, outre l’accès à des ressources computationnelles conséquentes (mais tout de même moindres qu’un entraînement ex nihilo), beaucoup de méthodes nécessitent de disposer de données labellisées, c’est-à-dire impliquent de posséder une base de données permettant de juger de la qualité des prédictions du modèle. Pour pallier cette absence, il est possible de mettre en oeuvre un processus humain d’annotation et fournir au modèle ces évaluations pour l’amener à s’améliorer (technique nommée reinforcement learning from human feedback).\nComme l’évoque encore Andrew Ng, l’accès à des modèles pré-entraînés change le cycle de développement des projets utilisateurs d’IA. Ces projets ne consistent plus, comme les logiciels classiques, à développer en amont des spécifications puis déployer un modèle correspondant à celles-ci mais, au contraire, à commencer par mettre en oeuvre rapidement un premier modèle dont le comportement sera évalué et amélioré en continu par des méthodes comme l’apprentissage par renforcement."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#mc-datascientest",
    "href": "infolettre/infolettre_15/index.html#mc-datascientest",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Masterclass datascientest sur le fine-tuning",
    "text": "Masterclass datascientest sur le fine-tuning\n\n\n\n\n\nNotre cycle de masterclass organisées en lien avec datascientest continue !\nAprès avoir exploré en détail les thématiques du traitement automatique du langage et de l’analyse d’image, nous progressons dans notre parcours avec le sujet du fine tuning.\nAu programme:\n\nRéutilisation de transformers (BERT) et de LLM (LLaMA) avec les librairies d’HuggingFace ;\nFine tuning de ces modèles.\n\nRendez-vous le 6 octobre de 10h à 12h ! Inscription ici"
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#autres-événements",
    "href": "infolettre/infolettre_15/index.html#autres-événements",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Autres événements",
    "text": "Autres événements\nQuelques événements ou informations intéressantes :\n\nHackathon velib, fermeture des inscriptions le 29 septembre ;\nHackathon de l’ONU: fermeture des inscriptions à la fin du mois ;\nPrix du jeune statisticien de l’IAOS : article à envoyer avant le 10 février 2024.\n\nLes personnes intéressées par former une équipe pour les hackathons peuvent contacter contact-ssphub@insee.fr."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#rejoindre-le-salon-tchap-ssp-hub",
    "href": "infolettre/infolettre_15/index.html#rejoindre-le-salon-tchap-ssp-hub",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Rejoindre le salon Tchap SSP Hub",
    "text": "Rejoindre le salon Tchap SSP Hub\nPour échanger autour des activités du réseau et, plus largement, discuter entre pairs des sujets data science, il existe un salon SSP Hub dans la messagerie sécurisée de l’État Tchap. Celui-ci réunit plus de 250 personnes et permet des échanges plus directs, plus fréquents et plus informels que la liste de diffusion mail.\nSi vous avez un compte sur Tchap, vous pouvez rejoindre ce salon en cherchant celui-ci par son nom « SSP Hub ». En cas de problème pour le rejoindre, n’hésitez pas à envoyer un mail à contact-ssphub@insee.fr."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html",
    "href": "infolettre/infolettre_13/index.html",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "",
    "text": "Note\n\n\n\nVous désirez intégrer la liste de diffusion ? L’inscription se fait ici."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#un-foisonnement-davancées",
    "href": "infolettre/infolettre_13/index.html#un-foisonnement-davancées",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Un foisonnement d’avancées…",
    "text": "Un foisonnement d’avancées…\nL’actualité des modèles d’intelligence artificielle est très chargée (résumé des annonces du mois de mars) et cette lettre se concentrera sur les actualités majeures.\nGithub Copilot, l’assistant de code de Microsoft, devrait prochainement utiliser ChatGPT. Après ChatGPT, le navigateur Bing s’enrichit également d’un créateur d’image basé sur Dall-E. Pendant ce temps, Midjourney (une des principales alternatives de création d’images) sort sa V5, améliorant encore le réalisme du rendu.\nFace à la recrudescence des montages utilisant Midjourney, qu’il s’agisse de deep fakes ou d’images à vocation humoristique, Le Monde a publié un guide pour reconnaître une image générée par une IA.\n\n\n\nExemple d’image générée par IA (source: Reddit)"
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#qui-alimentent-les-débats-autour-de-la-place-de-lia-dans-la-société-et-notre-économie-numérique",
    "href": "infolettre/infolettre_13/index.html#qui-alimentent-les-débats-autour-de-la-place-de-lia-dans-la-société-et-notre-économie-numérique",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "… qui alimentent les débats autour de la place de l’IA dans la société et notre économie numérique…",
    "text": "… qui alimentent les débats autour de la place de l’IA dans la société et notre économie numérique…\nAlors que ChatGPT est déjà utilisé par plus de 100 millions d’utilisateurs, seulement quatre mois après sa sortie, il est légitime de se poser la question, comme le fait Gaspard Koenig dans Les Echos, du rapport à la vérité des IA génératrices et des implications sociétales de la généralisation de ces assistants.\nLes prospectivistes, après s’être consacrés au bitcoin et à la blockchain, comme Jean Detrez - le héros de Jean Philippe Toussaint -, commencent à proposer des évaluations des conséquences économiques de cette (r)évolution. Dernier chiffre en date : d’après Goldman Sachs, 300 millions d’emplois au niveau mondial pourraient être supprimés ou amenés à évoluer.\nPar ailleurs, la question de l’ouverture des corpus ayant servi à entraîner ces modèles ou de la licence à réutiliser des modèles pré-entraînés est importante à plusieurs égards. D’abord, dans une perspective scientifique, il apparaît compliqué d’évaluer la qualité d’un modèle ou lui proposer des alternatives sans accès à des données scientifiques de base comme le nombre de paramètres (inconnu pour GPT-4 par exemple). Mais c’est aussi une question économique : si ces outils deviennent de plus en plus incontournables, quel sera le business model de ce secteur ? Ces services resteront-ils gratuits avec en contrepartie une réutilisation des données fournies, potentiellement opaque et difficilement contrôlable par l’utilisateur, ou seront-ils monétisés ?\nLa publication en open source de modèles de langage est donc un enjeu d’indépendance pour les organisations intéressées par l’utilisation de chatbots ou l’extraction d’information. Cette publication continue à suivre un rythme presque quotidien. Cet état de l’art des modèles publiés en open source publié en janvier est déjà largement dépassé. Et celui-ci ne sera probablement plus à jour tout aussi rapidement.\n\n\n\n\n\nDans la dernière quinzaine, l’une des principales annonces est la publication par LAION d’OpenFlamingo, une version open source de Flamingo, modèle développé par DeepMind (filiale de Google) pour décrire de manière automatique une scène présente sur une image et offrir des informations contextuelles.\nAfin de pouvoir intégrer à la fois des fonctionnalités de reconnaissance d’image et textuelle, celui-ci s’appuie sur des composantes open source pour les modèles de langage et de reconnaissance d’image et sur un jeu de données ouvertes. Des exemples de réutilisation en Python sont disponibles sur le dépôt Github.\nDans le registre IA ouverte, une équipe de chercheurs de plusieurs universités américaines a mis en oeuvre un chatbot ouvert, à partir des modèles LLaMA (Meta, voir Newsletter #11) ou Alpaca (Stanford) : Vicuna. Ce chatbot généraliste permet à un internaute de discuter sur une grande variété de sujets. En plus de s’appuyer sur des modèles ouverts, ce qui peut faciliter sa ré-utilisation dans un cadre interne, ce chat présente un avantage technique puisqu’il s’appuie sur des modèles de langage plus économes en ressources que les modèles type GPT-4.\nDans le même temps, Databricks s’est également appuyé sur LLaMA et Alpaca pour proposer un autre modèle de langage ouvert avec lequel il est possible d’échanger. Ce modèle s’appelle Dolly et est pensé comme premier clone du modèle Alpaca. Dolly peut être entrainé en 30 minutes sur un corpus massif et ne présente “que” 6 milliards de paramètres (qu’il hérite d’Alpaca) ce qui en fait, dans l’écosystème actuel des modèles de langage (LLM), un nain : à titre de comparaison GPT-3 comporte 175 milliards de paramètres et le nombre de paramètres de GPT-4 est inconnu mais pourrait être de l’ordre de la centaine de trillions.\nMozilla rejoint également le bal en investissant 30 millions de dollars pour lancer la startup Mozilla.ai. Pour Mozilla, cette startup sert à adapter la philosophie à l’origine du navigateur Firefox au développement d’intelligences artificielles : proposer des outils ouverts indépendants des principaux acteurs marchands du numérique, sur le modèle des communs plutôt que du bien privé.\nLa question de la sécurité et de la confidentialité des informations fournies à OpenAI a également été au centre de l’attention au cours de la dernière quinzaine. OpenAI a révélé une faille de sécurité à l’origine d’une fuite de données. Cette question de la confidentialité des informations fournies à ChatGPT a d’ailleurs amené la CNIL italienne à demander le blocage temporaire de l’outil d’OpenAI (voir ici).\nCette interdiction prend place dans un contexte de discussions intenses autour de la place à venir dans nos sociétés de ces robots conversationnels. Une lettre ouverte au fort écho médiatique publiée par des figures de la tech (dont Elon Musk) et des universitaires réclame un moratoire dans le développement de nouveaux modèles d’intelligence artificielle, dont les signataires soulignent à quel point il est difficile de les “comprendre, prédire ou contrôler de manière fiable”.\nCette lettre a été rapidement critiquée. En premier lieu par Andrew Ng qui souligne qu’en plus de l’impossibilité pratique de mettre en oeuvre un tel moratoire dans le cadre d’une recherche privée, ce type de sursis retarde la recherche sur des applications bénéfiques, notamment dans le domaine de la santé. Mais la critique est aussi venue de Timnit Gebru (DAIR), Emily Bender (University of Washington), Angelina McMillan-Major (University of Washington) et Margaret Mitchell (Hugging Face), autrices citées dans la lettre en référence au concept de “Stochastic Parrots” (les modèles de langage répètent des séquences de formes linguistiques comme des ensemble de mots observés dans les corpus d’apprentissage, en fonction de critères probabilistes sur la façon dont ces informations se combinent, mais sans aucune référence au sens)."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#mais-ne-nous-font-pas-oublier-certaines-autres-nouveautés-dignes-dintérêt",
    "href": "infolettre/infolettre_13/index.html#mais-ne-nous-font-pas-oublier-certaines-autres-nouveautés-dignes-dintérêt",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "… mais ne nous font pas oublier certaines autres nouveautés dignes d’intérêt",
    "text": "… mais ne nous font pas oublier certaines autres nouveautés dignes d’intérêt\nL’intensité de l’actualité autour de ChatGPT occulte beaucoup de faits qui auraient, dans un autre contexte, amené à des discussions passionnées.\nAinsi, la publication en open source de l’algorithme de recommandation de Twitter permet de mieux comprendre la manière dont fonctionne l’algorithme de recommandation de Twitter, notamment la manière dont des bulles de filtre peuvent advenir après avoir cliqué sur un post par curiosité.\nLa publication de ce code prend place quelques jours après la découverte qu’une partie importante du code de Twitter était déjà sur Github, sans doute suite à une fuite après l’un des licenciements massifs des derniers mois.\nPeut-être n’avez-vous pas remarqué mais Twitter n’a plus le même logo depuis lundi. L’oiseau bleu a été remplacé par un chien (un Shiba Inu), logo d’une cryptomonnaie, le dogecoin, dont Elon Musk avait fait une promotion controversée et dont les cours ont connu une envolée depuis.\nLe logiciel de création d’applications Docker (principal logiciel de conteneurisation) noue un partenariat avec HuggingFace, qui rassemble une immense bibliothèque de modèles, pour faciliter la réutilisation de modèles de deep learning. Avec cette approche, il est plus facile de proposer aux ré-utilisateurs de modèles des applications prêtes à l’emploi pour tester ou enrichir un modèle sur des infrastructures compatibles avec la technique de la conteneurisation, approche utilisée par les principales plateformes de data science modernes (notamment par celles s’appuyant sur le logiciel Onyxia).\nLa plateforme Observable propose un comparatif des principales syntaxes de manipulation de données (JavaScript, Python, R et SQL).\nEnfin, si vous appréciez les cartes, un hashtag à suivre est le #MapPromptMonday."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#première-journée-du-réseau-le-17-avril",
    "href": "infolettre/infolettre_13/index.html#première-journée-du-réseau-le-17-avril",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Première journée du réseau le 17 avril",
    "text": "Première journée du réseau le 17 avril\n\n\n\n\n\nNous rappelons la journée du réseau le 17 avril, en présentiel 📅 (Newsletter #12). Les places en présentiel sont presque épuisées, ne tardez pas à vous inscrire ! Si vous désirez tout de même suivre les échanges, inscrivez-vous à la retransmission par Zoom."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#les-masterclass-avec-datascientest-continuent",
    "href": "infolettre/infolettre_13/index.html#les-masterclass-avec-datascientest-continuent",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Les masterclass avec DataScientest continuent",
    "text": "Les masterclass avec DataScientest continuent\nNos cycles de masterclass datascientest continuent ! Les cycles parallèles NLP et analyse d’image continuent.\nAu programme:\n\nAnalyse d’image, niveau confirmé (📅 14 avril, 10h-12h )\nNLP, niveau avancé (📅 12 mai, 10h-12h )\nAnalyse d’image, niveau avancé (📅 9 juin, 10h-12h )\n\nInscription ici !"
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#replay-bonnes-pratiques-pour-la-mise-en-production-de-projets-data-science-30-mars",
    "href": "infolettre/infolettre_13/index.html#replay-bonnes-pratiques-pour-la-mise-en-production-de-projets-data-science-30-mars",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Replay “Bonnes pratiques pour la mise en production de projets data science” (30 mars)",
    "text": "Replay “Bonnes pratiques pour la mise en production de projets data science” (30 mars)\nLe replay de la présentation succincte du contenu du cours de l’ENSAE “Bonnes pratiques pour la mise en production de projets data science” ayant eu lieu dans le cadre du programme 10% (voir Newsletter #12) est disponible sur le site du programme 10%."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#replay-de-lévénement-autour-de-locrisation-avec-christopher-kermorvant-29-mars",
    "href": "infolettre/infolettre_13/index.html#replay-de-lévénement-autour-de-locrisation-avec-christopher-kermorvant-29-mars",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Replay de l’événement autour de l’OCRisation avec Christopher Kermorvant (29 mars)",
    "text": "Replay de l’événement autour de l’OCRisation avec Christopher Kermorvant (29 mars)\nChristopher Kermorvant, chercheur spécialisé en OCRisation et fondateur de Teklia, a proposé au réseau une présentation très pédagogique sur l’extraction de texte avec des méthodes de deep learning.\nA partir de l’exemple de recensements de la fin du XIXe siècle, nous avons ainsi bénéficié d’une excellente introduction à l’histoire des techniques d’OCRisation et la manière dont aujourd’hui ces modèles fonctionnent en associant reconnaissance d’image et analyse textuelle.\nLe replay est ici !"
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#replay-de-la-présentation-de-la-documentation-collaborative-carpentries-28-mars",
    "href": "infolettre/infolettre_13/index.html#replay-de-la-présentation-de-la-documentation-collaborative-carpentries-28-mars",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Replay de la présentation de la documentation collaborative Carpentries (28 mars)",
    "text": "Replay de la présentation de la documentation collaborative Carpentries (28 mars)\nKate Burnett-Isaacs, de Statistics Canada, nous a présenté l’initiative Meta Academy / Carpentries permettant de construire une documentation francophone de référence sur R, Python et Git à destination des utilisateurs de données.\nLe replay est ici !"
  },
  {
    "objectID": "infolettre/infolettre_11/index.html",
    "href": "infolettre/infolettre_11/index.html",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "",
    "text": "Note\n\n\n\nVous désirez intégrer la liste de diffusion ? L’inscription se fait ici.\nCe numéro reprend le format expérimenté à la fin de l’année 2022: un résumé des dernières actualités du monde de la data science précède la présentation plus classique des nouvelles du réseau."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#chatgpt-continue-de-faire-parler",
    "href": "infolettre/infolettre_11/index.html#chatgpt-continue-de-faire-parler",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "ChatGPT continue de faire parler",
    "text": "ChatGPT continue de faire parler\n\n\n\n\n\nL’actualité est encore largement dominée par les discussions autour de ChatGPT. Les débats continuent notamment sur la pertinence de ce type d’outil dans l’enseignement (cf. Télérama). L’article du New Yorker “ChatGPT is a blurry JPEG of the web” propose une analyse en profondeur de la manière dont les modèles de langage reconstruisent des réponses originales en mélangeant des corpus rencontrés lors de la phase d’entraînement.\nAlors qu’il est difficile d’avoir des informations sur le corpus de ChatGPT ou les méthodes d’apprentissage mises en oeuvre, des développeurs ont mis en place de nombreux chatbots thématiques sur https://beta.character.ai/ s’appuyant sur des modèles de langage ouverts.\nCe mois-ci, Facebook-Meta est également rentré dans la danse avec son modèle LLaMa, ouvert aux chercheurs et ayant vocation à fonctionner sur des installations moins gourmandes en ressources que les modèles GPT ou PaLM.\nA peine deux mois après le lancement tonitruant de ChatGPT, et la création d’un “code rouge” du côté de Google, le lancement difficile par Microsoft d’une version de test de son robot conversationnel dans son navigateur Bing montre que l’intégration à un moteur de recherche traditionnel n’est pas évidente, notamment pour éviter la diffusion de fausses informations. Alors que ChatGPT avait bénéficié d’un gros travail humain pour cadrer son comportement, il semblerait que l’IA de Bing soit moins consensuelle dans ses propos.\nPendant ce temps, la recherche sur les modèles de diffusion continue à avancer à grande vitesse. La dernière innovation est la capacité à reconstruire des images à partir d’IRM de l’activité du cerveau grâce au modèle Stable Diffusion."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#traitement-de-données-tabulaires-arrow-et-polars-au-centre-du-jeu",
    "href": "infolettre/infolettre_11/index.html#traitement-de-données-tabulaires-arrow-et-polars-au-centre-du-jeu",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Traitement de données tabulaires: Arrow et Polars au centre du jeu",
    "text": "Traitement de données tabulaires: Arrow et Polars au centre du jeu\n\n\n\n\n\nDu côté des données tabulaires plus traditionnelles, Apache Arrow continue de s’affirmer comme un incontournable.\nLa version 2.0 de Pandas qui vient de sortir permet de plus facilement s’appuyer, en arrière-plan, sur Arrow plutôt que Numpy qui offre des performances et des fonctionnalités limitées sur données non numériques (cf. https://datapythonista.me). Il s’agissait d’une des limites majeures de Pandas, identifiées dès 2017 par son créateur Wes McKinney (voir ici) qui est également très impliqué dans le développement d’Apache Arrow.\nLa librairie Polars connait une certaine hype et va sans doute devenir dans les mois à venir une librairie incontournable, en alternative à Pandas. La dernière version de DuckDB (sortie mi-février) renforce l’interconnexion entre ces deux écosystèmes (exemples). Si vous voulez en savoir plus sur Polars, il est recommandé de suivre l’évolution de la liste “Awesome Polars” faite par Damien Dotta (relayée par la très bonne newsletter du site Data Elixir) et de lire le post à venir prochainement sur le blog de notre réseau.\nPar ailleurs, l’article de Jordan Tigani “Big data is dead” vaut le détour."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#huggingface.js-lapi-dhuggingface-directement-accessible-depuis-un-navigateur-web",
    "href": "infolettre/infolettre_11/index.html#huggingface.js-lapi-dhuggingface-directement-accessible-depuis-un-navigateur-web",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Huggingface.js: l’API d’HuggingFace directement accessible depuis un navigateur web",
    "text": "Huggingface.js: l’API d’HuggingFace directement accessible depuis un navigateur web\n\n\n\n\n\nHuggingface et Observable sont chacun devenus des incontournables dans leur domaine (voir notre newsletter de décembre). Alors un rapprochement entre ces deux univers, permettant d’utiliser de nombreux modèles d’apprentissage via l’API d’HuggingFace directement dans le navigateur, ça donne envie de s’amuser.\nLe notebook, disponible sur Observable, illustre la richesse des fonctionnalités disponibles."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#première-journée-du-réseau-en-avril",
    "href": "infolettre/infolettre_11/index.html#première-journée-du-réseau-en-avril",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Première journée du réseau en avril",
    "text": "Première journée du réseau en avril\n\n\n\n\n\nNotre réseau organise des événements virtuels depuis un an. Pour renforcer l’esprit communautaire, nous proposons une journée du réseau le 17 avril, en présentiel 📅.\nCet événement aura lieu dans le tiers-lieu la Tréso à Malakoff. Pendant la journée se succèderont plusieurs séquences pour construire ensemble le réseau, partager autour du sujet de la data science et échanger ensemble.\nLe programme et les modalités pratiques d’inscription seront communiqués prochainement ! Vous pouvez néanmoins déjà marquer la date dans votre calendrier."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#présentation-de-la-documentation-collaborative-carpentries",
    "href": "infolettre/infolettre_11/index.html#présentation-de-la-documentation-collaborative-carpentries",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Présentation de la documentation collaborative Carpentries",
    "text": "Présentation de la documentation collaborative Carpentries\nPour favoriser l’adoption des langages R, Python et Git dans les administrations, le programme ModernStat piloté par l’OCDE et Statistics Canada, a lancé un projet nommé Meta Academy et s’est rapproché de l’organisation américaine Carpentries dont l’objectif est de proposer des parcours progressifs de formation dans les langages open source, associés à des documentations disponibles de manière ouverte.\nL’absence de contenu en Français et l’orientation principalement académique des contenus a amené le programme ModernStat à proposer aux Carpentries de créer de nouveaux parcours de formations en Français. Ces programmes seraient créés par des membres de la communauté francophone des utilisateurs des langages R, Python et Git.\nKate Burnett-Isaacs, de Statistics Canada, nous présentera l’initiative le mardi 28 mars à 15h 📅 (invitation Outlook). La présentation aura lieu en Anglais et sera suivie d’un échange (Français et Anglais possibles). Si vous êtes intéressés par la manière dont les nombreux contenus créés par les membres du réseau pourraient prendre place dans ce cadre, n’hésitez pas à venir pour en savoir plus ! Informations pratiques ici !"
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#masterclass-datascientest",
    "href": "infolettre/infolettre_11/index.html#masterclass-datascientest",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Masterclass datascientest",
    "text": "Masterclass datascientest\n\n\n\n\n\nLes inscriptions à nos cycles de masterclass datascientest sont toujours ouvertes ! Pendant le mois de mars, nous continuerons d’avancer dans les deux cursus parallèles.\nEn premier lieu, la masterclass d’introduction au deep learning (10 mars de 10h à 12h 📅) permettra d’initier notre parcours de computer vision avec la présentation de certains concepts centraux du deep learning (perceptron, convolution, transfer learning…).\nLa deuxième session mensuelle, ayant lieu le 24 mars de 10h à 12h 📅 sera elle l’occasion d’avancer dans notre parcours NLP grâce au sujet de la similarité textuelle et de la classification de textes avec des méthodes d’embeddings.\n\n\n\n\n\n\nNote\n\n\n\nPour vous inscrire, il suffit de remplir ce formulaire !"
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#onyxia",
    "href": "infolettre/infolettre_11/index.html#onyxia",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Onyxia",
    "text": "Onyxia\n\n\n\n\n\nLe dernier post sur le site web du réseau revient sur le projet Onyxia, le logiciel initié par l’équipe innovation de l’Insee et mis à disposition sur Github pour permettre à d’autres organisations de développer une infrastructure de data science à l’état de l’art.\nPour en savoir plus sur le contexte de naissance d’Onyxia, les choix techniques mis en oeuvre ou la communauté des réutilisateurs, c’est par ici."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#replay-de-lévénement-autour-des-packages-facilitant-laccès-à-lopen-data-de-linsee",
    "href": "infolettre/infolettre_11/index.html#replay-de-lévénement-autour-des-packages-facilitant-laccès-à-lopen-data-de-linsee",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Replay de l’événement autour des packages facilitant l’accès à l’open data de l’Insee",
    "text": "Replay de l’événement autour des packages facilitant l’accès à l’open data de l’Insee\n\n\n\n\n\nLe replay des présentations des packages Doremifasol (R) et Pynsee (Python) lors de notre événement du 13 février autour des packages facilitant l’accès à l’open data de l’Insee est en ligne !\nLa vidéo et les supports présentés sont mis à disposition sur le site web du réseau"
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#programme-10",
    "href": "infolettre/infolettre_11/index.html#programme-10",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Programme 10%",
    "text": "Programme 10%\nLa journée de lancement du programme 10% annonce une saison prometteuse ! Plus de 50 personnes, issues d’un large panel d’administrations, se sont rencontrées et ont échangées autour de projets mutualisables. Au final, une demi-douzaine de projets ont déjà été identifiés.\nSi vous n’étiez pas disponible lors de cette première journée, il est possible à tout moment de rejoindre cette communauté. Le prochain atelier a lieu le 14 mars au Lieu de la Transformation Publique 📅 !"
  },
  {
    "objectID": "infolettre/infolettre_09/index.html",
    "href": "infolettre/infolettre_09/index.html",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à ssphub-contact@insee.fr suffit\nLa rétrospective de l’année 2022 promettait une version plus personnalisée, inspirée des visualisations proposées par les réseaux sociaux pour synthétiser l’activité de leurs utilisateurs.\nCette newsletter un peu spéciale propose un retour sur la première année du réseau des data scientists de la statistique publique dont la préfiguration a commencé en mars 2022 et qui a été lancé officiellement en septembre. Vous pourrez retrouver à la fin de la newsletter des informations plus classiques: événements, retour sur les actions du réseau, formations, etc.\nElle permet aussi d’illustrer le potentiel d’outils qui ont été présentés dans la rétrospective de l’année 2022. Toutes les figures sont réactives, notamment quand vous passez votre souris dessus. Les principaux ingrédients qui ont été ici utilisés, et qui avaient été mentionnés dans la première partie de la rétrospective, sont Observable, Quarto et DuckDB. Les données sont stockées sur le système de stockage S3 du SSPCloud."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#lannée-du-réseau",
    "href": "infolettre/infolettre_09/index.html#lannée-du-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "L’année du réseau",
    "text": "L’année du réseau\nLe réseau comporte deux canaux de communication: une liste de diffusion mail et un canal de discussions instantanées. Intéressons nous d’abord à la liste de diffusion mail !\n\n\n\nPendant l’année 2022, 7 newsletters ont été diffusées par mail. Chacune a permis d’augmenter sensiblement le nombre de personnes dans la liste de diffusion. A la fin de l’année, il y avait 312 inscrits1 dans la liste de diffusion.\nLe réseau a organisé trois événements pendant l’année 2022. D’abord, avant l’été, deux open hours ont eu lieu. Cet événement informel prenant la forme de retour d’expérience a été l’occasion de discussions stimulantes autour de d’usage de la data science pour l’administration. En novembre, l’événement autour d’Observable animé par Nicolas Lambert a réuni près de 50 personnes."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#répartition-des-modes-daccès-au-réseau",
    "href": "infolettre/infolettre_09/index.html#répartition-des-modes-daccès-au-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Répartition des modes d’accès au réseau",
    "text": "Répartition des modes d’accès au réseau\nLe réseau propose deux canaux de diffusion de l’information: une liste de diffusion par mail et un canal de discussion instantanée qui utilise la messagerie sécurisée de l’Etat Tchap. Environ 55% des membres de la liste de diffusion (soit plus de 180 personnes) sont également inscrits sur le canal de discussion instantanée."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#composition-du-réseau",
    "href": "infolettre/infolettre_09/index.html#composition-du-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Composition du réseau",
    "text": "Composition du réseau\nLa diffusion d’informations par le réseau a permis de réunir des data scientists de 27 organisations différentes. L’Insee, qui représente 47% de l’effectif du réseau, est majoritaire. Suivent dans le palmarès, les services statistiques du Ministère de la Santé (DREES) et du Ministère du Développement Durable (SDES)."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#évolution-de-la-composition-du-réseau",
    "href": "infolettre/infolettre_09/index.html#évolution-de-la-composition-du-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Évolution de la composition du réseau",
    "text": "Évolution de la composition du réseau\nLa diffusion progressive d’informations par le biais des newsletters a permis de diversifier progressivement la composition de la liste de diffusion. Alors que la première newsletter de l’année 2022 avait été diffusée auprès de 14 institutions, ce sont des agents de 27 organisations qui ont reçues la dernière.\nLes événements organisés par le réseau ou les présentations spéciales, comme celle pour les administrateurs de l’INSEE en poste à l’ENSAE, ont également pu motiver des personnes à intégrer le réseau."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#programme-10",
    "href": "infolettre/infolettre_09/index.html#programme-10",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Programme 10%",
    "text": "Programme 10%\nLes membres du réseau des data scientists ont été particulièrement actifs dans le cadre du programme interministériel 10%, issu des recommandations d’un rapport INSEE-DINUM “Évaluation des besoins de l’État en compétences et expertises en matière de donnée”.\nLa saison 1 a donné sa chance à quatre projets, portés par différentes administrations. Si l’un d’eux existait déjà depuis plus de deux ans (projet Gouvdown), trois sont nés pour l’occasion, avec la mise en ligne de code immédiate (Cartiflette) ou postérieure au bootcamp de lancement (Socratext et matchSIRET) .\nTous les projets sont ouverts et disponible sur Github. Une statistique qui permet de représenter leur succès est le nombre de ⭐: c’est un peu un mélange entre un site en favori sous Firefox puisque cela permet de facilement retrouver un projet dans Github et le nombre de followers d’une page sur Facebook ou sur Twitter puisque cela permet de suivre l’activité d’un dépôt Github.\n\n\n\n\n\n\nNote\n\n\n\nCette visualisation fait appel à l’API Github. Si les figures ne s’affichent pas, cela peut être dû à un dépassement du nombre de requêtes par heure autorisées par l’API Github sans jeton. A l’heure actuelle, il n’existe pas encore de fonctionalité gratuite sous Observable pour stocker de manière sécurisée un jeton pour l’API Github.\n\n\nDérouler pour afficher une version non réactive\n\n\n\n\nProjet cartiflette\n\n\n\n\n\nProjet Socratext\n\n\n\n\n\nProjet Gouvdown\n\n\n\n\n\nProjet matchSIRET"
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#autres-actualités-du-réseau",
    "href": "infolettre/infolettre_09/index.html#autres-actualités-du-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Autres actualités du réseau",
    "text": "Autres actualités du réseau\n\nPrésentation de Gridviz par Julien Gaffuri\nPour rappel, le 20 Janvier 2023 de 11h à 12h30 Julien Gaffuri (Eurostat) viendra nous présenter la librairie open-source Gridviz. Réservez ce créneau pour découvrir cette librairie qui ouvre de nouvelles perspectives pour la mise à disposition de données géographiques !\nTélécharger l’invitation à l’événement sous format Outlook\n\n\n\nSource: Notebook Hello Gridviz par neocarto sur Observable\n\n\n\n\nPremière place européenne au hackathon Big Data de l’ONU\nLes résultats du hackathon big data de l’ONU, ayant eu lieu du 7 au Novembre 2022, ont été annoncés ! L’équipe Datadive - constituée de membres du réseau de l’INSEE, de la DGFIP et du CASD - est arrivée à la première place des équipes européennes 🎉.\n\n\nGit et bonnes pratiques: des formations de formateurs prévus pour les statisticiens publics\nLes nouvelles formations à Git et aux bonnes pratiques avec R, testées récemment à l’Insee et au service statistique du Ministère du Travail, la DARES, (voir newsletters de Novembre et Décembre), deviennent des formations nationales.\nPour pouvoir diffuser les bonnes pratiques favorisant le partage de codes et la qualité des projets statistiques, il est nécessaire d’avoir le plus d’enseignants possibles pour cette formation. Pour permettre cela, un appel à candidat pour une formation de formateurs a été diffusée à l’Insee et dans les services statistiques ministériels. Si vous êtes intéressés et ne l’avez pas reçu, n’hésitez pas à envoyer un mail à contact-ssphub@insee.fr.\nEn attendant, les supports de ces formations sont déjà disponibles sur inseefrlab.github.io/formation-bonnes-pratiques-git/ et sur inseefrlab.github.io/formation-bonnes-pratiques-R/. Les codes sources sont bien-sûr ouverts et disponibles sur Github, tant pour la première partie que pour la seconde. Ceux-ci sont construits collectivement, n’hésitez pas à suggérer des modifications depuis Github.\nUn site web plus complet devrait prochainement voir le jour pour accompagner cette formation. En complément de celui-ci, des éléments peuvent déjà être trouvés dans le cours de 3e année de l’ENSAE sur la mise en production de projets data science et dans la documentation collaborative utilitR."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#footnotes",
    "href": "infolettre/infolettre_09/index.html#footnotes",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nLes actions de communication du mois de janvier ont permis d’augmenter sensiblement le nombre de personnes dans cette liste (340 début janvier). Un retour spécial sur le mois de janvier sera l’occasion idéale pour une autre rétrospective quantitative.↩︎"
  },
  {
    "objectID": "infolettre/infolettre.html",
    "href": "infolettre/infolettre.html",
    "title": "Infolettres",
    "section": "",
    "text": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA\n\n\nInfolettre du mois d’octobre 2025\n\n\n\n\n\n25 oct. 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLa rentrée 2025: actualités, nouveautés, interview de rentrée\n\n\nInfolettre du mois de Septembre 2025\n\n\n\n\n\n29 sept. 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks\n\n\nInfolettre du mois de Mars 2024\n\n\n\n\n\n7 mars 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLe RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web\n\n\nInfolettre du mois de Février 2024\n\n\n\n\n\n20 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud\n\n\nInfolettre du mois de Décembre 2023\n\n\n\n\n\n21 déc. 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCoûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM\n\n\nInfolettre de rentrée, Septembre 2023\n\n\n\n\n\n10 sept. 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPropositions de lecture estivale\n\n\nInfolettre estivale, Juillet 2023\n\n\n\n\n\n1 juil. 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDes innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter\n\n\nInfolettre du mois d’Avril 2023\n\n\n\n\n\n1 avr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley\n\n\nInfolettre du mois de Mars 2023, deuxième quinzaine\n\n\n\n\n\n15 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web\n\n\nInfolettre du mois de Mars 2023\n\n\n\n\n\n1 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images\n\n\nInfolettre du mois de Février 2023\n\n\n\n\n\n30 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRetex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz\n\n\nInfolettre du mois de Janvier 2023\n\n\n\n\n\n10 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\nL’année 2022 dans le monde de la data science : IA, transformation de RStudio, Observable\n\n\nInfolettre du mois de Décembre 2022\n\n\n\n\n\n31 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\nArchive des infolettres et lettres Big Data\n\n\nLes infolettres et lettres Big Data antérieures 👵👴, avant la publication sous forme de blog\n\n\n\n\n\n31 août 2022\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "event/presentation-du-projet-meta-academy-carpentries/index.html",
    "href": "event/presentation-du-projet-meta-academy-carpentries/index.html",
    "title": "Présentation du projet Meta Academy - Carpentries",
    "section": "",
    "text": "Replay de l’événement:"
  },
  {
    "objectID": "event/presentation-des-packages-r-et-python-pour-acceder-a-lopen-data-de-linsee/index.html",
    "href": "event/presentation-des-packages-r-et-python-pour-acceder-a-lopen-data-de-linsee/index.html",
    "title": "Présentation des packages R et Python pour accéder à l’open data de l’Insee",
    "section": "",
    "text": "Présentation autour des packages développés par des data scientists pour faciliter la récupération des données officielles de l’Insee :"
  },
  {
    "objectID": "event/presentation-des-packages-r-et-python-pour-acceder-a-lopen-data-de-linsee/index.html#replay",
    "href": "event/presentation-des-packages-r-et-python-pour-acceder-a-lopen-data-de-linsee/index.html#replay",
    "title": "Présentation des packages R et Python pour accéder à l’open data de l’Insee",
    "section": "Replay",
    "text": "Replay"
  },
  {
    "objectID": "event/ocrisation-teklia/index.html",
    "href": "event/ocrisation-teklia/index.html",
    "title": "“OCRisation, état de l’art et projets auxquels participe Teklia” par Christopher Kermorvant",
    "section": "",
    "text": "Le replay de l’événement est disponible ci-dessous.\n\nLes slides présentées par Christopher Kermorvant sont aussi disponibles ici."
  },
  {
    "objectID": "event/2025-04-16-Parquet/index.html",
    "href": "event/2025-04-16-Parquet/index.html",
    "title": "Atelier - Comment récupérer des données sous format Parquet ?",
    "section": "",
    "text": "L’atelier a eu lieu le 16 avril 2025 (15h - 16h30), en présentiel à l’Insee et en distanciel pour les membres du réseau du SSP Hub. Environ 35 personnes ont participé de l’Insee (DG ou directions régionales), de différents services statistiques ministériels ou d’autres horizons. Merci à tous pour les échanges !\n\nSlides de la présentation\n\n\n\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"https://inseefrlab.github.io/ssphub-ateliers-slides/slides-data/parquet#/title-slide\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Voir les slides\n  &lt;/a&gt;\n&lt;/p&gt;`\n\n\n\n\n\n\n\n\nDocumentation de l’atelier & replay\nLe matériel lié à l’atelier, y compris le replay, est disponible ici. \n\n\nQuestions / contact\nSi vous avez la moindre question 🤨, n’hésitez pas à contacter 📧 contact-ssphub@insee.fr."
  },
  {
    "objectID": "event/2024-10-14-network-day/index.html",
    "href": "event/2024-10-14-network-day/index.html",
    "title": "Deuxième journée du SSPHub",
    "section": "",
    "text": "Les sessions plénières de la journée du réseau sont disponibles ci-dessous:\n\n\nSéquencement de la vidéo et slides\n\n\n00:00-03:30: Introduction de la journée par Romain Lesur\n03:30-46:30: “Le blé vu du ciel : images satellitaires et prédiction des rendements agricoles à l’échelle de la parcelle” (Service statistique ministériel du Ministère de l’Agriculture, de la souveraineté alimentaire et de la Forêt).\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/agriculture.pdf\",\n  \"Télécharger les slides du SSM Agriculture\"\n)\n\n\n\n\n\n\n\n46:30-83:00: “Identifier et classer les causes de décès en automatisant le traitement des certificats en langage naturel” (CépiDC, Inserm)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/cepidc.pdf\",\n  \"Télécharger les slides du CépiDC\"\n)\n\n\n\n\n\n\n\n86:00-123:00: Keynote de Pascal Rivière (chef de l’Inspection générale de l’Insee) “Data science et statistique publique : contexte institutionnel et évolutions”\n123:00-164:00: “Extraction automatisée de tableaux dans des PDF pour la construction de statistiques d’entreprises” (Insee)\n\n\ncreateButton(\n  \"https://ssplab.pages.lab.sspcloud.fr/table-extraction-evaluation/#/title-slide\",\n  \"Voir les slides de l'Insee\"\n)\n\n\n\n\n\n\n\n164:00-205:00: “Scraper et retravailler les offres d’emploi en ligne pour permettre des analyses fines du marché du travail, le projet JOCAS” (Dares: Service statistique ministériel du Ministère du Travail et de l’Emploi)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/jocas.pptx\",\n  \"Télécharger les slides de la DARES\"\n)\n\n\n\n\n\n\n\nA partir de 205:00: Keynote de Pierre Etienne Devineau (ex-DINUM) sur les enjeux rencontrés par le projet Albert autour de l’entraînement et de la mise en production de grands modèles de langage (LLM) francophones.\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/albert.pptx\",\n  \"Télécharger les slides de la keynote\"\n)\n\n\n\n\n\n\n\nQuelques photos de l’événement:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: \n\n\n\nLes slides des différentes présentations peuvent être téléchargées ⬆️\n\nfunction createButton(slides, message=\"Télécharger les slides\"){\n  const button = html`\n  &lt;p class=\"text-center\"&gt;\n    &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n      &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;${message}\n    &lt;/a&gt;\n  &lt;/p&gt;`\n  return button\n}\n\n\n\n\n\n\n\n\nRappel du programme de la journée\n\n\n9h30-10h: Accueil, moment de convivialité autour d’un café.\n10h-11h30: Retours d’expérience de projets innovants autour de la classification de textes ou d’images :\n\nLe blé vu du ciel : images satellitaires et prédiction des rendements agricoles à l’échelle de la parcelle (SSM Agriculture) ;\nIdentifier et classer les causes de décès en automatisant le traitement des certificats en langage naturel (CépiDC, Inserm).\n\n11h30-12h15: Atelier de réflexion autour des applications des méthodes de classification de textes ou d’images sur les données des * participants.\n12h15-14h: Pause déjeuner, moment de convivialité.\n14h-14h30: Pascal Rivière (Chef de l’inspection générale de l’Insee) interviendra sur le thème “Data science et statistique publique : contexte institutionnel et évolutions”.\n14h30-16h: Retours d’expérience de projets faisant intervenir un processus innovant d’extraction de données :\n\nExtraction automatisée de tableaux dans des PDF pour la construction de statistiques d’entreprises (Insee) ;\nScraper et retravailler les offres d’emploi en ligne pour permettre des analyses fines du marché du travail, le projet JOCAS (Dares, SSM Travail) ;\n\n16h-16h30: Pause, moment de convivialité.\n16h30-17h30: Keynote par Pierre Etienne Devineau (Ministères sociaux) et Léo Guillaume (Dinum). Les enjeux rencontrés par le projet Albert autour de l’entraînement et de la mise en production de grands modèles de langage (LLM) francophones.\n17h30-19h30: Pot, moment de convivialité.\n\n\nPour toute information : contact-ssphub@insee.fr\n📺️ La première journée du réseau ayant eu lieu en 2023 est également disponible en replay."
  },
  {
    "objectID": "event/2024-10-14-network-day/index.html#replay-de-la-deuxième-journée-du-ssphub-le-14-octobre-2024",
    "href": "event/2024-10-14-network-day/index.html#replay-de-la-deuxième-journée-du-ssphub-le-14-octobre-2024",
    "title": "Deuxième journée du SSPHub",
    "section": "",
    "text": "Les sessions plénières de la journée du réseau sont disponibles ci-dessous:\n\n\nSéquencement de la vidéo et slides\n\n\n00:00-03:30: Introduction de la journée par Romain Lesur\n03:30-46:30: “Le blé vu du ciel : images satellitaires et prédiction des rendements agricoles à l’échelle de la parcelle” (Service statistique ministériel du Ministère de l’Agriculture, de la souveraineté alimentaire et de la Forêt).\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/agriculture.pdf\",\n  \"Télécharger les slides du SSM Agriculture\"\n)\n\n\n\n\n\n\n\n46:30-83:00: “Identifier et classer les causes de décès en automatisant le traitement des certificats en langage naturel” (CépiDC, Inserm)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/cepidc.pdf\",\n  \"Télécharger les slides du CépiDC\"\n)\n\n\n\n\n\n\n\n86:00-123:00: Keynote de Pascal Rivière (chef de l’Inspection générale de l’Insee) “Data science et statistique publique : contexte institutionnel et évolutions”\n123:00-164:00: “Extraction automatisée de tableaux dans des PDF pour la construction de statistiques d’entreprises” (Insee)\n\n\ncreateButton(\n  \"https://ssplab.pages.lab.sspcloud.fr/table-extraction-evaluation/#/title-slide\",\n  \"Voir les slides de l'Insee\"\n)\n\n\n\n\n\n\n\n164:00-205:00: “Scraper et retravailler les offres d’emploi en ligne pour permettre des analyses fines du marché du travail, le projet JOCAS” (Dares: Service statistique ministériel du Ministère du Travail et de l’Emploi)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/jocas.pptx\",\n  \"Télécharger les slides de la DARES\"\n)\n\n\n\n\n\n\n\nA partir de 205:00: Keynote de Pierre Etienne Devineau (ex-DINUM) sur les enjeux rencontrés par le projet Albert autour de l’entraînement et de la mise en production de grands modèles de langage (LLM) francophones.\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/albert.pptx\",\n  \"Télécharger les slides de la keynote\"\n)\n\n\n\n\n\n\n\nQuelques photos de l’événement:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: \n\n\n\nLes slides des différentes présentations peuvent être téléchargées ⬆️\n\nfunction createButton(slides, message=\"Télécharger les slides\"){\n  const button = html`\n  &lt;p class=\"text-center\"&gt;\n    &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n      &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;${message}\n    &lt;/a&gt;\n  &lt;/p&gt;`\n  return button\n}\n\n\n\n\n\n\n\n\nRappel du programme de la journée\n\n\n9h30-10h: Accueil, moment de convivialité autour d’un café.\n10h-11h30: Retours d’expérience de projets innovants autour de la classification de textes ou d’images :\n\nLe blé vu du ciel : images satellitaires et prédiction des rendements agricoles à l’échelle de la parcelle (SSM Agriculture) ;\nIdentifier et classer les causes de décès en automatisant le traitement des certificats en langage naturel (CépiDC, Inserm).\n\n11h30-12h15: Atelier de réflexion autour des applications des méthodes de classification de textes ou d’images sur les données des * participants.\n12h15-14h: Pause déjeuner, moment de convivialité.\n14h-14h30: Pascal Rivière (Chef de l’inspection générale de l’Insee) interviendra sur le thème “Data science et statistique publique : contexte institutionnel et évolutions”.\n14h30-16h: Retours d’expérience de projets faisant intervenir un processus innovant d’extraction de données :\n\nExtraction automatisée de tableaux dans des PDF pour la construction de statistiques d’entreprises (Insee) ;\nScraper et retravailler les offres d’emploi en ligne pour permettre des analyses fines du marché du travail, le projet JOCAS (Dares, SSM Travail) ;\n\n16h-16h30: Pause, moment de convivialité.\n16h30-17h30: Keynote par Pierre Etienne Devineau (Ministères sociaux) et Léo Guillaume (Dinum). Les enjeux rencontrés par le projet Albert autour de l’entraînement et de la mise en production de grands modèles de langage (LLM) francophones.\n17h30-19h30: Pot, moment de convivialité.\n\n\nPour toute information : contact-ssphub@insee.fr\n📺️ La première journée du réseau ayant eu lieu en 2023 est également disponible en replay."
  },
  {
    "objectID": "event/2024-02-29-mauviere/index.html",
    "href": "event/2024-02-29-mauviere/index.html",
    "title": "Eric Mauvière, “La dataviz pour donner du sens aux données et communiquer un message”",
    "section": "",
    "text": "29 février (15h - 16h)\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-02-09-mauviere/conf_ssphub_item7-1.pdf\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Télécharger les slides\n  &lt;/a&gt;\n&lt;/p&gt;`\n\n\n\n\n\n\nUn exemple issu de la présentation d’Eric :"
  },
  {
    "objectID": "event/2024-02-29-mauviere/index.html#eric-mauvière-la-dataviz-pour-donner-du-sens-aux-données-et-communiquer-un-message",
    "href": "event/2024-02-29-mauviere/index.html#eric-mauvière-la-dataviz-pour-donner-du-sens-aux-données-et-communiquer-un-message",
    "title": "Eric Mauvière, “La dataviz pour donner du sens aux données et communiquer un message”",
    "section": "",
    "text": "29 février (15h - 16h)\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-02-09-mauviere/conf_ssphub_item7-1.pdf\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Télécharger les slides\n  &lt;/a&gt;\n&lt;/p&gt;`\n\n\n\n\n\n\nUn exemple issu de la présentation d’Eric :"
  },
  {
    "objectID": "event/2022-06-20-funathon/index.html",
    "href": "event/2022-06-20-funathon/index.html",
    "title": "Funathon de juin 2022",
    "section": "",
    "text": "Présentation\nEn 2022, une nouvelle édition a eu lieu les 20 et 21 juin, tournant cette fois autour de la thématique de l’environnement, et du changement climatique. Cette fois encore, 9 sujets ont été proposés, chaque fois accompagnés de notebooks d’explication (en R et en Python). Ceci a notamment permis de travailler sur :\n\nl’extraction de données de Twitter,\nl’analyse textuelle des données du grand Débat,\nla réalisation d’analyses graphiques autour de la montée des eaux,\nl’analyse des données issues de la base Ademe sur les logements,\nle machine learning à partir des données de la météo,\nles données satellites,\nl’utilisation de Fasttext,\nd’Elastic Search,\nenfin, 3 Master class ont également été proposées sur :\n\n\nla pertinence d’utiliser Python quand on connait R ;\n\n\nune initiation à Elastic ;\n\n\nL’utilisation du deep learning pour classifier des données issues d’images satellites.\n\n\n\nTous les niveaux d’expertise étaient les bievenus.\n# Détails L’ensemble du materiel associé, et les différents liens vers les master class, sont présents dans le repo du funathon."
  },
  {
    "objectID": "event.html",
    "href": "event.html",
    "title": "Evénements",
    "section": "",
    "text": "Troisième journée du SSPHub\n\n\nProgramme et modalités d’inscription à la 3e journée du réseau\n\n\n\n\n\n\n15 sept. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtelier - Comment récupérer des données sous format Parquet ?\n\n\nLe format Parquet est un format de données connaissant une popularité importante du fait de ses caractéristiques techniques (orientation colonne, compression…\n\n\n\n\n\n\n16 avr. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtelier - Comment récupérer des données par API ?\n\n\nLes API (Application Programming Interface) sont un mode d’accès aux données en expansion. Grâce aux API, l’automatisation de scripts est facilitée puisqu’il n’est plus…\n\n\n\n\n\n\n9 avr. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeuxième journée du SSPHub\n\n\nProgramme et modalités d’inscription à la 2e journée du réseau\n\n\n\n\n\n\n14 oct. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto : Une évolution de R Markdown pour des travaux statistiques reproductibles\n\n\nPour fiabiliser la production de documents construits en valorisant des données (tableaux, graphiques, etc.), RStudio (devenu Posit depuis) a construit il y a quelques…\n\n\n\n\n\n\n2 mai 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nEric Mauvière, “La dataviz pour donner du sens aux données et communiquer un message”\n\n\nLe 29 février (15h - 16h), Eric Mauvière nous fera une présentation, avec de nombreux exemples issus de la statistique publique, de la manière dont une visualisation de…\n\n\n\n\n\n\n29 févr. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremière journée du SSPHub\n\n\nReplay de la première journée de présentation du SSPHub\n\n\n\n\n\n\n29 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n“OCRisation, état de l’art et projets auxquels participe Teklia” par Christopher Kermorvant\n\n\nLe 29 mars de 15h à 16h nous recevons Christopher Kermorvant, chercheur spécialisé en OCRisation et fondateur de Teklia. Il nous fera un état de l’art de l’OCRisation puis…\n\n\n\n\n\n\n29 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation du projet Meta Academy - Carpentries\n\n\nPour favoriser l’adoption des langages R, Python et Git dans les administrations, le programme ModernStat piloté par l’OCDE et Statistics Canada, a lancé un projet…\n\n\n\n\n\n\n28 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation des packages R et Python pour accéder à l’open data de l’Insee\n\n\nL’Insee met à disposition ses données par le biais d’API ou par son site web. Pour faciliter la…\n\n\n\n\n\n\n13 févr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation de gridviz par Julien Gaffuri\n\n\nEvénement de présentation de gridviz par Julien Gaffuri (Eurostat)\n\n\n\n\n\n\n20 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation d’Observable par Nicolas Lambert\n\n\nobservable est la nouvelle plateforme de dataviz réactive. Initiée par Mike Bostock (créateur de D3.js), ce réseau social de la dataviz a pour…\n\n\n\n\n\n\n16 nov. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunathon de juin 2022\n\n\nPrésentation du deuxième Funathon du SSPLab organisé le 20 juin 2022 autour de 9 sujets, en R et en Python.\n\n\n\n\n\n\n19 juin 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunathon de juin 2021\n\n\nPrésentation du premier Funathon du SSPLab organisé le 21 juin 2021 autour de 8 sujets, en R et en Python, à partir de données Airbnb\n\n\n\n\n\n\n20 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéminaire - la méthodologie des appariements\n\n\nSéminaire de Méthodologie statistique et de sciences des données du 12 avril 2021 : revue des méthodes d’appariement et des principaux concepts et quelques exemples de…\n\n\n\n21 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéminaire - nouvelles approches pour coder dans une nomenclature : machine learning et autocomplétion\n\n\nSéminaire de Méthodologie statistique et de sciences des données du 14 janvier 2020 : innovation et expériences pratiques récentes pour réaliser des tâches de classification…\n\n\n\n14 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéminaire - Big Data et statistiques publiques : questions de méthodes et lancement du SSPLab\n\n\nSéminaire de Méthodologie statistique et de sciences des données du 30 novembre 2016 : enjeu technique et statistique de l’utilisation de la science des données pour la…\n\n\n\n30 nov. 2016\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "course/ssplab-geomatique/index.html",
    "href": "course/ssplab-geomatique/index.html",
    "title": "Géomatique appliquée à la statistique",
    "section": "",
    "text": "La cartographie thématique et les statistiques ont émergé progressivement au cours du 19e siècle du besoin des états naissants de se connaître, les deux disciplines ayant rapidement cultivé une forte interdépendance. La diffusion des statistiques territoriales a favorisé l’essor des représentations cartographiques modernes. En contrepartie, la cartographie a nourri les réflexions sur la prise en compte de l’espace dans l’étude des faits sociaux.\nLes progrés de l’informatique ont décuplé la capacité des acteurs publics et privés à diffuser des cartes statistiques. Il n’a jamais été aussi simple de produire et de communiquer une information statistique à l’aide d’une carte. Ce constat est néanmoins à tempérer, car la cartographie statistique reste toujours une pratique d’initiés. Une présentation simple des outils géomatiques récents qui permettent de stocker, traiter et diffuser l’information spatiale pour le statisticien est donc utile.\nCe document propose une présentation unifiée des concepts géomatiques, qui pourra être appliquée quelque soit l’outil choisi. La présentation est cependant restreinte aux données vectorielles et ne traite pas des images (données dites rasters). Leur traitement nécessite en effet des outils adaptés, tels que la reconnaissance d’image, qui dépasse le cadre de ce document"
  },
  {
    "objectID": "blog/retrospective2022/index.html",
    "href": "blog/retrospective2022/index.html",
    "title": "Rétrospective de l’année 2022",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit\nLa fin de l’année est généralement synonyme de bétisiers, best of ou rétrospectives personnalisées qui nous permettent de nous rappeler les événements marquants de l’année.\nPour célébrer la fin de l’année 2022, la newsletter de janvier adopte un format un peu spécial pour proposer, en deux temps, deux rétrospectives.\nCette première newsletter revient sur les principaux événements de l’année 2022 dans le monde de la data science. La seconde newsletter proposera une rétrospective quantitative sur le réseau des data scientists de la statistique publique, à la manière des rétrospectives personnalisées de nos applications préférées.\n\nLes IA créatrices de contenu à l’honneur\nSi l’année 2022 a été particulièrement riche dans le domaine de la data science, c’est principalement grâce à deux coups médiatiques d’OpenAI, à savoir Dall-E et ChatGPT.\nCes deux outils ont beaucoup fait parler d’eux, au-delà de la sphère traditionnelle de la data science. Le buzz a été intense sur Twitter ou sur Mastodon, le réseau social dont le nombre d’utilisateurs a nettement augmenté en réaction au rachat de Twitter par Elon Musk en fin d’année.\n\nCes innovations, parce qu’elles pourraient avoir des effets à long terme sur la manière dont le grand public appréhende l’intelligence artificielle, ont beaucoup intéressé les médias traditionnels, notamment Le Monde, The Economist et sa “Nouvelle Frontière” ou le Guardian qui s’interroge sur la nature des tâches que l’intelligence artificielle pourra remplacer à terme : procédurales et régies par des règles bien définies ou bien également des activités nécessitant de la créativité et des capacités d’analyse ?\nPour une fois, il ne s’agit donc pas de souligner exclusivement les limites de ces modèles voire leurs dérives (deep fake, biais racistes…) mais aussi de s’enthousiasmer sur leur potentiel créatif. Il est difficile de rester insensible à certaines des créations artistiques des modèles Dall-E, Stable Diffusion, Midjourney et consorts ou de résister à la tentation de tester la capacité de ChatGPT à répondre à des questions complexes Les chercheurs, et pas des moindres (notamment Andrew Ng ou Gaël Varoquaux) se sont également saisis de cette question et ont souligné les biais de raisonnement et excès de confiance de ces IA.\n\n\n\nhttps://github.com/Stability-AI/stablediffusion\n\n\nSi vous désirez utiliser Python de manière créative pour générer du contenu avec Stable Diffusion, vous pouvez consulter ce tutoriel qui fonctionne sur le SSPCloud ou sur Google Colab.\n\n\n\nLe succès des modèles de diffusion\nCes IA génératrices de contenu reposent toutes, à plusieurs niveaux, sur des réseaux de neurone.\nLe premier étage de la fusée est un modèle de langage (large language model) qui synthétise un langage en un ensemble complexe de paramètres. Les plus connus sont BERT et GPT-3. L’inflation dans le nombre de paramètres n’est pas prête de s’arrêter. Si les ressources nécessaires à entraîner en 2018 le modèle BERT (110 millions de paramètres) avaient déjà été critiquées en raison de leur coût financier et environnemental, cette complexité a encore augmenté depuis. Le modèle GPT-3, sorti en 2020, et qui sert de base à Dall-E et ChatGPT intègre 175 milliards de paramètres. Un chiffre qui apparaît minime par rapport aux 17O trillions de paramètres attendus pour le modèle GPT-4 en 2023.\nEn ce qui concerne les IA créatrices de contenu visuel, le deuxième étage de la fusée est un modèle d’analyse d’image qui apprend à associer des images à une description textuelle afin de détecter des structures communes entre des mots ou des séquences de mots et des formes sur des images. Il s’agit de déconstruire une forme en une structure minimale de pixels qui permet de l’identifier.\n\n\n\nSource: Sebastian Raschka\n\n\nEnsuite, pour générer une image à partir d’une description inédite intervient le modèle de diffusion qui reconstruit une image à partir du mélange de l’ensemble des pixels qui traduisent les concepts principaux d’une instruction. L’une des explications les plus pédagogiques pour comprendre le fonctionnement de ces modèles vient du Washington Post.\nSinon, on peut demander directement à ChatGPT de nous expliquer:\n\n\n\nL’actualité dans le monde du deep learning\nSi le succès d’estime de ces IA génératrices consacre les modèles de diffusion, l’année du deep learning ne se réduit pas à cette actualité.\nL’année a notamment été marquée par la compétition entre les librairies et écosystèmes TensorFlow, développé par Google, et PyTorch projet initié par Facebook/Meta. PyTorch, plus récent, bénéficie d’une dynamique plus ascendante que TensorFlow. Le succès d’HuggingFace, plateforme de mise à disposition de modèles, et où les implémentations PyTorch sont systématiques alors que celles en TensorFlow sont rares a participé à la diffusion de PyTorch.\nPreuve du succès de PyTorch, cet écosystème est dissocié de Meta depuis septembre afin de devenir un outil généraliste géré par la Linux Foundation. À l’inverse, Google semble se détacher graduellement de TensorFlow pour privilégier son nouvel écosystème JAX.\n\n\nDu changement côté RStudio\nDepuis quelques années, RStudio a fait le choix de devenir un écosystème de data science généraliste et non plus exclusivement attaché au langage R.\nCette année, cela s’est traduit par la publication, très commentée, de Quarto qui vise à proposer, dans de nombreux langages de programmation, des fonctionalités de publications reproductibles équivalentes à l’un des produits emblématiques de RStudio, à savoir R Markdown. Rien de mieux pour être convaincu de l’intérêt de cet outil que d’observer la galerie d’exemples, d’explorer la documentation très riche, ou de tester soi-même sur un exemple. Cet été, RStudio a également annoncé que Shiny, un autre produit emblématique, serait maintenant disponible sous Python, comme alternative à Dash ou Streamlit.\nL’année 2022 a été l’occasion, pour RStudio, d’un autre changement, symbolique celui-ci. Afin de détacher son image du langage R, l’entreprise a en effet changé de nom pour devenir posit. L’entreprise n’a néanmoins pas abandonné son activité foisonnante dans R puisque Hadley Wickham a commencé à publier de nouveaux chapitres pour une nouvelle édition augmentée de l’ouvrage de référence R For Data Science.\n\n\nObservable devient un incontournable dans le monde de la dataviz\nPour permettre des visualisations interactives, cela fait plusieurs années que JavaScript est un incontournable et que le web assembly retient de plus en plus d’attention.\nLes journaux traditionnels utilisent ainsi de plus en plus le data scrollytelling , cette technique de narration qui consiste à présenter des informations sous forme de récit interactif, en utilisant une combinaison de texte et de graphiques qui apparaissent et disparaissent en fonction des actions du lecteur. L’un des exemples les plus réussis des dernières années a sans doute été la visualisation du New York Times “How the virus got out”. Cette approche a également été adoptée par le Ministère de l’Agriculture pour diffuser les chiffrés clés du recensement agricole. Nos voisins anglais ne sont pas en reste puisque les derniers résultats du recensement sont proposés sur un site web remarquable de fluidité.\nAfin de permettre une diffusion accrue de visualisations en JavaScript, Mike Bostock, déjà créateur de la librairie de dataviz de référence D3.js, est à l’origine de la plateforme observable, sorte de Github de la dataviz permettant du partage et de la réutilisation de notebooks réactifs. En cette année 2022, la plateforme a connu un véritable boom et est devenu un incontournable dans le domaine. L’une des raisons est l’ajout de fonctionalités qui permettent d’étendre le public cible au delà des développeurs web, déjà accoutumés à Javascript. Parmi les fonctionalités les plus remarquables, la possibilité depuis Novembre d’utiliser des requêtes SQL grâce à DuckDB permet aux habitués de R ou de Python de retrouver des manipulations auxquels ils sont habitués. La librairie Plot offre une grammaire proche de ggplot2.\nLa communauté des cartographes a été particulièrement active sur Observable, notamment à l’occasion du #30daymapchallenge. Nicolas Bertin (neocarto), dont on ne peut que recommander l’introduction à Observable faite pour le réseau, ou Eric Mauvière font partie des comptes à suivre dans la communauté francophone.\nObservable, en tant que langage construit sur JavaScript, est également disponible pour les utilisateurs de Quarto, ce qui permet de mettre à disposition des visualisation réactives sans passer nécessairement par la plateforme observablehq.com pour mettre à disposition des visualisations réactives, ce qui constitue une alternative intéressante aux applications qui nécessitent un serveur en arrière plan, comme Shiny ou Dash.\n \n\n\nLes autres actualités en France\nLe rapport du conseil d’État pour la construction d’une IA de “confiance” a donc été publié en une année 2022 où les avancées techniques des dernières années commencent à être accessibles grâce à des outils plus grand public, ce qui va nécessairement soulever des enjeux éthiques et juridiques.\nLe projet Onyxia, qui vise à proposer une infrastructure de data science à l’état de l’art pour data scientists, a organisé son deuxième Openlab. L’occasion de revenir sur le projet, sa philosophie, ses dernières avancées mais aussi d’échanger sur les perspectives de réutilisation dans de multiples environnements et de nouer des partenariats qui permettront au projet de grandir encore en 2023."
  },
  {
    "objectID": "blog/polars/index.html",
    "href": "blog/polars/index.html",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "",
    "text": "Le concept de dataframe est central pour le data scientist qui manipule des données tabulaires. En Python, Pandas est la solution de loin la plus populaire. En moyenne, le package est téléchargé 4 millions de fois par semaine, depuis des années.\nUn petit nouveau apporte un vent de fraîcheur dans le domaine : Polars.\nSes atouts ? D’excellentes performances et une expressibilité qui le rapproche d’un dplyr.\nCe post de blog revient sur les principaux atouts de Polars, sans vouloir être exhaustif. Un notebook illustrant les principales fonctionnalités du package vise à le compléter :"
  },
  {
    "objectID": "blog/polars/index.html#lévaluation-lazy",
    "href": "blog/polars/index.html#lévaluation-lazy",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "L’évaluation lazy",
    "text": "L’évaluation lazy\nPlusieurs éléments expliquent cette rapidité.\nEn premier lieu, Polars est conçu pour optimiser les requêtes : grâce au mode lazy (“paresseux”), on laisse la possibilité au moteur d’analyser ce qu’on souhaite faire pour proposer une exécution optimale (pour la lecture comme pour la transformation des jeux de données). La lazy evaluation est une méthode assez commune pour améliorer la vitesse des traitements et est utilisée, entre autres, par Spark.\nDu fait de la lazy evaluation il est ainsi possible, par exemple, si un filtre sur les lignes arrive tardivement, de le remonter dans l’ordre des opérations effectuées par Python afin que les opérations ultérieures ne soient effectuées que sur l’ensemble optimal de données. Ces optimisations sont détaillées dans la documentation officielle."
  },
  {
    "objectID": "blog/polars/index.html#lecture-optimisée-des-fichiers",
    "href": "blog/polars/index.html#lecture-optimisée-des-fichiers",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "Lecture optimisée des fichiers",
    "text": "Lecture optimisée des fichiers\nL’utilisateur Pandas est habitué à lire du CSV avec pd.read_csv. Avec Polars, il existe deux manières, très ressemblantes de le faire.\nimport polars as pl\n\n# Création d'une requête\nq = (\n    pl.scan_csv(\"iris.csv\") # Lecture lazy\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .groupby(\"species\")\n    .agg(pl.all().sum())\n)\n\n# Exécution de la requête\ndf = q.collect()\nAvec cette syntaxe, les connaisseurs de Pyspark retrouveront facilement leurs petits (ours 🐻).\nOn peut toujours lire de manière plus directe (en mode eager, “impatient”) en utilisant la fonction read_csv, et ensuite appliquer des transformations optimisables en glissant habilement lazy :\ndf = pl.read_csv(\"iris.csv\")\n\ndf_res = df.lazy() # ←  ici :)\n  .filter(pl.col(\"sepal_length\") &gt; 5)\n  .groupby(\"species\")\n  .agg(pl.all().sum())\n  .collect()\nPolars fonctionne également très bien avec le format Parquet, comme illustré dans le notebook qui accompagne ce post."
  },
  {
    "objectID": "blog/polars/index.html#parallélisation",
    "href": "blog/polars/index.html#parallélisation",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "Parallélisation",
    "text": "Parallélisation\nPolars parallélise les traitements dès que cela est possible, notamment dans le cas d’agrégation. Chaque coeur se charge d’une partie de l’agrégation et envoie des données plus légères à Python qui va finaliser l’agrégation.\n\n\n\nParallélisation\n\n\nIllustration du principe de la parallélisation\nSur les systèmes proposant de nombreux coeurs, cela peut faire gagner beaucoup de temps."
  },
  {
    "objectID": "blog/polars/index.html#des-couches-basses-à-la-pointe",
    "href": "blog/polars/index.html#des-couches-basses-à-la-pointe",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "Des couches basses à la pointe",
    "text": "Des couches basses à la pointe\nEnfin, le choix d’utiliser à la fois le format de représentation en mémoire Arrow et le langage Rust pour le coeur de la bibliothèque n’est pas étranger à cette performance."
  },
  {
    "objectID": "blog/polars/index.html#calculs-out-of-memory",
    "href": "blog/polars/index.html#calculs-out-of-memory",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "Calculs out of memory",
    "text": "Calculs out of memory\nPolars travaille vite mais présente aussi l’avantage de lire naturellement des jeux de données hors des limites de la mémoire de l’ordinateur grâce à sa capacité de lire en flux (méthode qu’on appelle le streaming).\n# La même requête que tout à l'heure va lire le fichier \"en flux\"\ndf = q.collect(streaming=True)\nDe plus, Polars lit nativement les fichiers Parquet qui par ses propriétés permet d’aller beaucoup plus vite que le CSV !"
  },
  {
    "objectID": "blog/onyxia/index.html",
    "href": "blog/onyxia/index.html",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "",
    "text": "Onyxia est un logiciel open source développé par l’Insee (disponible sur Github ) permettant de fournir un environnement de traitement de données à l’état de l’art. Principalement conçu pour permettre le travail interactif des data scientists, l’expérience fournie avec Onyxia favorise également la reproductibilité des travaux et leur mise en production.\nLe logiciel Onyxia est installé par des organisations souhaitant créer un datalab, c’est-à-dire une plateforme interactive de traitement de données. Ces organisations ont toutes le point commun de vouloir construire une plateforme qui embrasse les technologies cloud que sont la conteneurisation et le stockage objet tout en mettant à disposition celles-ci dans un environnement user-friendly où l’interconnexion entre ces différentes briques est gérée de manière cohérente. Les technologies cloud native sont devenues indispensables dans l’écosystème de la donnée, du fait d’une meilleure gestion des ressources de traitement ou de la capacité à créer un environnement parfaitement reproductible pour une mise en production accélérée.\nCe post de blog a pour objectif de présenter la raison d’être d’Onyxia, sa génèse et les solutions qu’apporte cette infrastructure à des irritants classiques des projets novateurs de data science."
  },
  {
    "objectID": "blog/onyxia/index.html#contexte",
    "href": "blog/onyxia/index.html#contexte",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Contexte",
    "text": "Contexte\nL’écosystème de la data science est en mouvement accéléré depuis 10 ans et le rôle du data scientist dans les organisations valorisant de la donnée évolue continuellement (Davenport et Patil 2022). Les data scientists modernes sont amenés à utiliser de plus en plus de langages et doivent être capables de maîtriser plusieurs architectures informatiques. La frontière est ainsi moins nette que par le passé entre statisticiens et informaticiens. De plus, les innovations récentes dans le monde du développement logiciel, notamment l’adoption massive de l’approche DevOps - approche qui consiste à automatiser la production de livrables dès la conception du prototype - a également fait évoluer les pratiques des data scientists.\nCe besoin de ressources informatiques croissantes, de flexibilité dans le prototypage de solutions informatiques et l’évolution des pratiques consistant à mettre à disposition en continu des livrables ont eu des implications importantes sur les architectures informatiques dominantes dans l’écosystème de la donnée. Pour répondre au besoin croissant de puissance de traitement, les serveurs partagés, organisés sous forme de clusters, se sont développés dans de nombreuses organisations. Après avoir connue son heure de gloire au début des années 2010, l’infrastructure HDFS (Hadoop Distributed File System), qui reposait sur des clusters où les données et la puissance de traitement étaient distribuées et collocalisées, a laissé place à des infrastructures plus scalables, basées sur l’approche de la conteneurisation."
  },
  {
    "objectID": "blog/onyxia/index.html#de-hdfs-à-la-conteneurisation",
    "href": "blog/onyxia/index.html#de-hdfs-à-la-conteneurisation",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "De HDFS à la conteneurisation",
    "text": "De HDFS à la conteneurisation\n\n\n\n\n\n\nNote\n\n\n\nCette partie plus technique développe des éléments pour comprendre le succès récent des infrastructures conteneurisées.\nElle pourra intéresser le lecteur curieux sur les fondements des infrastructures cloud modernes mais n’est pas nécessaire à la compréhension générale de l’article.\n\n\nLa conteneurisation, qui repose sur l’idée que les serveurs de stockage de la donnée peuvent être dissociés de ceux effectuant les traitements, sert de fondement aux principales plateformes cloud actuelles fournissant des services à la demande.\nCe nouveau paradigme part de deux constats. Le premier est que les échanges de données entre les noeuds d’un serveur sont aujourd’hui peu coûteux. Avec des flux réseaux suffisants et une technologie performante, il est donc possible d’échanger à un coût modéré de gros volumes de données au sein d’une infrastructure. Le deuxième constat est que la maintenance d’une infrastructure conteneurisée, faite pour être très malléable, est plus légère que celle d’une infrastructure basée sur des machines virtuelles ou sur les infrastructrures calibrées pour l’analytique big data comme HDFS reposant sur la collocalisation des données et des traitements1.\nLes données étant stockées sur des serveurs différents de ceux exécutant les traitements, l’accès à celles-ci se fait à travers des API qui permettent de traiter le système de stockage distant comme un système de fichiers classique. Onyxia a adopté une implémentation open source du système de stockage S3 appelée MinIO.\nEn ce qui concerne le traitement des données, le fait d’utiliser un système de conteneurs, c’est-à-dire une configuration logicielle portable minimaliste prête à l’emploi (par opposition aux machines virtuelles qui impliquent un système d’exploitation complet), offre une grande liberté sur le choix des logiciels de traitement. De nombreuses technologies open source devenues standards dans le monde de la data science (Jupyter, RStudio, ElasticSearch…) existent déjà sous cette forme et peuvent ainsi être adoptées dans une telle infrastructure pour fournir des services prêts-à-l’emploi pour les data scientists. La mise en musique de toutes ces petites boites auto-suffisantes, notamment l’optimisation des ressources concurrentes sur un serveur, est permise par la technologie d’orchestration Kubernetes.\n\n\n\nCentralisation des ressources par Onyxia\n\n\n\n\n\n\n\n\n\n\nNotePlus de détails pour comprendre le changement de paradigme vers la conteuneurisation 👇\n\n\n\n\n\nLes infrastructures big data reposent sur le principe du cluster (grappe) informatique. Des serveurs sont connectés entre eux, ce qui forme de manière imagée une grappe. Cette interconnexion de plusieurs serveurs entre eux peut se faire au niveau :\n\ndu stockage : les données volumineuses ne sont pas stockées sur un seul serveur mais au contraire réparties ;\ndu traitement : les calculs sont effectués par blocs sur plusieurs serveurs et le résultat de ceux-ci est ensuite transmis à un serveur maître.\n\nLe système Hadoop Distributed File System a été pensé pour tirer parti de l’algorithme de traitement parallélisé MapReduce proposé en 2004 par Google. Les fichiers volumineux sont fractionnés et répartis sur plusieurs serveurs.\n\nFonctionnement d’une architecture MapReduce (source: Datascientest)\nLa spécificité de l’architecture HDFS est que non seulement le stockage est distribué mais également aussi la puissance de traitement associée. On parle à ce propos de collocalisation : les traitements ont lieu sur les mêmes serveurs que ceux où sont stockés les données. Cela permet de réduire les mouvements de données (shuffle dans l’image ci-dessus) qui sont coûteux du point de vue de la performance. Cette collocalisation a permis au système HDFS de devenir, au début de la décennie 2010, le paradigme dominant. En tirant parti de la parallélisation permise par des langages très efficaces comme Spark tout en limitant les échanges réseaux pouvant faire perdre en performance, cette architecture a attiré au-delà de l’écosystème du big data.\nLe système HDFS présente néanmoins certaines limites qui expliquent sa perte de succès avec l’émergence d’un nouveau paradigme plus flexible.\nEn premier lieu, ce système nécessite beaucoup de ressources du fait de son design. Comme les traitements sont lourds et partagés pour des usages concurrents, les noeuds constituant le cluster peuvent subir des arrêts à cause de surcharge des ressources. Pour tenir compte de la nature instable de cette infrastructure big data, les fichiers sont dupliqués. Ainsi, lors d’une erreur sur le serveur générant un arrêt du nœud (par exemple à cause de traitements trop gourmands), les traitements sur l’ensemble des données sont sécurisés évitant également la perte partielle ou totale de ces dernières.\nL’implication est que les données, déjà volumineuses, sont dupliquées plusieurs fois impliquant des architectures assez monumentales. Si la duplication de la donnée n’est pas en soi choquante afin d’éviter la perte de données, cela a un effet pervers dans un système de collocalisation. A chaque ajout de noeuds pour le stockage de données, il est également nécessaire d’ajouter des ressources pour les traiter. Il est donc compliqué de décorréler l’ajout de ressources de stockage et de traitement. Cette absence de flexibilité est pénalisante dans un monde où les données sont mises à jour fréquemment et où les technologies de traitement, donc les besoins associés, évoluent rapidement. Les infrastructures HDFS sont donc lourdes à changer, que ce soit pour ajouter des ressources ou faire évoluer les distributions logicielles présentes dessus.\nLe deuxième facteur qui a favorisé le changement de paradigme est l’amélioration des échanges réseaux. Il n’est plus aussi coûteux que par le passé de transférer des volumes importants de données au sein d’une infrastructure. Cela facilite la décorrélation entre environnement de stockage et de traitement.\nCette séparation des environnements de stockage et de traitement permet alors d’adopter pour chacun les technologies les plus performantes. Dans le domaine du stockage, celle qui a rencontré le plus de succès est le système de stockage S3 développé par Amazon. L’implémentation open source du système S3 est MinIO, utilisée par Onyxia.\nDans le domaine du traitement, la technologie la plus performante dépend de la nature de la tâche réalisée. Selon qu’on désire effectuer de la recherche textuelle, des visualisations de données ou de l’analyse d’image, on ne va pas vouloir utiliser la même technologie. Pour mettre à disposition des logiciels sur un serveur, il existe principalement deux approches concurrentes.\nLa première repose sur le principe des machines virtuelles. Cette approche n’est pas nouvelle et de nombreuses organisations ont proposé ou proposent encore ce type d’infrastructures pour des serveurs collectifs de traitement. Cette approche est néanmoins lourde : elle nécessite un système d’exploitation complet dont il faudra ensuite adapter la configuration lors de l’installation de chaque logiciel. Plusieurs logiciels coexistent donc dans ce système d’exploitation même si un seul, par exemple, Python, est utilisé. Les machines virtuelles sont des infrastructures assez polluantes puisque pour faire fonctionner un système d’exploitation dans son ensemble, il est nécessaire de mobiliser des ressources plus importantes que celles seulement nécessaires aux traitements. De plus, la configuration d’un système d’exploitation, et notamment, la gestion de la dépendance de multiples logiciels à des configurations systèmes qui peuvent ne pas correspondre, n’est pas triviale. Il est donc lourd de faire évoluer une infrastructure reposant sur des machines virtuelles. L’absence de flexibilité d’une infrastructure reposant sur le principe des machines virtuelles est pénalisante dans un écosystème mouvant comme celui de la data science, où une partie importante du travail de prototypage consiste à tester plusieurs technologies pour déterminer celle s’intégrant le mieux dans un processus de traitement de données.\nLe système de la conteneurisation a justement été pensé pour cela : plutôt qu’installer de nombreuses librairies au niveau du système, pour une fraction d’utilisateurs limitée à chacune, il est plus intéressant de créer des environnements complets qui vont exister de manière conjointe. Chaque framework va être construit comme un conteneur autosuffisant avec un système d’exploitation minime et un nombre minimal de couches de configurations supplémentaires. Un framework est livré sous la forme d’une image Docker, une technologie qui permet d’empaqueter un logiciel et ses dépendances sous la forme de boites minimalistes et les mettre à disposition facilement pour une réutilisation. Il existe par exemple des images Docker pour pouvoir utiliser RStudio, Jupyter, VSCode avec des configurations minimales afin d’exécuter du Python ou du R. A partir de celles-ci, l’utilisateur qui désire des configurations supplémentaires peut ajouter les couches qui lui sont utiles.\nMais les images Docker ne se réduisent pas à la mise à disposition d’environnements de développement. Une partie des technologies les plus appréciées de l’écosystème de la data science sont également livrées sous forme d’images Docker. Par exemple, le moteur de recherche ElasticSearch, très utilisé pour la recherche textuelle, peut être empaqueté dans une image Docker. Le logiciel Onyxia propose dès lors, dans un catalogue vivant, un certain nombre de logiciels très utiles pour les data scientists ayant fait l’objet d’un tel empaquetage. Les nombreuses images Docker servant à créer des services pour les data scientists sont disponibles en open source sur Github.\nPour organiser sur un serveur la coexistence de multiples utilisateurs de services gourmands en ressource, la solution Kubernetes fait aujourd’hui office de référence. Entre sa création en 2014 et aujourd’hui, cette solution d’orchestration, c’est-à-dire de gestion d’une infrastructure, est devenue incontournable. Outre son allocation dynamique des ressources, elle permet de transformer facilement le livrable d’une chaine de traitement en application disponible en continu. Ceci est particulièrement adapté dans un contexte de diversification des livrables fournis par les data scientists (API, application web, modèle…) et d’adoption d’une démarche DevOps voire MLOps."
  },
  {
    "objectID": "blog/onyxia/index.html#la-solution-onyxia",
    "href": "blog/onyxia/index.html#la-solution-onyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "La solution Onyxia",
    "text": "La solution Onyxia\n\nD’un cloud de l’administration à un logiciel ouvert\nPour permettre aux data scientists des administrations françaises de bénéficier de technologies cloud sans être dépendant d’un fournisseur de service privé, l’équipe innovation de l’Insee a eu l’idée de créer un datalab basé sur la philosophie de la conteneurisation en mobilisant exclusivement des composants open-source.\nCe datalab, né à l’Insee en 2018, a été ouvert à l’administration publique sous la forme d’une instance https://www.sspcloud.fr/ à condition d’utiliser des données ouvertes. En plus des agents déjà en poste dans l’administration, cette infrastructure sert depuis deux ans à former les élèves de l’ENSAE et de l’ENSAI dans le cadre de leur formation en data science.\nDébut 2023, ce sont plus de 3000 agents et étudiants qui sont inscrits sur cette infrastructure avec, en moyenne, 300 utilisateurs hebdomadaires. L’infrastructure de traitement propose 10 TB de RAM, 1100 CPU disponibles et 34 GPU. La capacité de stockage associée est de 150 TB.\nPour les utilisations internes de données plus sensibles, l’équipe innovation de l’Insee a rendu disponible le code source derrière le SSP Cloud dans le cadre d’un logiciel nommé Onyxia (https://www.onyxia.sh/). Ce logiciel est pensé comme un kit qui peut être installé sur un cluster Kubernetes, technologie détaillée précédemment."
  },
  {
    "objectID": "blog/onyxia/index.html#onyxia-en-bref",
    "href": "blog/onyxia/index.html#onyxia-en-bref",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Onyxia en bref",
    "text": "Onyxia en bref\n\nOnyxia propose principalement deux composants de valeur :\n\nune interface web qui agit comme la porte d’entrée du data scientist sur son datalab, lui facilitant l’accès aux technologies cloud et lui permettant de démarrer ses environnements de traitement de la donnée. L’interface ergonomique permet aux utilisateurs de données néophytes de démarrer des services standardisés sans se préoccuper de la configuration mais aussi aux data scientists plus aguerris de bénéficier de vastes possibilités de personnalisation du service.\ndes catalogues de logiciels : une petite vingtaine de services interactifs dont les plus utilisés sont RStudio, Jupyter, VScode, une quinzaine de services spécialisés dans les bases de données (Postgres, ElasticSearch…), 5 services d’automatisation (MLflow…) et 2 services de dataviz (Redash et Superset)\n\n\nLe catalogue des services disponibles dans Onyxia.\n\nCes deux composants peuvent être adaptés en fonction des besoins internes de chaque organisation. Tous les services interactifs sont automatiquement connectés à l’espace de stockage S3, et au coffre de secret Vault. La gestion des droits d’accès aux données stockées dans l’espace de stockage S3 ou dans des services de bases de données (ElasticSearch, PostGreSQL…) est automatisée afin que chaque service puisse accéder aux données sur lesquelles l’utilisateur détient des droits.\nOnyxia étant un ensemble malléable de logiciels conteneurisés, il est possible de ne pas adopter l’ensemble des services proposés par l’équipe de l’Insee qui maintient Onyxia. Il est également possible de changer certaines des briques de base pour l’adapter à des éléments d’infrastructure interne. Par exemple, il est possible d’adapter la destination du service de stockage ou les configurations des environnements data science pour l’adapter à des ressources."
  },
  {
    "objectID": "blog/onyxia/index.html#linterface-et-les-services-proposés-par-onyxia",
    "href": "blog/onyxia/index.html#linterface-et-les-services-proposés-par-onyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "L’interface et les services proposés par Onyxia",
    "text": "L’interface et les services proposés par Onyxia\n\n\nOnyxia offre des marges de manoeuvre sur l’interface\n\n\n\nL’une des principales forces d’Onyxia est d’offrir une multiplicité de services différents avec une interconnexion entre eux gérée de manière cohérente.\nLes conteneurs sont démarrés comme des services à la demande et la configuration automatique de ceux-ci permet d’assurer aux data scientists l’accès aux données disponibles dans des espaces de stockage ou des bases de données créées par l’utilisateur.\nLe catalogue de services se présente par le biais d’un formulaire ergonomique où l’utilisateur choisit la brique qu’il désire utiliser:\n\nLes data scientists et statisticiens n’ont donc pas besoin de connaître les détails du fonctionnement des briques techniques d’Onyxia pour utiliser la plateforme. Les éléments techniques comme la connexion au système de stockage sont, par défaut, déjà configurés :\n\nL’interface ergonomique permet de paramétrer certaines configurations si besoin, notamment les ressources à disposition du conteneur. Néanmoins l’allocation dynamique des ressources offre déjà de la flexibilité :\n\nL’utilisateur a accès à l’ensemble des services qu’il a ouvert depuis une page dédiée :\n\nLes services interactifs comme Jupyter, VSCode ou RStudio permettent alors à l’utilisateur d’accéder à une interface pour exécuter des traitements Python ou R.\n\nL’accès aux données peut se faire depuis la ligne de commande (via un utilitaire Minio Client) ou par un package Python ou R dédié qui permet de traiter le système de stockage distant comme un système local. Les traitements sont exécutés sur les serveurs de la plateforme qui héberge les notebooks, indépendamment de la machine par laquelle l’utilisateur accède au service. Par exemple, dans le cas du SSPCloud, les traitements sont exécutés depuis des serveurs hébergés à l’Insee."
  },
  {
    "objectID": "blog/onyxia/index.html#la-communauté-onyxia",
    "href": "blog/onyxia/index.html#la-communauté-onyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "La communauté Onyxia",
    "text": "La communauté Onyxia\nTous les composants sont proposés en open source par l’Insee ce qui permet de fédérer une communauté d’utilisateurs et de développeurs de ce produit. Il s’agit d’un bel exemple de mutualisation au sein de l’État et au delà. Les dépôts peuvent être retrouvés sur le Github de l’équipe innovation (celui de l’interface web, celui des images pour la data-science…). La communauté peut proposer de nouveaux services dans le catalogue.\nCette approche bottom up a déjà permis d’adapter des services aux besoins des utilisateurs ou d’améliorer la solution grâce à des retours des ré-utilisateurs d’Onyxia."
  },
  {
    "objectID": "blog/onyxia/index.html#les-plateformes-basées-sur-onyxia",
    "href": "blog/onyxia/index.html#les-plateformes-basées-sur-onyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Les plateformes basées sur Onyxia",
    "text": "Les plateformes basées sur Onyxia\nLa plateforme d’origine, le SSPCloud, est ouverte à tous les agents de l’État et à plusieurs écoles. Celle-ci est exclusivement limitée à l’exploitation de données open data. Cette stratégie d’offreur de services de traitement sur l’open data permet de montrer l’expertise de l’Insee sur les sujets data science.\nLes principaux usages de cette plateforme sont les suivants :\n\nla formation ;\nl’organisation de hackathons ;\nla mise à disposition de services innovants et visualisations utilisant de l’open data ;\n\nGrâce à la mise à disposition de la solution Onyxia sur Github, il est néanmoins possible d’adapter cette plateforme pour des datalab internes, sur données plus sensibles.\nL’Insee n’est donc désormais plus seul et fédère de nombreux acteurs autour de son projet. Fin 2021, Eurostat a été la première organisation en dehors de l’Insee à choisir Onyxia pour construire son Cloud Agnostic Data Lab. Expertise France pour le projet DATAFID a fait le choix d’Onyxia tout comme le CASD, le GENES ou encore le BercyHub avec le projet Nubonyxia.\nD’autres organisations sont plus dans une phase de POC ou d’étude : l’INS norvégien, Pole Emploi, Data4Good, le ministère de l’Intérieur, le ministère de la Justice, l’Inria…\nDans le cadre du TOSIT, association qui réunit de gros acteurs publics et privés autour de solutions open source, un certain nombre d’entreprises s’intéressent à Onyxia."
  },
  {
    "objectID": "blog/onyxia/index.html#vidéo-de-présentation-donyxia",
    "href": "blog/onyxia/index.html#vidéo-de-présentation-donyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Vidéo de présentation d’Onyxia",
    "text": "Vidéo de présentation d’Onyxia"
  },
  {
    "objectID": "blog/onyxia/index.html#références",
    "href": "blog/onyxia/index.html#références",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Références",
    "text": "Références\n\n\nDavenport, Thomas H, et DJ Patil. 2022. « Is Data Scientist Still the Sexiest Job of the 21st Century? » Harvard business review. https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century."
  },
  {
    "objectID": "blog/onyxia/index.html#footnotes",
    "href": "blog/onyxia/index.html#footnotes",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nOn peut ajouter que cette question n’est pas exclusivement technologique. Même s’il est volontairement polémique, l’article de Jordan Tigani “Big Data is Dead” illustre bien le changement de paradigme du monde de la tech.↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Vous pouvez trouver ici les articles de blog et des projets innovants portés par des membres du SSP.\n\n\n\n\n\n\n\n\n\n\n\nGuide d’utilisation des données du recensement de la population au format Parquet\n\n\n\nPython\n\nR\n\nParquet\n\n\n\nUn post de blog pour accompagner la mise à disposition des données détaillées du recensement au format Parquet.\n\n\n\n\n\n\n23 oct. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnyxia: l’infrastructure cloud mère des dragons\n\n\n\nInsee\n\nsspcloud\n\n\n\nLes technologies cloud sont incontournables dans l’écosystème de la donnée. Pour ne pas se rendre dépendante de fournisseurs de services externes, l’Insee a développé un…\n\n\n\n\n\n\n10 mai 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolars, une alternative fraîche à Pandas\n\n\n\nPython\n\nPandas\n\nPolars\n\nData wrangling\n\n\n\nPolars, une alternative moderne et fluide à Pandas\n\n\n\n\n\n\n10 févr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfolettre n°9\n\n\n\nInsee\n\nRetrospective\n\nInfolettre\n\n\n\nAprès la rétrospective de l’année 2022 de la data science, il est temps de se pencher sur l’année du réseau avec des visualisations interactives produites grâce à…\n\n\n\n\n\n\n10 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRétrospective de l’année 2022\n\n\n\nNLP\n\nObservable\n\nQuarto\n\nDeep learning\n\nInsee\n\nRetrospective\n\nInfolettre\n\n\n\nLa data science a beaucoup fait parler d’elle en 2022, notamment du fait des deux coups médiatiques d’openAI, à savoir…\n\n\n\n\n\n\n31 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe plongement lexical ou comment apprendre à lire à un ordinateur\n\n\n\nInsee\n\nNLP\n\n\n\nIntroduction aux méthodes de traitement du langage naturel.\n\n\n\n\n\n\n3 oct. 2022\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Le SSPHub, le réseau des data scientists",
    "section": "",
    "text": "Le réseau des data scientists du Service Statistique Publique (SSP) est constitué principalement, mais non exclusivement, par les administrations en charge de la production de statistiques officielles (Insee et Services Statistiques Ministériels principalement).\nLe réseau répond à plusieurs objectifs, dont les principaux sont:\n\nLe partage et la diffusion de connaissances au sein de la communauté des data scientists de l’administration autour des pratiques et des innovations de la data-science ;\nLa valorisation de travaux novateurs dans le champ de la production statistique ;\nFaciliter les échanges entre pairs, qu’ils appartiennent au service statistique public ou non.\n\nAfin de mieux cerner les objectifs, le public cible, les thèmes abordés par le réseau, et les moyens associés, un manifeste 📜 a été rédigé de manière collective."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "Le SSPHub, le réseau des data scientists",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nPlus de détails sont disponibles sur le site de l’Insee ici↩︎"
  },
  {
    "objectID": "manifeste.html",
    "href": "manifeste.html",
    "title": "Le manifeste du réseau des data scientists du service statistique public",
    "section": "",
    "text": "Note\n\n\n\nCe manifeste est une production collective. Pour proposer des modifications à celui-ci, qui seront discutées collégialement, vous pouvez suivre ce lien vers ."
  },
  {
    "objectID": "manifeste.html#footnotes",
    "href": "manifeste.html#footnotes",
    "title": "Le manifeste du réseau des data scientists du service statistique public",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nRapport de l’Inspection Générale de l’Insee N° 2020_48/DG75-B001 (non public)↩︎\nLe Service Statistique Public (SSP) regroupe les institutions en charge de la production de statistiques officielles. Il est principalement constitué de l’Insee et des services statistiques ministériels (SSM). Pour en savoir plus, le site de l’Insee propose des éléments supplémentaires.↩︎\nLe Service Statistique Public (SSP) regroupe les institutions en charge de la production de statistiques officielles. Il est principalement constitué de l’Insee et des services statistiques ministériels (SSM). Pour en savoir plus, le site de l’Insee propose des éléments supplémentaires.↩︎\nLe Service Statistique Public (SSP) regroupe les institutions en charge de la production de statistiques officielles. Il est principalement constitué de l’Insee et des services statistiques ministériels (SSM). Pour en savoir plus, le site de l’Insee propose des éléments supplémentaires.↩︎\nIl est possible de rejoindre ce canal Tchap sur simple demande à contact-ssphub@insee.fr↩︎"
  },
  {
    "objectID": "blog/embedding/index.html",
    "href": "blog/embedding/index.html",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "",
    "text": "Avec le développement de la collecte automatisée d’information numérique, les données textuelles sont devenues omniprésentes, que ce soit sous la forme d’e-mails, de réponses à des enquêtes, d’articles de presse ou encore de commentaires sur les réseaux sociaux. Ces données peuvent être une source très riche d’informations mobilisable par les statisticiens, pour peu qu’ils parviennent à en faire un traitement statistique. Ainsi, une problématique récurrente dans la statistique publique consiste à classer des informations formulées en langage courant (professions, noms de produits, noms de communes, etc.) dans des nomenclatures standardisées (PCS1, NAF2, COG3…).\nOr, le traitement des données textuelles pose une difficulté particulière: le langage naturel n’a pas de sens pour un ordinateur ! Un ordinateur ne travaille qu’avec des nombres, et ne peut pas manipuler directement des mots, des expressions ou des phrases. C’est pourquoi de multiples méthodes ont été développées au cours des dernières décennies pour proposer des solutions génériques permettant de traiter des corpus de données textuelles à la fois peu structurés et hétérogènes. Cet ensemble de méthodes de traitement automatisé du langage, plus connues sous l’acronyme NLP (natural langage processing) constituent encore aujourd’hui un champ de recherche particulièrement actif.\nCe billet de blog n’a pas l’ambition de proposer un aperçu des méthodes de NLP, mais simplement de présenter deux méthodes fréquemment utilisées pour transformer l’information textuelle pour la rendre compréhensible et utilisable par une machine:"
  },
  {
    "objectID": "blog/embedding/index.html#traiter-un-texte-comme-une-information-numérique-les-approches-possibles",
    "href": "blog/embedding/index.html#traiter-un-texte-comme-une-information-numérique-les-approches-possibles",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "Traiter un texte comme une information numérique : les approches possibles",
    "text": "Traiter un texte comme une information numérique : les approches possibles\n\nL’approche bag of words\nLe principe du bag of words est qu’on peut décrire un document comme un dictionnaire de mots (un sac de mots) dans lequel on pioche plus ou moins fréquemment un terme en fonction de son nombre d’occurrences.\nLa manière la plus simple de transformer des phrases ou des libellés textuels en une information numérique est de passer par un objet que l’on appelle la matrice document-terme. L’idée est de compter le nombre de fois où les mots (les termes, en colonne) sont présents dans chaque phrase ou libellé (le document, en ligne). Cette matrice fournit alors une représentation numérique des données textuelles.\nConsidérons un corpus constitué des trois phrases suivantes :\n\n_“La pratique du tricot et du crochet_”\n“Transmettre la passion du timbre”\n“Vivre de sa passion”\n\nLa matrice document-terme associée à ce corpus est la suivante :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrochet\nde\ndu\net\nla\npassion\npratique\nsa\ntimbre\ntransmettre\ntricot\nvivre\n\n\n\n\nLa pratique du tricot et du crochet\n1\n0\n2\n1\n1\n0\n1\n0\n0\n0\n1\n0\n\n\nTransmettre sa passion du timbre\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n0\n0\n\n\nVivre de sa passion\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n\nMission accomplie ! 🎉 Chaque phrase du corpus est associée à un vecteur numérique.\nIl est maintenant possible de manipuler cette matrice comme des données tabulaires classiques. Par exemple, on pourrait appliquer l’un des algorithmes usuels de classification (régression logistique, forêt aléatoire, gradient boosting, etc.) pour classer ces phrases dans des catégories.\nL’approche bag-of-words répond donc au besoin initial de transformer les données pour les rendre manipulables par une machine, en représentant les données textuelles sous la forme d’une matrice document-terme. Cette approche présente néanmoins une limite: elle traite tous les termes de façon indépendante et ne restitue pas la proximité de certains termes. Par exemple, rien dans la matrice document-terme de l’exemple précédent n’indique que les termes ’tricot” et “crochet” relèvent du même champ lexical. Un autre type de représentation plus complexe et plus riche constitue souvent comme une meilleure option : le plongement lexical.\n\n\nLe plongement lexical\nLe plongement lexical (word embedding en anglais) consiste à projeter l’ensemble des termes qui apparaissent dans le corpus dans un espace numérique à \\(n\\) dimensions. Chaque mot est représenté par un vecteur de taille fixe (comprenant \\(n\\) nombres), de façon à ce que deux mots dont le sens est proche possèdent des représentations numériques proches. Ainsi les mots « chat » et « chaton » devraient avoir des vecteurs de plongement assez similaires, eux-mêmes également assez proches de celui du mot « chien » et plus éloignés de la représentation du mot « maison ».\n\n\n\nIllustration du word embedding\n\n\n\nIllustration du plongement lexical. Source : Post de blog Word Embedding : Basics\n\n \nChacune des \\(n\\) composantes va encoder des informations différentes, comme le fait d’être un être vivant ou un objet, le genre, l’âge, le niveau d’abstraction, etc. C’est pour cette raison que des termes appartenant au même champ lexical auront des représentations numériquement proches. En pratique, les vecteurs de plongement ont des dizaines voire des centaines de composantes et il est impossible d’associer à chacune une interprétation univoque : toutes les notions s’entremêlent, mais chaque composante a un rôle à jouer.\nLe plongement lexical possède deux avantages par rapport à l’approche bag of words. D’une part, il fournit une représentation dense des termes, qui est plus adaptée aux algorithmes d’apprentissage statistique que la représentation creuse (matrice contenant beaucoup de zéros) de l’approche bag of words. D’autre part, les opérations mathématiques ont un sens sur les vecteurs du plongement. C’est là la magie du plongement lexical: il devient possible de faire des mathématiques avec les mots. Ainsi par exemple, les vecteurs résultant de la différence entre les représentations des mots « femme » et « homme » d’une part, et des mots « reine » et « roi » d’autre part, devraient être proches, car conceptuellement ces couples de mots sont régis par la même relation : un changement de genre.\nCette formule, souvent résumée sous la forme,\n\\[\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\\]\na assuré le succès des embeddings, car elle permet à une machine d’appréhender les relations logiques entre les mots.\nJusqu’ici, nous avons parlé du plongement de mots, mais comment obtenir le plongement d’un libellé textuel ? Une possibilité est de considérer tous les mots qui composent le libellé et de calculer la moyenne de leurs vecteurs de plongement."
  },
  {
    "objectID": "blog/embedding/index.html#construction-dun-plongement-lexical",
    "href": "blog/embedding/index.html#construction-dun-plongement-lexical",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "Construction d’un plongement lexical",
    "text": "Construction d’un plongement lexical\nUn plongement lexical se construit en parcourant un grand corpus de textes et en repérant les mots qui apparaissent souvent dans le même contexte. L’ensemble des articles Wikipedia est un des corpus de prédilection des personnes ayant construit des plongements lexicaux. Il comporte en effet des phrases complètes, contrairement à des informations issues de commentaires de réseaux sociaux, et propose des rapprochements intéressants entre des personnes, des lieux, etc.\nLe contexte d’un mot est défini par une fenêtre de taille fixe autour de ce mot. La taille de la fenêtre est un paramètre de la construction de l’embedding. Le corpus fournit un grand ensemble d’exemples mots-contexte, qui peuvent servir à entraîner un réseau de neurones.\nPlus précisément, il existe deux approches :\n\nContinuous bag of words (CBOW), où le modèle est entraîné à prédire un mot à partir de son contexte ;\nSkip-gram, où le modèle tente de prédire le contexte à partir d’un seul mot.\n\n\n\n\nIllustration de la différence entre les approches CBOW et Skip-gram\n\n\n\nIllustration de la différence entre les approches CBOW et Skip-gram. Source : Anwarvic sur StackOverflow\n\n \n\nAlgorithmes célèbres\nLa méthode de construction d’un plongement lexical présentée ci-dessus est celle de l’algorithme Word2Vec. Il s’agit d’un modèle open-source développé par une équipe de Google en 2013. Word2Vec a été le pionnier en termes de modèles de plongement lexical.\nLe modèle GloVe constitue un autre exemple4. Développé en 2014 à Stanford, ce modèle ne repose pas sur des réseaux de neurones mais sur la construction d’une grande matrice de co-occurrences de mots. Pour chaque mot, il s’agit de calculer les fréquences d’apparition des autres mots dans une fenêtre de taille fixe autour de lui. La matrice de co-occurrences obtenue est ensuite factorisée par une décomposition en valeurs singulières. Il est également possible de produire des plongements de mots à partir du modèle de langage BERT, développé par Google en 2019, dont il existe des déclinaisons dans différentes langues, notamment en Français (les modèles CamemBERT ou FlauBERT)\nEnfin, le modèle FastText, développé en 2016 par une équipe de Facebook, fonctionne de façon similaire à Word2Vec mais se distingue particulièrement sur deux points :\n\nEn plus des mots eux-mêmes, le modèle apprend des représentations pour les n-grams de caractères (sous-séquences de caractères de taille \\(n\\), par exemple « tar », « art » et « rte » sont les trigrammes du mot « tarte »), ce qui le rend notamment robuste aux variations d’orthographe ;\nLe modèle a été optimisé pour que son entraînement soit particulièrement rapide.\n\nA l’Insee, plusieurs modèles de classification de libellés textuels dans des nomenclatures reposent sur l’algorithme de plongement lexical FastText.\n\n\n\nIllustration du modèle fastText\n\n\n\nIllustration du fonctionnement du modèle fastText sur un libellé de profession\n\n \n\n\nComment utiliser ces modèles en pratique ?\nCollecter à nouveau les données ayant servi à entrainer un modèle puis le ré-entraîner implique énormément de ressources, ce qui est coûteux en temps et peu écologique5.\nEn Python, plusieurs librairies proposent les modèles Word2Vec, GloVe, BERT ou FastText. Le package gensim les met toutes en œuvre à l’exception de BERT. Ce dernier est disponible sur HuggingFace, la principale plateforme de mise à disposition de modèles pré-entraînés. Il est ainsi possible d’utiliser BERT avec les librairies PyTorch ou Keras. Chacun des modèles présentés possède également son package dédié, généralement développé par l’équipe de recherche ayant entraîné le modèle.\nEn R, il faut utiliser les packages word2vec, text2vec (pour le modèle GloVe) et fastTextR."
  },
  {
    "objectID": "blog/embedding/index.html#bonus-le-plongement-lexical-en-version-ludique",
    "href": "blog/embedding/index.html#bonus-le-plongement-lexical-en-version-ludique",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "Bonus : le plongement lexical en version ludique",
    "text": "Bonus : le plongement lexical en version ludique\nLe résultat d’un plongement lexical peut avoir de nombreux usages. Il rend notamment possible le calcul de la proximité entre deux mots quelconques.\nUne manière de procéder est de calculer la similarité cosinus entre les vecteurs de plongement des deux mots. Plus précisément, la similarité entre deux mots de représentations vectorielles \\(u\\) et \\(v\\) est définie comme le cosinus de leur angle \\( \\) : \\[cos(\\theta) = \\frac{u \\cdot v}{\\lVert u\\rVert \\lVert v\\rVert}\\]\n\n\n\nIllustration de la similarité cosinus\n\n\n\nIllustration de la similarité cosinus en deux dimensions\n\n \nLe calcul de la proximité entre les mots est à la base du jeu cemantix. Le principe est proche du jeu Wordle mais s’en distingue sur un point : il y a certes un mot à trouver chaque jour et il s’agit de faire des propositions de mots mais le jeu répond en donnant la proximité entre les mots proposés et le mot du jour. Ainsi, au fil des propositions, on a une vision de plus en plus précise du champ lexical associé au mot mystère, jusqu’à finalement le trouver."
  },
  {
    "objectID": "blog/embedding/index.html#footnotes",
    "href": "blog/embedding/index.html#footnotes",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nLa nomenclature PCS (professions et catégories socioprofessionnelles) sert à la codification des professions dans le recensement et les enquêtes auprès des ménages. Elle permet ainsi de classer un ensemble de professions dans une même catégorie. Par exemple, dans sa dernière version (PCS 2020), la catégorie des “Professions libérales de santé” (31A) regroupe diverses professions médicales: médecins libéraux, dentistes, psychologues, vétérinaires, pharmaciens libéraux… Une description plus complète de cette nomenclature et de son historique est disponible sur le site de l’Insee↩︎\nLa NAF (nomenclature d’activités française), est une nomenclature des activités économiques productives, principalement élaborée pour faciliter l’organisation de l’information économique et sociale. Il s’agit d’une typologie facilitant la représentation de l’économie sous forme de secteurs. Par exemple, au sein de l’industrie manufacturière (section C), la NAF distingue les industries alimentaires de l’industrie de l’habillement ou de l’industrie automobile. Une description plus complète de cette nomenclature et de son historique est disponible sur le site de l’Insee↩︎\nLe Code Officiel Géographique est le référentiel permettant de relier des adresses, des noms de communes ou encore des noms de collectivités locales à un identifiant unique. Pour plus d’informations, voir le site de l’Insee↩︎\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation↩︎\nStrubell, Ganesh, and McCallum (2019) estiment que l’entraînement d’un modèle à l’état de l’art dans le domaine du NLP nécessite autant d’énergie que ce que consommeraient cinq voitures, en moyenne, au cours de l’ensemble de leur cycle de vie.↩︎"
  },
  {
    "objectID": "blog/parquetRP/index.html",
    "href": "blog/parquetRP/index.html",
    "title": "Guide d’utilisation des données du recensement de la population au format Parquet",
    "section": "",
    "text": "Ce guide présente quelques exemples d’utilisation des données du recensement de la population diffusées au format Parquet. Il s’agit d’une version HTML enrichissant le guide publié sur le site insee.fr pour les langages Python  et  avec des exemples interactifs pouvant être construits par le biais de Quarto Markdown et Observable.\nL’ensemble des codes utilisés pour produire cette note est disponible sur le dépôt Github  InseeFrLab/exemples-recensement-parquet au format Quarto Markdown.\nPour plus d’informations sur le format Parquet, dans un contexte de statistique publique, se référer à Dondon et Lamarche (2023). Pour un exemple sur la différence entre format CSV et Parquet illustré sur les données du recensement de la population, voir Mauvière (2022).\nCe guide propose d’utiliser DuckDB à travers plusieurs langages pour effectuer des traitements sur les fichiers détails du recensement. Par rapport à d’autres approches, DuckDB a été choisi pour son efficacité ainsi que pour son universalité1."
  },
  {
    "objectID": "blog/parquetRP/index.html#requêtes-sur-les-colonnes-select",
    "href": "blog/parquetRP/index.html#requêtes-sur-les-colonnes-select",
    "title": "Guide d’utilisation des données du recensement de la population au format Parquet",
    "section": "Requêtes sur les colonnes (SELECT)",
    "text": "Requêtes sur les colonnes (SELECT)\nLa liste des colonnes à extraire du fichier peut être renseignée avec la clause SELECT. Celles-ci peuvent être renommées en appliquant au passage la clause AS.\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nInputs.table(\n    db.query(\"SELECT IPONDI AS poids, AGED, VOIT FROM table_individu LIMIT 10\")\n)\n```\n\n\nduckdb.sql(\"SELECT IPONDI AS poids, AGED, VOIT FROM table_individu LIMIT 10\")\n\n\ndbGetQuery(\n  con,\n  \"SELECT IPONDI AS poids, AGED, VOIT FROM table_individu LIMIT 10\"\n)\n\n\ntable_individu %&gt;%\n  select(poids = IPONDI, AGED, VOIT) %&gt;%\n  head(10)\n\n\n\n\nInputs.table(\n    db.query(\"SELECT IPONDI AS poids, AGED, VOIT FROM table_individu LIMIT 10\")\n)\n\n\n\n\n\n\nDuckDB propose également des fonctionnalités pour extraire des colonnes à travers des expressions régulières. De nombreux exemples peuvent être trouvés sur cette page.\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nInputs.table(\n    db.query(\"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') FROM table_individu LIMIT 10\")\n)\n```\n\n\nduckdb.sql(\"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') FROM table_individu LIMIT 10\")\n\n\ndbGetQuery(\n  con,\n  \"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') FROM table_individu LIMIT 10\"\n)\n\n\ntable_individu %&gt;%\n  select(poids = IPONDI, contains(\"AGE\")) %&gt;%\n  head(10)\n\n\n\n\nInputs.table(\n    db.query(\"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') FROM table_individu LIMIT 10\")\n)"
  },
  {
    "objectID": "blog/parquetRP/index.html#requêtes-sur-les-lignes-where",
    "href": "blog/parquetRP/index.html#requêtes-sur-les-lignes-where",
    "title": "Guide d’utilisation des données du recensement de la population au format Parquet",
    "section": "Requêtes sur les lignes (WHERE)",
    "text": "Requêtes sur les lignes (WHERE)\nPour extraire un sous-échantillon des données complètes, la clause WHERE permet d’appliquer des filtres à partir de conditions logiques. Par exemple, il est possible de ne conserver, du fichier national, que les données de l’Aude (11), de la Haute-Garonne (31) et de l’Hérault (34).\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nInputs.table(\n    db.query(\"SELECT * FROM table_individu WHERE DEPT IN ('11', '31', '34') LIMIT 10\")\n)\n```\n\n\nduckdb.sql(\"SELECT * FROM table_individu WHERE DEPT IN ('11', '31', '34')\")\n\n\ndbGetQuery(\n  con,\n  \"SELECT * FROM table_individu WHERE DEPT IN ('11', '31', '34')\"\n)\n\n\ntable_individu %&gt;%\n  filter(DEPT %in% c(\"11\", \"31\", \"34\")) %&gt;%\n  head(10)\n\n\n\n\nInputs.table(\n    db.query(\"SELECT * FROM table_individu WHERE DEPT IN ('11', '31', '34') LIMIT 10\")\n)\n\n\n\n\n\n\nIl est également possible de formater cette liste telle qu’attendue par SQL à partir d’une liste Python ou d’un vecteur R plus classique. Pour cela, le code suivant peut servir de modèle :\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nliste_regions = [\"11\", \"31\", \"34\"]\nliste_regions_sql = liste_regions.map(item =&gt; `'${item}'`).join(\",\")\nInputs.table(\n    db.query(`SELECT * FROM table_individu WHERE DEPT IN (${liste_regions_sql}) LIMIT 10`)\n)\n```\n\n\ncon = duckdb.connect()\n\ncon.execute('''\n  CREATE OR REPLACE VIEW table_individu\n  AS SELECT * FROM read_parquet(\"FD_INDCVI_2020.parquet\")\n'''\n)\n\nliste_regions = [\"11\", \"31\", \"34\"]\n\ndep_slots = \", \".join([\"?\" for _ in liste_regions])\nquery = \"SELECT * FROM table_individu WHERE DEPT IN ({})\".format(dep_slots)\nliste_regions_sql = \", \".join([f\"'{dep}'\" for dep in liste_regions])\ncon.execute(query, liste_regions).fetchdf()\n\n\nliste_regions &lt;- c(\"11\", \"31\", \"34\")\nliste_regions_sql &lt;- glue_sql_collapse(\n  lapply(\n    liste_regions, function(dep) glue_sql(\"'{`dep`}'\", .con=con)\n    ),\n  \", \"\n)\nquery &lt;- glue_sql(\n  \"SELECT * FROM table_individu WHERE DEPT IN ({liste_regions_sql})\",\n  .con=con\n)\ndbGetQuery(con, query)\n\n\nliste_regions &lt;- c(\"11\", \"31\", \"34\")\ntable_individu %&gt;%\n  filter(DEPT %in% liste_regions)\n\n\n\n\nliste_regions = [\"11\", \"31\", \"34\"]\nliste_regions_sql = liste_regions.map(item =&gt; `'${item}'`).join(\",\")\nInputs.table(\n    db.query(`SELECT * FROM table_individu WHERE DEPT IN (${liste_regions_sql}) LIMIT 10`)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour en savoir plus sur les prepared statements avec DuckDB en Python, et plus généralement pour avoir des exemples d’utilisations différentes, c’est ici que ça se passe.\n\nLes filtres sur les observations peuvent être faits à partir de critères sur plusieurs colonnes. Par exemple, pour ne conserver que les observations de la ville de Nice où la date d’emménagement est postérieure à 2020, la requête suivante peut être utilisée :\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nInputs.table(\n    db.query(\n        \"SELECT * FROM table_logement WHERE COMMUNE = '06088' and AEMM &gt; 2020\"\n    )\n)\n```\n\n\nquery = \"SELECT * FROM table_logement WHERE COMMUNE = '06088' and AEMM &gt; 2020\"\nduckdb.sql(query)\n\n\ndbGetQuery(\n  con,\n  \"SELECT * FROM table_logement WHERE COMMUNE = '06088' and AEMM &gt; 2020\"\n)\n\n\ntable_logement %&gt;%\n  filter(COMMUNE == \"06088\") %&gt;%\n  filter(AEMM &gt; 2020)\n# Peut aussi s'écrire en une fois :\n# table_logement %&gt;% filter(COMMUNE == \"06088\", AEMM &gt; 2020)\n\n\n\n\nInputs.table(\n    db.query(\n        \"SELECT * FROM table_logement WHERE COMMUNE = '06088' and AEMM &gt; 2020\"\n    )\n)"
  },
  {
    "objectID": "blog/parquetRP/index.html#footnotes",
    "href": "blog/parquetRP/index.html#footnotes",
    "title": "Guide d’utilisation des données du recensement de la population au format Parquet",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nDes propositions d’enrichissements de cette documentation à partir d’implémentations alternatives, par exemple s’appuyant sur Arrow et dbplyr ou sur Polars sont bienvenues sur le Github InseeFrLab/exemples-recensement-parquet.↩︎"
  },
  {
    "objectID": "blog/recap2022/index.html",
    "href": "blog/recap2022/index.html",
    "title": "Infolettre n°9",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit\nLa rétrospective de l’année 2022 promettait une version plus personnalisée, inspirée des visualisations proposées par les réseaux sociaux pour synthétiser l’activité de leurs utilisateurs.\nCette newsletter un peu spéciale propose un retour sur la première année du réseau des data scientists de la statistique publique dont la préfiguration a commencé en mars 2022 et qui a été lancé officiellement en septembre. Vous pourrez retrouver à la fin de la newsletter des informations plus classiques: événements, retour sur les actions du réseau, formations, etc.\nElle permet aussi d’illustrer le potentiel d’outils qui ont été présentés dans la rétrospective de l’année 2022. Toutes les figures sont réactives, notamment quand vous passez votre souris dessus. Les principaux ingrédients qui ont été ici utilisés, et qui avaient été mentionnés dans la première partie de la rétrospective, sont Observable, Quarto et DuckDB. Les données sont stockées sur le système de stockage S3 du SSPCloud."
  },
  {
    "objectID": "blog/recap2022/index.html#lannée-du-réseau",
    "href": "blog/recap2022/index.html#lannée-du-réseau",
    "title": "Infolettre n°9",
    "section": "L’année du réseau",
    "text": "L’année du réseau\nLe réseau comporte deux canaux de communication: une liste de diffusion mail et un canal de discussions instantanées. Intéressons nous d’abord à la liste de diffusion mail !\n\n\n\nPendant l’année 2022, 7 newsletters ont été diffusées par mail. Chacune a permis d’augmenter sensiblement le nombre de personnes dans la liste de diffusion. A la fin de l’année, il y avait 312 inscrits1 dans la liste de diffusion.\nLe réseau a organisé trois événements pendant l’année 2022. D’abord, avant l’été, deux open hours ont eu lieu. Cet événement informel prenant la forme de retour d’expérience a été l’occasion de discussions stimulantes autour de d’usage de la data science pour l’administration. En novembre, l’événement autour d’Observable animé par Nicolas Lambert a réuni près de 50 personnes."
  },
  {
    "objectID": "blog/recap2022/index.html#répartition-des-modes-daccès-au-réseau",
    "href": "blog/recap2022/index.html#répartition-des-modes-daccès-au-réseau",
    "title": "Infolettre n°9",
    "section": "Répartition des modes d’accès au réseau",
    "text": "Répartition des modes d’accès au réseau\nLe réseau propose deux canaux de diffusion de l’information: une liste de diffusion par mail et un canal de discussion instantanée qui utilise la messagerie sécurisée de l’Etat Tchap. Environ 55% des membres de la liste de diffusion (soit plus de 180 personnes) sont également inscrits sur le canal de discussion instantanée."
  },
  {
    "objectID": "blog/recap2022/index.html#composition-du-réseau",
    "href": "blog/recap2022/index.html#composition-du-réseau",
    "title": "Infolettre n°9",
    "section": "Composition du réseau",
    "text": "Composition du réseau\nLa diffusion d’informations par le réseau a permis de réunir des data scientists de 27 organisations différentes. L’Insee, qui représente 47% de l’effectif du réseau, est majoritaire. Suivent dans le palmarès, les services statistiques du Ministère de la Santé (DREES) et du Ministère du Développement Durable (SDES)."
  },
  {
    "objectID": "blog/recap2022/index.html#évolution-de-la-composition-du-réseau",
    "href": "blog/recap2022/index.html#évolution-de-la-composition-du-réseau",
    "title": "Infolettre n°9",
    "section": "Évolution de la composition du réseau",
    "text": "Évolution de la composition du réseau\nLa diffusion progressive d’informations par le biais des newsletters a permis de diversifier progressivement la composition de la liste de diffusion. Alors que la première newsletter de l’année 2022 avait été diffusée auprès de 14 institutions, ce sont des agents de 27 organisations qui ont reçues la dernière.\nLes événements organisés par le réseau ou les présentations spéciales, comme celle pour les administrateurs de l’INSEE en poste à l’ENSAE, ont également pu motiver des personnes à intégrer le réseau."
  },
  {
    "objectID": "blog/recap2022/index.html#programme-10",
    "href": "blog/recap2022/index.html#programme-10",
    "title": "Infolettre n°9",
    "section": "Programme 10%",
    "text": "Programme 10%\nLes membres du réseau des data scientists ont été particulièrement actifs dans le cadre du programme interministériel 10%, issu des recommandations d’un rapport INSEE-DINUM “Évaluation des besoins de l’État en compétences et expertises en matière de donnée”.\nLa saison 1 a donné sa chance à quatre projets, portés par différentes administrations. Si l’un d’eux existait déjà depuis plus de deux ans (projet Gouvdown), trois sont nés pour l’occasion, avec la mise en ligne de code immédiate (Cartiflette) ou postérieure au bootcamp de lancement (Socratext et matchSIRET) .\nTous les projets sont ouverts et disponible sur Github. Une statistique qui permet de représenter leur succès est le nombre de ⭐: c’est un peu un mélange entre un site en favori sous Firefox puisque cela permet de facilement retrouver un projet dans Github et le nombre de followers d’une page sur Facebook ou sur Twitter puisque cela permet de suivre l’activité d’un dépôt Github.\n\n\n\n\n\n\nNote\n\n\n\nCette visualisation fait appel à l’API Github. Si les figures ne s’affichent pas, cela peut être dû à un dépassement du nombre de requêtes par heure autorisées par l’API Github sans jeton. A l’heure actuelle, il n’existe pas encore de fonctionalité gratuite sous Observable pour stocker de manière sécurisée un jeton pour l’API Github.\n\n\nDérouler pour afficher une version non réactive\n\n\n\n\nProjet cartiflette\n\n\n\n\n\nProjet Socratext\n\n\n\n\n\nProjet Gouvdown\n\n\n\n\n\nProjet matchSIRET"
  },
  {
    "objectID": "blog/recap2022/index.html#autres-actualités-du-réseau",
    "href": "blog/recap2022/index.html#autres-actualités-du-réseau",
    "title": "Infolettre n°9",
    "section": "Autres actualités du réseau",
    "text": "Autres actualités du réseau\n\nPrésentation de Gridviz par Julien Gaffuri\nPour rappel, le 20 Janvier 2023 de 11h à 12h30 Julien Gaffuri (Eurostat) viendra nous présenter la librairie open-source Gridviz. Réservez ce créneau pour découvrir cette librairie qui ouvre de nouvelles perspectives pour la mise à disposition de données géographiques !\nTélécharger l’invitation à l’événement sous format Outlook\n\n\n\nSource: Notebook Hello Gridviz par neocarto sur Observable\n\n\n\n\nPremière place européenne au hackathon Big Data de l’ONU\nLes résultats du hackathon big data de l’ONU, ayant eu lieu du 7 au Novembre 2022, ont été annoncés ! L’équipe Datadive - constituée de membres du réseau de l’INSEE, de la DGFIP et du CASD - est arrivée à la première place des équipes européennes 🎉.\n\n\nGit et bonnes pratiques: des formations de formateurs prévus pour les statisticiens publics\nLes nouvelles formations à Git et aux bonnes pratiques avec R, testées récemment à l’Insee et au service statistique du Ministère du Travail, la DARES, (voir newsletters de Novembre et Décembre), deviennent des formations nationales.\nPour pouvoir diffuser les bonnes pratiques favorisant le partage de codes et la qualité des projets statistiques, il est nécessaire d’avoir le plus d’enseignants possibles pour cette formation. Pour permettre cela, un appel à candidat pour une formation de formateurs a été diffusée à l’Insee et dans les services statistiques ministériels. Si vous êtes intéressés et ne l’avez pas reçu, n’hésitez pas à envoyer un mail à contact-ssphub@insee.fr.\nEn attendant, les supports de ces formations sont déjà disponibles sur inseefrlab.github.io/formation-bonnes-pratiques-git/ et sur inseefrlab.github.io/formation-bonnes-pratiques-R/. Les codes sources sont bien-sûr ouverts et disponibles sur Github, tant pour la première partie que pour la seconde. Ceux-ci sont construits collectivement, n’hésitez pas à suggérer des modifications depuis Github.\nUn site web plus complet devrait prochainement voir le jour pour accompagner cette formation. En complément de celui-ci, des éléments peuvent déjà être trouvés dans le cours de 3e année de l’ENSAE sur la mise en production de projets data science et dans la documentation collaborative utilitR."
  },
  {
    "objectID": "blog/recap2022/index.html#footnotes",
    "href": "blog/recap2022/index.html#footnotes",
    "title": "Infolettre n°9",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nLes actions de communication du mois de janvier ont permis d’augmenter sensiblement le nombre de personnes dans cette liste (340 début janvier). Un retour spécial sur le mois de janvier sera l’occasion idéale pour une autre rétrospective quantitative.↩︎"
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Ressources utiles",
    "section": "",
    "text": "Une sélection de ressources utiles pour se former ou se perfectionner à la data science.\nN’hésitez pas à soumettre les ressources que vous jugez utiles sur notre GitHub .\n\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Appariements de données individuelles : concepts, méthodes, conseils\n            \n\n            \n              Un appariement consiste à rapprocher deux bases de données d’origine distincte partageant des unités statistiques communes mais contenant des informations différentes. Ce document de travail porte sur les cas où un identifiant commun n'existe pas et introduit en pratique les appariements de données individuelles sur traits d'identité.\n            \n\n            \n            \n              \n                \n                3 juil. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Les réseaux de neurones appliqués à la statistique publique : méthodes et cas d’usages\n            \n\n            \n              Ce document de travail propose une introduction rapide aux réseaux de neurones, de leurs fondements théoriques jusqu’à leur mise en oeuvre pratique en R et python sur des problématiques spécifiques de statistique publique. Il  illustre les possibilités et les limites à travers trois cas d’usage détaillés sur  l’imputation de valeurs manquantes dans une enquête, l’exploitation de fichiers d’images et la réduction de dimension.\n            \n\n            \n            \n              \n                \n                16 févr. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Parcours complet sur le MLOps\n            \n\n            \n              Un site web très complet qui fait un effort\nde synthèse sur l'écosystème foisonnant du MLOps\n\n            \n\n            \n            \n              \n                \n                1 févr. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Neural Network: from zero to hero\n            \n\n            \n              A course by Andrej Karpathy on building neural networks, from scratch, in code.\n\n\n  \"We start with the basics of backpropagation and build up to modern deep neural networks, like GPT. Language models are an excellent place to learn deep learning, even if your intention is to eventually go to other areas like computer vision because most of what you learn will be immediately transferable.\"\n\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Observable pour la cartographie\n            \n\n            \n              Nicolas Lambert (neocarto) propose\nbeaucoup de ressources pédagogiques sur la cartographie depuis Observable.\nBeaucoup de ressources s'appuient\nsur bertin.js,\nune librairie très puissante et flexible pour la représentation cartographique.\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Data visualisation with d3.js\n            \n\n            \n              d3.js est la librairie favorite des spécialistes de\ndataviz en Javascript. Dans cette série de notebooks,\nproposée par Arthur Katossky,\nvous découvrirez comment utiliser la librairie pour\nconstruire des visualisations de données réactives.\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Découvrir Observable avec des données françaises\n            \n\n            \n              Eric Mauvière propose\nbeaucoup de ressources pédagogiques sur la plateforme Observable.\nBeaucoup s'appuient\nsur des données de la statistique publique, comme le fichier des\nprénoms ou le recensement.\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Introduction à Observable Plot.js\n            \n\n            \n              La librairie Plot.js\nvise à faciliter l'utilisation des fonctionnalités graphiques de\nJavascript . Elle propose une syntaxe très proche de celle\ndes librairies ggplot2 () ou seaborn (Python ).\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Python pour la data science\n            \n\n            \n              Un site web complet pour découvrir la richesse\nde Python  pour la data science. Ce cours\nest enseigné par Lino Galiana en\ndeuxième année (Master 1) de l'ENSAE.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Mise en production de projets de data science\n            \n\n            \n              Un site web complet pour découvrir la manière dont\ndes projets data-science peuvent être valorisés\net maintenus dans le temps.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Portail de la formation du SSPCloud\n            \n\n            \n              Le SSPCloud, la plateforme cloud\ndéveloppée par l'Insee, propose\nun certain nombre de tutoriels\nen Python  ou R .\n\n            \n\n            \n            \n              \n                \n                27 avr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Géomatique appliquée à la statistique\n            \n\n            \n              Une présentation simple des outils géomatiques récents qui permettent de stocker, traiter et diffuser l'information spatiale\n\n            \n\n            \n            \n              \n                \n                17 févr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            L'économétrie en grande dimension\n            \n\n            \n              Ce document de travail est une courte introduction aux principaux problèmes que l'on rencontre lorsque l'on souhaite faire de l'économétrie en grande dimension, c'est-à-dire lorsque p &gt; n - pour chaque observation, on dispose d'un nombre de caractéristiques potentiellement proportionnel ou plus grand que la taille de l'échantillon.\n\n            \n\n            \n            \n              \n                \n                17 févr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Utiliser Git dans Jupyter Notebook ?\n            \n\n            \n              Un extrait du cours\nde Python pour la data-science\nde l'ENSAE.\n\n            \n\n            \n            \n              \n                \n                1 janv. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            dataESR: portail de l'open-data du Ministère de l'Enseignement Supérieur\n            \n\n            \n              #dataESR est un portail développé\npar le service statistique du Ministère de l'Enseignement Supérieur et de la Recherche\npour vous aider à trouver les ressources en données sur l'enseignement supérieur, la recherche et l'innovation.\n\n            \n\n            \n            \n              \n                \n                1 janv. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            DevOps for Data Science\n            \n\n            \n              Un site web très complet sur la manière\ndont l'approche DevOps peut être\nimportée dans des projets data-science.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            R reproducibility toolkit for the practical researcher\n            \n\n            \n              Un site web très complet développé par des universitaires\nsud-américains pour présenter la manière dont les\nprojets en R peuvent être construits de manière\nreproductible.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            R for Data Science\n            \n\n            \n              Un incontournable écrit par Hadley Wickham\nafin de faire découvrir l'univers du\ntidyverse (ggplot, dplyr...) de manière\nthématique.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            R Packages\n            \n\n            \n              Un incontournable écrit par Hadley Wickham\nafin d'apprendre à développer\ndes packages R.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            RZine, une collection de ressources utiles en R\n            \n\n            \n              Un site web développé par l'université Paris Diderot\ncomportant de nombreuses ressources utiles en R.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Machine Learning in Python with scikit-learn\n            \n\n            \n              Un MOOC de l'INRIA sur scikit-learn, l'écosystème\ncentral du Machine Learning en Python.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            utilitR\n            \n\n            \n              \n  Le projet utilitR est une documentation sur l’usage du logiciel\n  , née à l’Insee,\n  destinée à tout utilisateur intéressé par la manipulation de données.\n\n\n\n  La documentation utilitR ne fait aucun pré-requis de niveau:\n  à la fois le débutant\n  et l'utilisateur plus expert désirant découvrir un nouveau champ ou bénéficier\n  d'une aide-mémoire pourront trouver du contenu qui les intéresse.\n\n\n\n  Afin que les exemples soient concrets,\n  tous les jeux de données sont issus du\n  site de l'Insee.\n\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            The Hitchhiker's guide to Python\n            \n\n            \n              Un livre de référence\nsur les bonnes pratiques pour des\nprojets Python .\n\n            \n\n            \n            \n              \n                \n                27 avr. 2016\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Google’s R Style Guide\n            \n\n            \n              Un guide de référence sur les bonnes pratiques\ndans l'écriture de code R\n\n            \n\n            \n            \n              \n                \n                27 avr. 2016\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Formation Initiation au Deep Learning (FIDLE)\n            \n\n            \n              Une formation du CNRS sur les problématiques\nd'apprentissage automatique (machine learning)\net d'apprentissage profond (deep learning).\n\n            \n\n            \n            \n              \n                \n                27 avr. 2016\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Apprendre Pandas en 10 minutes !\n            \n\n            \n              Un tutoriel des créateurs de Pandas pour apprendre en\npeu de temps à manipuler et analyser\nles données sous Python .\n\n            \n\n            \n            \n              \n                \n                27 avr. 2016\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "event/2021-06-22-funathon/index.html",
    "href": "event/2021-06-22-funathon/index.html",
    "title": "Funathon de juin 2021",
    "section": "",
    "text": "Présentation\nLa première édition du Funathon a eu lieu le 21 et 22 juin 2021 et a réuni environ 150 agents, souvent réunis en équipe. Cette première édition a tourné autour de l’utilisation des données d’Airbnb, 8 sujets plus ou moins guidés étant proposés, avec chaque fois des notebooks (en R et en Python) permettant d’entrer facilement dans les sujets. Tous les niveaux d’expertise étaient les bievenus. Merci à tous !\n\n\nDétails du programme des deux jours\nCher.e.s Funambules,\nVous allez pouvoir vous consacrer cœur et âme pendant deux jours aux joies de la Datascience. Cette expérience sera à la fois longue et courte. Longue car nos calendriers surchargés nous permettent rarement de travailler en collaboration sur un sujet statistique. Mais trop courte, car s’approprier les données, imaginer leurs usages, tester des méthodes de Machine Learning demandent du temps et de la patience. Avant de commencer votre exploration, nous vous recommandons de bien lire les documents et les aides mis à disposition (nous remercions l’équipe du SSPLab pour son coup de main). Nous vous suggérons également de ne pas négliger la partie statistique descriptive, sans quoi vous risquez de mal interpréter les résultats de vos analyses plus complexes.\nLes données proviennent du site Inside Airbnb qui est devenu un acteur de premier plan dans la connaissance de Airbnb. Soutenu par les grandes métropoles européennes et mondiales, le site propose des données scrappées du site Airbnb. Ces données sont utilisées par les agences d’urbanismes (Urban planning) et la recherche académique pour documenter les conséquences des locations Airbnb sur les systèmes urbains. Ces données contiennent par construction des informations personnelles éventuellement identifiantes. Il est interdit d’en faire un usage commercial, ce qui va de soi dans le cadre du Funathon qui est une opération de formation et d’appropriation des outils de Machine Learning. En revanche, en tant que données personnelles, elles doivent être traitées par nous les Funambules, acteurs de la connaissance statistique, avec la même exigence de rigueur et d’éthique que toutes autres sources de données statistiques ou administratives.\nPendant ces deux jours, certain.e.s Funambules vont découvrir un nouvel outil de travail, le Datalab. Cet outil que nous offre l’équipe de la DIIT (que nous remercions chaleureusement) est l’Armageddon de la Datascience. Sa puissance est redoutable et grisante. En contrepartie, en tant qu’outil en devenir, son appropriation demande plus de temps. N’hésitez pas à consulter le canal général de slack pour poser des questions et y trouver éventuellement des réponses à vos interrogations.\nLa Datascience n’est pas seulement l’apanage des plus aguerri.e.s ou des plus virtuoses en informatique. Les esprits curieux ont toute leur place pour penser les données et proposer des approches innovantes. La critique dans les équipes des résultats obtenus nous semble indispensable et ce d’autant plus que les données ne sont pas parfaites. Vos analyses seront forcément biaisées et il est important de connaître et discuter leurs limites.\nN’oubliez pas, nous sommes des nains sur des épaules de géants, alors n’hesitez pas à recopier du code.\nL’ensemble du materiel des deux jours est disponible ici. Premier élément à ouvrir : 0_Kit_De_Demarrage. Puis, laissez vous guider !"
  },
  {
    "objectID": "event/2024-05-02-quarto/index.html",
    "href": "event/2024-05-02-quarto/index.html",
    "title": "Quarto : Une évolution de R Markdown pour des travaux statistiques reproductibles",
    "section": "",
    "text": "2 mai (15h - 16h30)\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://cderv.github.io/2024-quarto-evolution-rmd/#/title-slide\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Voir les slides\n  &lt;/a&gt;\n&lt;/p&gt;`"
  },
  {
    "objectID": "event/2024-05-02-quarto/index.html#christophe-dervieux-quarto-une-évolution-de-r-markdown-pour-des-travaux-statistiques-reproductibles",
    "href": "event/2024-05-02-quarto/index.html#christophe-dervieux-quarto-une-évolution-de-r-markdown-pour-des-travaux-statistiques-reproductibles",
    "title": "Quarto : Une évolution de R Markdown pour des travaux statistiques reproductibles",
    "section": "",
    "text": "2 mai (15h - 16h30)\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://cderv.github.io/2024-quarto-evolution-rmd/#/title-slide\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Voir les slides\n  &lt;/a&gt;\n&lt;/p&gt;`"
  },
  {
    "objectID": "event/2025-04-09-API/index.html",
    "href": "event/2025-04-09-API/index.html",
    "title": "Atelier - Comment récupérer des données par API ?",
    "section": "",
    "text": "L’atelier a eu lieu le 9 avril 2025 (15h - 16h30), en présentiel à l’Insee et en distanciel pour les membres du réseau du SSP Hub. Environ 35 personnes ont participé de l’Insee (DG ou directions régionales), de différents services statistiques ministériels ou d’autres horizons. Merci à tous pour les échanges !\n\nSlides de la présentation\n\n\n\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"https://inseefrlab.github.io/ssphub-ateliers-slides/slides-data/api.html#/title-slide\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Voir les slides\n  &lt;/a&gt;\n&lt;/p&gt;`\n\n\n\n\n\n\n\n\nDocumentation de l’atelier & replay\nLe matériel lié à l’atelier, y compris le replay, est disponible ici. \n\n\nQuestions / contact\nSi vous avez la moindre question 🤨, n’hésitez pas à contacter 📧 contact-ssphub@insee.fr."
  },
  {
    "objectID": "event/2025-12-01-network-day/index.html",
    "href": "event/2025-12-01-network-day/index.html",
    "title": "Troisième journée du SSPHub",
    "section": "",
    "text": "Inscription\n👉️ Les inscriptions se font ici. 👈️ Possibilité de participer en présentiel et en ligne.\n\n\nAgenda\n\n9h30-10h: Accueil, moment de convivialité autour d’un café.\nAgenda en construction\n17h30-19h30: Pot, moment de convivialité.\n\nPour toute information : contact-ssphub@insee.fr\n📺️ La deuxième journée du réseau ayant eu lieu en 2024 est également disponible en replay."
  },
  {
    "objectID": "event/presentation-de-gridviz-par-julien-gaffuri/index.html",
    "href": "event/presentation-de-gridviz-par-julien-gaffuri/index.html",
    "title": "Présentation de gridviz par Julien Gaffuri",
    "section": "",
    "text": "Présentation par Julien Gaffuri de la librairie Gridviz. Le replay est disponible plus bas 👇\nGridviz est une librairie open source (disponible sur Github) consacrée à la visualisation cartographique de données carroyées (ou données géolocalisées par des (x,y) à carroyer). Très efficace, elle permet de représenter de manière fluide des volumes importants de données."
  },
  {
    "objectID": "event/presentation-de-gridviz-par-julien-gaffuri/index.html#replay",
    "href": "event/presentation-de-gridviz-par-julien-gaffuri/index.html#replay",
    "title": "Présentation de gridviz par Julien Gaffuri",
    "section": "Support et replay",
    "text": "Support et replay\nLe support est disponible ici."
  },
  {
    "objectID": "event/presentation-dobservable-par-nicolas-lambert/index.html",
    "href": "event/presentation-dobservable-par-nicolas-lambert/index.html",
    "title": "Présentation d’Observable par Nicolas Lambert",
    "section": "",
    "text": "Présentation d’observable par Nicolas Lambert sous la forme d’un notebook interactif.\nLe replay est disponible plus bas 👇\nobservable est une plateforme de dataviz réactive qui propose des notebooks communautaires de visualisations de données:\nQuelques ressources supplémentaires utiles:"
  },
  {
    "objectID": "event/presentation-dobservable-par-nicolas-lambert/index.html#replay",
    "href": "event/presentation-dobservable-par-nicolas-lambert/index.html#replay",
    "title": "Présentation d’Observable par Nicolas Lambert",
    "section": "Replay",
    "text": "Replay"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SSPHub",
    "section": "",
    "text": "// echo: false\n// output: false\ninscrits = 730\n// echo: false\nbadge = html`&lt;a href=\"https://grist.numerique.gouv.fr/o/ssphub/forms/jSjAV3L2F8mmiRVuVEpfF7/103\"&gt;&lt;img alt=\"Static Badge\" src=\"https://img.shields.io/badge/${inscrits}_inscrits-blue?style=social&label=⭐️%20Rejoindre%20la%20liste%20de%20diffusion&color=8A2BE2&link=https%3A%2F%2Fgrist.numerique.gouv.fr%2Fo%2Fssphub%2Fforms%2FjSjAV3L2F8mmiRVuVEpfF7%2F103\"&gt;\n&lt;/a&gt;\n`"
  },
  {
    "objectID": "index.html#les-derniers-billets-de-blog-et-événements",
    "href": "index.html#les-derniers-billets-de-blog-et-événements",
    "title": "SSPHub",
    "section": "Les derniers billets de blog et événements",
    "text": "Les derniers billets de blog et événements\nL’ensemble des billets de blog peut être retrouvé sur la page dédiée, tout comme les événements.\n\n\n\n\n\n\n\n\n\n\nTroisième journée du SSPHub\n\n\nProgramme et modalités d’inscription à la 3e journée du réseau\n\n\n\n\n\n\n15 sept. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtelier - Comment récupérer des données sous format Parquet ?\n\n\nLe format Parquet est un format de données connaissant une popularité importante du fait de ses caractéristiques techniques (orientation colonne, compression…\n\n\n\n\n\n\n16 avr. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtelier - Comment récupérer des données par API ?\n\n\nLes API (Application Programming Interface) sont un mode d’accès aux données en expansion. Grâce aux API, l’automatisation de scripts est facilitée puisqu’il n’est plus…\n\n\n\n\n\n\n9 avr. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeuxième journée du SSPHub\n\n\nProgramme et modalités d’inscription à la 2e journée du réseau\n\n\n\n\n\n\n14 oct. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto : Une évolution de R Markdown pour des travaux statistiques reproductibles\n\n\nPour fiabiliser la production de documents construits en valorisant des données (tableaux, graphiques, etc.), RStudio (devenu Posit depuis) a construit il y a quelques…\n\n\n\n\n\n\n2 mai 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nEric Mauvière, “La dataviz pour donner du sens aux données et communiquer un message”\n\n\nLe 29 février (15h - 16h), Eric Mauvière nous fera une présentation, avec de nombreux exemples issus de la statistique publique, de la manière dont une visualisation de…\n\n\n\n\n\n\n29 févr. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide d’utilisation des données du recensement de la population au format Parquet\n\n\nUn post de blog pour accompagner la mise à disposition des données détaillées du recensement au format Parquet.\n\n\n\n\n\n\n23 oct. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnyxia: l’infrastructure cloud mère des dragons\n\n\nLes technologies cloud sont incontournables dans l’écosystème de la donnée. Pour ne pas se rendre dépendante de fournisseurs de services externes, l’Insee a développé un…\n\n\n\n\n\n\n10 mai 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremière journée du SSPHub\n\n\nReplay de la première journée de présentation du SSPHub\n\n\n\n\n\n\n29 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n“OCRisation, état de l’art et projets auxquels participe Teklia” par Christopher Kermorvant\n\n\nLe 29 mars de 15h à 16h nous recevons Christopher Kermorvant, chercheur spécialisé en OCRisation et fondateur de Teklia. Il nous fera un état de l’art de l’OCRisation puis…\n\n\n\n\n\n\n29 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation du projet Meta Academy - Carpentries\n\n\nPour favoriser l’adoption des langages R, Python et Git dans les administrations, le programme ModernStat piloté par l’OCDE et Statistics Canada, a lancé un projet…\n\n\n\n\n\n\n28 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation des packages R et Python pour accéder à l’open data de l’Insee\n\n\nL’Insee met à disposition ses données par le biais d’API ou par son site web. Pour faciliter la…\n\n\n\n\n\n\n13 févr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolars, une alternative fraîche à Pandas\n\n\nPolars, une alternative moderne et fluide à Pandas\n\n\n\n\n\n\n10 févr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation de gridviz par Julien Gaffuri\n\n\nEvénement de présentation de gridviz par Julien Gaffuri (Eurostat)\n\n\n\n\n\n\n20 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfolettre n°9\n\n\nAprès la rétrospective de l’année 2022 de la data science, il est temps de se pencher sur l’année du réseau avec des visualisations interactives produites grâce à…\n\n\n\n\n\n\n10 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRétrospective de l’année 2022\n\n\nLa data science a beaucoup fait parler d’elle en 2022, notamment du fait des deux coups médiatiques d’openAI, à savoir…\n\n\n\n\n\n\n31 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation d’Observable par Nicolas Lambert\n\n\nobservable est la nouvelle plateforme de dataviz réactive. Initiée par Mike Bostock (créateur de D3.js), ce réseau social de la dataviz a pour…\n\n\n\n\n\n\n16 nov. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe plongement lexical ou comment apprendre à lire à un ordinateur\n\n\nIntroduction aux méthodes de traitement du langage naturel.\n\n\n\n\n\n\n3 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunathon de juin 2022\n\n\nPrésentation du deuxième Funathon du SSPLab organisé le 20 juin 2022 autour de 9 sujets, en R et en Python.\n\n\n\n\n\n\n19 juin 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunathon de juin 2021\n\n\nPrésentation du premier Funathon du SSPLab organisé le 21 juin 2021 autour de 8 sujets, en R et en Python, à partir de données Airbnb\n\n\n\n\n\n\n20 juin 2021\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#les-projets-innovants-du-ssphub",
    "href": "index.html#les-projets-innovants-du-ssphub",
    "title": "SSPHub",
    "section": "Les projets innovants du SSPHub",
    "text": "Les projets innovants du SSPHub\nL’ensemble des projets innovants peut être retrouvé sur la page dédiée.\n\n\n\n\n\n\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\nUtiliser les images satellites pour améliorer le recensement de la population dans les territoire ultra-marins\n\n\n\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtraction automatique du tableau des filiales et participations des comptes sociaux des entreprises\n\n\nExtraire les informations de tableaux de comptes sociaux, en particulier des tableaux des filiales et participations, contenus dans des images scannées mises à disposition…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparaison des méthodes d’appariement et apport du machine learning\n\n\nTester et comparer différentes méthodes d’appariements afin de dégager des recommandations pour les travaux nécessaires à la construction des répertoires, notamment dans le…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#les-dernières-newsletters",
    "href": "index.html#les-dernières-newsletters",
    "title": "SSPHub",
    "section": "Les dernières newsletters",
    "text": "Les dernières newsletters\nToutes les newsletters précédemment publiées sont disponibles sur la page dédiée.\n\n\n\n\n\n\n\n\n\n\nDe belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA\n\n\nInfolettre du mois d’octobre 2025\n\n\n\n\n\n\n25 oct. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa rentrée 2025: actualités, nouveautés, interview de rentrée\n\n\nInfolettre du mois de Septembre 2025\n\n\n\n\n\n\n29 sept. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks\n\n\nInfolettre du mois de Mars 2024\n\n\n\n\n\n\n7 mars 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web\n\n\nInfolettre du mois de Février 2024\n\n\n\n\n\n\n20 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nRétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud\n\n\nInfolettre du mois de Décembre 2023\n\n\n\n\n\n\n21 déc. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM\n\n\nInfolettre de rentrée, Septembre 2023\n\n\n\n\n\n\n10 sept. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropositions de lecture estivale\n\n\nInfolettre estivale, Juillet 2023\n\n\n\n\n\n\n1 juil. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDes innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter\n\n\nInfolettre du mois d’Avril 2023\n\n\n\n\n\n\n1 avr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nTapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley\n\n\nInfolettre du mois de Mars 2023, deuxième quinzaine\n\n\n\n\n\n\n15 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web\n\n\nInfolettre du mois de Mars 2023\n\n\n\n\n\n\n1 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images\n\n\nInfolettre du mois de Février 2023\n\n\n\n\n\n\n30 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz\n\n\nInfolettre du mois de Janvier 2023\n\n\n\n\n\n\n10 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nL’année 2022 dans le monde de la data science : IA, transformation de RStudio, Observable\n\n\nInfolettre du mois de Décembre 2022\n\n\n\n\n\n\n31 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchive des infolettres et lettres Big Data\n\n\nLes infolettres et lettres Big Data antérieures 👵👴, avant la publication sous forme de blog\n\n\n\n\n\n\n31 août 2022\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#les-réseaux-partenaires",
    "href": "index.html#les-réseaux-partenaires",
    "title": "SSPHub",
    "section": "Les réseaux partenaires",
    "text": "Les réseaux partenaires\nQuelques communautés de la data-science avec lesquels nous collaborons\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            CoP OCDE\n            \n\n            \n              Le groupe Community of Practice de l'OCDE est un réseau informel organisé autour des sujets d'innovation statistique.\n\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Eurostat & les  trusted smart statistics (TSS) \n            \n\n            \n              Eurostat encadre les réseaux des instituts nationaux de statistiques et les travaux européens autour de l'exploration de nouvelles sources de données et des nouveaux outils de datascience pour la statistique officielle\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            L'ENSAE, l'ENSAI et le CREST\n            \n\n            \n              L'ENSAE, l'ENSAI sont des écoles d'ingénieur en statistiques, science des données et analyse économique. Le CREST est un centre rassemblant des enseignants-chercheurs d’économie de l’École polytechnique et du CNRS\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            La chaire Finance digitale\n            \n\n            \n              L’objectif de la chaire est de conduire des travaux de recherche sur l’ensemble des innovations, de services, de produits ou d’organisations en lien avec le numérique, susceptibles de modifier le métier de l’intermédiaire financier. Elle est le fruit d'un partenariat entre Télécom Paris, l’université Paris II Panthéon-Assas, l’Institut Louis Bachelier, le Groupement des Cartes Bancaires (CB), la Caisse des Dépôts et l'Insee.\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Lab IA (Etalab) & la DINUM\n            \n\n            \n              La communauté des data scientists et acteurs de l’IA pour l’administration française et plus généralement la DINUM\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Onyxia\n            \n\n            \n              La communauté Onyxia, à l'origine du SSPCloud,\na pour objectif de fournir une plateforme flexible pour expérimenter\nles outils modernes de la data-science.\n\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Spyrales\n            \n\n            \n              Une communauté d'agents de l'Etat pour s'entraider en R et Python\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            UNECE ML Group\n            \n\n            \n              Le travail de recherche du Groupe ML est divisé en 5 groupes de travail visant à traiter différentes problématiques liées à l'utilisation de l'apprentissage automatique pour les statistiques officielles.\n\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            grrr\n            \n\n            \n              Grrr (\"pour quand votre R fait Grrr\") est un\ngroupe Slack (plateforme de discussion instantanée)\nfrancophone dédié aux échanges et à l’entraide autour de R.\nIl s'agit du point central de la communauté R francophone.\nIl est ouvert à tou.te.s et se veut accessible aux débutants. Vous pouvez même utiliser un pseudonyme si vous préférez.\n\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "infolettre/infolettre_08/index.html",
    "href": "infolettre/infolettre_08/index.html",
    "title": "L’année 2022 dans le monde de la data science : IA, transformation de RStudio, Observable",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit\nLa fin de l’année est généralement synonyme de bétisiers, best of ou rétrospectives personnalisées qui nous permettent de nous rappeler les événements marquants de l’année.\nPour célébrer la fin de l’année 2022, la newsletter de janvier adopte un format un peu spécial pour proposer, en deux temps, deux rétrospectives.\nCette première newsletter revient sur les principaux événements de l’année 2022 dans le monde de la data science. La seconde newsletter proposera une rétrospective quantitative sur le réseau des data scientists de la statistique publique, à la manière des rétrospectives personnalisées de nos applications préférées.\n\nLes IA créatrices de contenu à l’honneur\nSi l’année 2022 a été particulièrement riche dans le domaine de la data science, c’est principalement grâce à deux coups médiatiques d’OpenAI, à savoir Dall-E et ChatGPT.\nCes deux outils ont beaucoup fait parler d’eux, au-delà de la sphère traditionnelle de la data science. Le buzz a été intense sur Twitter ou sur Mastodon, le réseau social dont le nombre d’utilisateurs a nettement augmenté en réaction au rachat de Twitter par Elon Musk en fin d’année.\n\nCes innovations, parce qu’elles pourraient avoir des effets à long terme sur la manière dont le grand public appréhende l’intelligence artificielle, ont beaucoup intéressé les médias traditionnels, notamment Le Monde, The Economist et sa “Nouvelle Frontière” ou le Guardian qui s’interroge sur la nature des tâches que l’intelligence artificielle pourra remplacer à terme : procédurales et régies par des règles bien définies ou bien également des activités nécessitant de la créativité et des capacités d’analyse ?\nPour une fois, il ne s’agit donc pas de souligner exclusivement les limites de ces modèles voire leurs dérives (deep fake, biais racistes…) mais aussi de s’enthousiasmer sur leur potentiel créatif. Il est difficile de rester insensible à certaines des créations artistiques des modèles Dall-E, Stable Diffusion, Midjourney et consorts ou de résister à la tentation de tester la capacité de ChatGPT à répondre à des questions complexes Les chercheurs, et pas des moindres (notamment Andrew Ng ou Gaël Varoquaux) se sont également saisis de cette question et ont souligné les biais de raisonnement et excès de confiance de ces IA.\n\n\n\nhttps://github.com/Stability-AI/stablediffusion\n\n\nSi vous désirez utiliser Python de manière créative pour générer du contenu avec Stable Diffusion, vous pouvez consulter ce tutoriel qui fonctionne sur le SSPCloud ou sur Google Colab.\n\n\n\nLe succès des modèles de diffusion\nCes IA génératrices de contenu reposent toutes, à plusieurs niveaux, sur des réseaux de neurone.\nLe premier étage de la fusée est un modèle de langage (large language model) qui synthétise un langage en un ensemble complexe de paramètres. Les plus connus sont BERT et GPT-3. L’inflation dans le nombre de paramètres n’est pas prête de s’arrêter. Si les ressources nécessaires à entraîner en 2018 le modèle BERT (110 millions de paramètres) avaient déjà été critiquées en raison de leur coût financier et environnemental, cette complexité a encore augmenté depuis. Le modèle GPT-3, sorti en 2020, et qui sert de base à Dall-E et ChatGPT intègre 175 milliards de paramètres. Un chiffre qui apparaît minime par rapport aux 17O trillions de paramètres attendus pour le modèle GPT-4 en 2023.\nEn ce qui concerne les IA créatrices de contenu visuel, le deuxième étage de la fusée est un modèle d’analyse d’image qui apprend à associer des images à une description textuelle afin de détecter des structures communes entre des mots ou des séquences de mots et des formes sur des images. Il s’agit de déconstruire une forme en une structure minimale de pixels qui permet de l’identifier.\n\n\n\nSource: Sebastian Raschka\n\n\nEnsuite, pour générer une image à partir d’une description inédite intervient le modèle de diffusion qui reconstruit une image à partir du mélange de l’ensemble des pixels qui traduisent les concepts principaux d’une instruction. L’une des explications les plus pédagogiques pour comprendre le fonctionnement de ces modèles vient du Washington Post.\nSinon, on peut demander directement à ChatGPT de nous expliquer:\n\n\n\nL’actualité dans le monde du deep learning\nSi le succès d’estime de ces IA génératrices consacre les modèles de diffusion, l’année du deep learning ne se réduit pas à cette actualité.\nL’année a notamment été marquée par la compétition entre les librairies et écosystèmes TensorFlow, développé par Google, et PyTorch projet initié par Facebook/Meta. PyTorch, plus récent, bénéficie d’une dynamique plus ascendante que TensorFlow. Le succès d’HuggingFace, plateforme de mise à disposition de modèles, et où les implémentations PyTorch sont systématiques alors que celles en TensorFlow sont rares a participé à la diffusion de PyTorch.\nPreuve du succès de PyTorch, cet écosystème est dissocié de Meta depuis septembre afin de devenir un outil généraliste géré par la Linux Foundation. À l’inverse, Google semble se détacher graduellement de TensorFlow pour privilégier son nouvel écosystème JAX.\n\n\nDu changement côté RStudio\nDepuis quelques années, RStudio a fait le choix de devenir un écosystème de data science généraliste et non plus exclusivement attaché au langage R.\nCette année, cela s’est traduit par la publication, très commentée, de Quarto qui vise à proposer, dans de nombreux langages de programmation, des fonctionalités de publications reproductibles équivalentes à l’un des produits emblématiques de RStudio, à savoir R Markdown. Rien de mieux pour être convaincu de l’intérêt de cet outil que d’observer la galerie d’exemples, d’explorer la documentation très riche, ou de tester soi-même sur un exemple. Cet été, RStudio a également annoncé que Shiny, un autre produit emblématique, serait maintenant disponible sous Python, comme alternative à Dash ou Streamlit.\nL’année 2022 a été l’occasion, pour RStudio, d’un autre changement, symbolique celui-ci. Afin de détacher son image du langage R, l’entreprise a en effet changé de nom pour devenir posit. L’entreprise n’a néanmoins pas abandonné son activité foisonnante dans R puisque Hadley Wickham a commencé à publier de nouveaux chapitres pour une nouvelle édition augmentée de l’ouvrage de référence R For Data Science.\n\n\nObservable devient un incontournable dans le monde de la dataviz\nPour permettre des visualisations interactives, cela fait plusieurs années que JavaScript est un incontournable et que le web assembly retient de plus en plus d’attention.\nLes journaux traditionnels utilisent ainsi de plus en plus le data scrollytelling , cette technique de narration qui consiste à présenter des informations sous forme de récit interactif, en utilisant une combinaison de texte et de graphiques qui apparaissent et disparaissent en fonction des actions du lecteur. L’un des exemples les plus réussis des dernières années a sans doute été la visualisation du New York Times “How the virus got out”. Cette approche a également été adoptée par le Ministère de l’Agriculture pour diffuser les chiffrés clés du recensement agricole. Nos voisins anglais ne sont pas en reste puisque les derniers résultats du recensement sont proposés sur un site web remarquable de fluidité.\nAfin de permettre une diffusion accrue de visualisations en JavaScript, Mike Bostock, déjà créateur de la librairie de dataviz de référence D3.js, est à l’origine de la plateforme observable, sorte de Github de la dataviz permettant du partage et de la réutilisation de notebooks réactifs. En cette année 2022, la plateforme a connu un véritable boom et est devenu un incontournable dans le domaine. L’une des raisons est l’ajout de fonctionalités qui permettent d’étendre le public cible au delà des développeurs web, déjà accoutumés à Javascript. Parmi les fonctionalités les plus remarquables, la possibilité depuis Novembre d’utiliser des requêtes SQL grâce à DuckDB permet aux habitués de R ou de Python de retrouver des manipulations auxquels ils sont habitués. La librairie Plot offre une grammaire proche de ggplot2.\nLa communauté des cartographes a été particulièrement active sur Observable, notamment à l’occasion du #30daymapchallenge. Nicolas Bertin (neocarto), dont on ne peut que recommander l’introduction à Observable faite pour le réseau, ou Eric Mauvière font partie des comptes à suivre dans la communauté francophone.\nObservable, en tant que langage construit sur JavaScript, est également disponible pour les utilisateurs de Quarto, ce qui permet de mettre à disposition des visualisation réactives sans passer nécessairement par la plateforme observablehq.com pour mettre à disposition des visualisations réactives, ce qui constitue une alternative intéressante aux applications qui nécessitent un serveur en arrière plan, comme Shiny ou Dash.\n \n\n\nLes autres actualités en France\nLe rapport du conseil d’État pour la construction d’une IA de “confiance” a donc été publié en une année 2022 où les avancées techniques des dernières années commencent à être accessibles grâce à des outils plus grand public, ce qui va nécessairement soulever des enjeux éthiques et juridiques.\nLe projet Onyxia, qui vise à proposer une infrastructure de data science à l’état de l’art pour data scientists, a organisé son deuxième Openlab. L’occasion de revenir sur le projet, sa philosophie, ses dernières avancées mais aussi d’échanger sur les perspectives de réutilisation dans de multiples environnements et de nouer des partenariats qui permettront au projet de grandir encore en 2023."
  },
  {
    "objectID": "infolettre/infolettre_10/index.html",
    "href": "infolettre/infolettre_10/index.html",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit"
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#un-événement-autour-des-packages-facilitant-laccès-à-lopen-data-de-linsee",
    "href": "infolettre/infolettre_10/index.html#un-événement-autour-des-packages-facilitant-laccès-à-lopen-data-de-linsee",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Un événement autour des packages facilitant l’accès à l’open data de l’Insee",
    "text": "Un événement autour des packages facilitant l’accès à l’open data de l’Insee\n\n\n\n\n\nAprès les présentations d’observable et de gridviz nous vous proposons un nouvel événement. Celui-ci sera autour de l’open data à travers la présentation des packages facilitant la récupération de données de l’Insee disponibles depuis le site web ou les API.\nDeux présentations sont prévues : - Pierre Lamarche présentera le package  doremifasol. C’est notamment grâce à ce package que la documentation utilitR peut s’appuyer sur des données bien connues des utilisateurs d’open data (Filosofi, recensement…) - Hadrien Leclerc nous présentera le package  Pynsee qui est utilisé depuis deux ans à l’ENSAE pour apprendre aux futurs data scientists à récupérer des données de cadrage.\nCes deux présentations seront suivies d’un temps d’échange.\nCet événement aura lieu le 13 février de 15h à 16h30 (📅 invitation Outlook). Si vous êtes utilisateurs de données, que vous veniez de l’Insee ou non, ces packages peuvent vous intéresser !"
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#masterclass-datascientest",
    "href": "infolettre/infolettre_10/index.html#masterclass-datascientest",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Masterclass datascientest",
    "text": "Masterclass datascientest\n\n\n\n\n\nLes masterclass organisées avec l’organisme de formation spécialisé datascientest reprennent ! Après une première masterclass au mois de décembre consacrée au MLOps, notre réseau va proposer de nouvelles séances.\nLes premières séances vont s’organiser autour de deux cursus parallèles, qui commenceront par des introductions pour monter graduellement en niveau et se rapprocher des cas d’usages que rencontrent nos data scientists.\nLe premier parcours sera orienté autour des problématiques de NLP. La première séance aura lieu le 10 février, de 10h à 12h et constituera une introduction au NLP avec un retour sur certains concepts centraux (preprocessing, tokenisation, lemmatisation…) et des exemples d’applications avec le package SpaCy. Une deuxième séance dans ce parcours est déjà programmée, le 24 mars de 10h à 12h, sur le thème de la similarité textuelle et de la classification de textes grâce aux méthodes d’embeddings.\nLe deuxième parcours cible la problématique de l’analyse d’images. Une première séance, qui aura lieu le 10 mars de 10h à 12h reviendra sur certains concepts centraux du deep learning (perceptron, convolution, transfer learning…). Les séances suivantes, dont les dates n’ont pas encore été arrêtées, s’intéresseront à des cas d’usages comme l’OCRisation ou la détection d’objets dans des images.\n\n\n\n\n\n\nNote\n\n\n\nPour vous inscrire, il suffit de remplir ce formulaire !"
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#questionnaire-sur-vos-besoins-en-formation-data-science",
    "href": "infolettre/infolettre_10/index.html#questionnaire-sur-vos-besoins-en-formation-data-science",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Questionnaire sur vos besoins en formation data science",
    "text": "Questionnaire sur vos besoins en formation data science\n\n\n\n\n\nEn cette période de recensement, le réseau propose également le sien ! Pour déterminer au mieux la répartition des besoins en formation sur les sujets data science et ainsi pouvoir proposer des événements pertinents, nous vous proposons un questionnaire sur vos besoins en formation."
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#utilitr-recherche-des-rédacteurs-dexercices",
    "href": "infolettre/infolettre_10/index.html#utilitr-recherche-des-rédacteurs-dexercices",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "utilitR recherche des rédacteurs d’exercices !",
    "text": "utilitR recherche des rédacteurs d’exercices !\n\n\n\n\n\nDans le but de continuer à développer utilitR, documentation collaborative et ouverte, l’équipe du projet souhaite encourager des contributions volontaires pour ajouter des exercices à chaque fiche thématique. L’objectif est de produire pour chaque chapitre un ensemble d’exercices, de difficulté graduelle, permettant de mettre en application les concepts présentés dans la fiche.\n\nCes exercices seraient accessibles depuis le site web mais aussi à travers le portail de formation du SSP Cloud, sous la forme de notebooks d’autoformation.\nL’équipe du projet utilitR est donc à la recherche des personnes motivées pour rédiger des exercices ou mettre à disposition des bouts de code ou des exercices déjà préparés. Si vous désirez apporter votre pierre à l’édifice, toute contribution, même modeste, sur cette page, sera appréciée par l’équipe utilitr.\nCette évolution de la documentation vise à prolonger l’effort continu pour construire une documentation vivante, interactive et originale. L’esthétique du site web book.utilitr.org a ainsi été revue récemment afin de rendre la documentation plus ergonomique tout en ajoutant des fonctionnalités utiles aux lecteurs, comme la possibilité de surligner ou de prendre des notes."
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#proposez-un-billet-de-blog",
    "href": "infolettre/infolettre_10/index.html#proposez-un-billet-de-blog",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Proposez un billet de blog !",
    "text": "Proposez un billet de blog !\n\n\n\n\n\nLe site web du réseau (https://ssphub.netlify.app/) propose depuis septembre une section blog. Vos idées et contributions sont les bienvenues pour l’enrichir !\nPour souligner l’aspect collectif de cette section, un guide des contributeurs vient de voir le jour. Celui-ci expose la démarche à suivre, de la phase de discussion pour définir le sujet du billet aux outils proposés pour faciliter la rédaction et la soumission de celui-ci depuis Github ."
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#la-saison-2-du-programme-10-arrive",
    "href": "infolettre/infolettre_10/index.html#la-saison-2-du-programme-10-arrive",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "La saison 2 du programme 10% arrive",
    "text": "La saison 2 du programme 10% arrive\n\n\n\n\n\nL’attente était insoutenable mais la nouvelle saison de 10% est enfin là ! Rejoignez ce programme, issu des recommandations du rapport de l’Inspection Générale de l’Insee et de la DINUM, où des data scientists proposent de consacrer jusqu’à 10% de leur temps de travail à des projets transversaux !\nAu-delà de la participation à ces projets, le programme 10% est également l’opportunité d’échanger des idées avec des data scientists d’autres administrations et de bénéficier de formations.\nAprès un webinaire d’information sur le programme le 31 janvier (inscription via eventbrite), la journée de lancement de la saison 2 se tiendra le 14 février au Bercy Lab (plus d’infos à venir).\nCette saison, plus longue que la première, permettra de pérenniser certains des projets de la saison 1 mais aussi de lancer de nouveaux projets.\nInscrivez-vous dès maintenant pour ne pas manquer la saison 2 de 10%!\n\n\n\n\n\n\nNote\n\n\n\nPour plus d’information sur le programme : lab-ia@data.gouv.fr"
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#report-de-la-journée-de-la-donnée",
    "href": "infolettre/infolettre_10/index.html#report-de-la-journée-de-la-donnée",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Report de la journée de la donnée",
    "text": "Report de la journée de la donnée\nLa journée de la donnée organisée par l’Administrateur Ministériel des Données, Algorithmes et Codes sources (AMDAC) du Ministère de la Santé, initialement prévue le 31 janvier, est reportée à une date ultérieure."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html",
    "href": "infolettre/infolettre_12/index.html",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "",
    "text": "Note\n\n\n\nVous désirez intégrer la liste de diffusion ? L’inscription se fait ici.\nDu fait de la densité des actualités dans le monde de la data science et des multiples événements à venir dans le cadre de ce réseau, nous proposons d’accélérer le rythme de publication des newsletters."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#la-charte-graphique-de-linsee-sur-le-tapis-rouge",
    "href": "infolettre/infolettre_12/index.html#la-charte-graphique-de-linsee-sur-le-tapis-rouge",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "La charte graphique de l’Insee sur le tapis rouge",
    "text": "La charte graphique de l’Insee sur le tapis rouge\nLa semaine dernière avait lieu la cérémonie des Oscars. Grâce à un fil de Clara Dealberto, on peut mesurer l’influence des graphistes de l’Insee sur les stylistes des stars :\n\n\n\nTweet de Clara Dealberto\n\n\nLe fil, qui met à l’honneur l’Insee, vaut le détour ; n’hésitez pas à le consulter ! Ou à découvrir celui sur le Met Gala."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#chatgpt-encore-et-toujours",
    "href": "infolettre/infolettre_12/index.html#chatgpt-encore-et-toujours",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "ChatGPT encore et toujours",
    "text": "ChatGPT encore et toujours\n\n\n\n\n\nChatGPT continue de focaliser l’attention. Dans la veine de l’article désignant les modèles de langages sous le terme de “stochastic parrots” (“perroquets stochastiques”), Arthur Charpentier parle lui de “société du bullshit” pour désigner la manière dont ChatGPT offre, sous un raisonnement en apparence logique, de manière indifférenciée des absurdités et des vérités.\nEspérons que lorsque ChatGPT sera embarqué dans les voitures General Motors, il ne nous donnera pas de fausse indication pour changer un pneu ou ne se retournera pas contre le conducteur comme le ferait HAL 9000.\nUn article intéressant de Wired questionne d’ailleurs l’empreinte carbone que pourrait impliquer la généralisation des modèles de langage dans les moteurs de recherche, qui font face à des milliards de requêtes quotidiennes.\nAu moment où OpenAI rend public GPT-4, une version plus riche de son modèle GPT-3 qui servait de base à ChatGPT, l’un des cofondateurs d’OpenAI revient sur la stratégie d’ouverture (ou plutôt l’absence d’ouverture) d’OpenAI : “Nous avions tord” (voir The Verge). Hasard du calendrier, cette déclaration a eu lieu presque au même moment que la publication d’un robot conversationnel ouvert OpenChatKit. La concurrence est néanmoins âpre puisqu’une équipe de Microsoft a déjà proposé l’intégration à ChatGPT d’un module permettant d’interagir avec ChatGPT également par le biais d’images."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#des-turbulences-dans-la-silicon-valley",
    "href": "infolettre/infolettre_12/index.html#des-turbulences-dans-la-silicon-valley",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Des turbulences dans la Silicon Valley",
    "text": "Des turbulences dans la Silicon Valley\n\n\n\nUne infographie des faillites bancaires par Mike Bostock. Source: Notebook Observable\n\n\nL’autre actualité phare des quinze derniers jours est la faillite de la Silicon Valley Bank. Aux Etats-Unis, il s’agit de la plus principale faillite bancaire depuis 2008 aux Etats-Unis.\nLa Fed est rapidement intervenue pour endiguer la panique bancaire, même si la banque était en fait déjà dans ses radars bien avant sa faillite (voir NYT)."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#retour-sur-les-évolutions-récentes-du-monde-de-la-data-science",
    "href": "infolettre/infolettre_12/index.html#retour-sur-les-évolutions-récentes-du-monde-de-la-data-science",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Retour sur les évolutions récentes du monde de la data science",
    "text": "Retour sur les évolutions récentes du monde de la data science\nLe panorama technologique 2023 de Matt Turck confirme la tendance à la diversification des technologies à maîtriser pour mener un projet de data science. Cette complexification des outils et des rôles dans l’écosystème de la donnée, évoquée dans l’article “Is Data Scientist Still the Sexiest Job of the 21st Century?”, est ici confirmée.\nLes derniers sondages auprès des recruteurs américains montrent la popularité des data engineers, plus spécialisés que les data scientists dans la mise en oeuvre d’infrastructures techniques pour valoriser des données. Le profil de data engineer apparaît en deuxième place dans le classement des profils les plus recherchés par les recruteurs alors que les data scientists n’apparaissent plus dans les premières places du classement."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#le-big-data-nest-pas-mort",
    "href": "infolettre/infolettre_12/index.html#le-big-data-nest-pas-mort",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Le big data n’est pas mort",
    "text": "Le big data n’est pas mort\nUne réponse intéressante à l’article “Big data is dead” (voir Newsletter #11) revient sur l’intérêt de disposer de données historiques longues pour l’entrainement de modèles d’apprentissage."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#r-directement-dans-le-navigateur",
    "href": "infolettre/infolettre_12/index.html#r-directement-dans-le-navigateur",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "R directement dans le navigateur",
    "text": "R directement dans le navigateur\nAvec un peu de retard sur Python, il devient maintenant possible de faire du R directement depuis le navigateur web, c’est-à-dire sans installation du logiciel R, grâce à WebR. Cette approche est typique du Web Assembly où les langages de programmation sont directement utilisés depuis le navigateur, sans installation préalable."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#première-journée-du-réseau-en-avril-17-avril",
    "href": "infolettre/infolettre_12/index.html#première-journée-du-réseau-en-avril-17-avril",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Première journée du réseau en avril (17 avril)",
    "text": "Première journée du réseau en avril (17 avril)\n\n\n\n\n\nDéjà annoncée dans la Newsletter #11, nous rappelons la journée du réseau le 17 avril, en présentiel 📅.\nLe nombre de places dans l’espace à disposition étant limité, une invitation par mail et un lien d’inscription seront communiqués dans la semaine pour pouvoir participer à cet événement en présentiel dans le tiers-lieu la Tréso à Malakoff."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#bonnes-pratiques-en-python-présentation-lors-des-ateliers-du-programme-10-30-mars",
    "href": "infolettre/infolettre_12/index.html#bonnes-pratiques-en-python-présentation-lors-des-ateliers-du-programme-10-30-mars",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Bonnes pratiques en Python : présentation lors des ateliers du programme 10% (30 mars)",
    "text": "Bonnes pratiques en Python : présentation lors des ateliers du programme 10% (30 mars)\nDans le cadre du programme 10%, des présentations ont lieu avant certains ateliers de travail sur les projets communautaires.\nLa prochaine présentation, qui aura lieu le jeudi 30 mars de 14h à 15h 📅, sera donnée par des membres du réseau. Elle portera sur une présentation des outils favorisant les bonnes pratiques de développement en Python et de l’intérêt de ces bonnes pratiques pour faciliter la mise en production de projets de data science. Il s’agira d’une présentation succincte du contenu du cours de l’ENSAE “Bonnes pratiques et mise en production de projets data science”.\nPlus d’infos à venir par le biais du canal Tchap de notre réseau."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#un-événement-autour-de-locrisation-avec-christopher-kermorvant-29-mars",
    "href": "infolettre/infolettre_12/index.html#un-événement-autour-de-locrisation-avec-christopher-kermorvant-29-mars",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Un événement autour de l’OCRisation avec Christopher Kermorvant (29 mars)",
    "text": "Un événement autour de l’OCRisation avec Christopher Kermorvant (29 mars)\nLe mercredi 29 mars de 15h à 16h 📅 nous recevons Christopher Kermorvant, chercheur spécialisé en OCRisation et fondateur de Teklia. Christopher mène actuellement plusieurs projets de numérisation de textes anciens, notamment d’OCRisation de vieux recensements avec l’INED.\nPendant cet événement, Christopher nous fera un état de l’art de l’OCRisation puis nous présentera des projets qu’il a pu mener récemment avec Teklia.\nIl est possible de suivre la présentation via Zoom ou, pour les personnes présentes à l’Insee, en 2-C-496."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#présentation-de-la-documentation-collaborative-carpentries-28-mars",
    "href": "infolettre/infolettre_12/index.html#présentation-de-la-documentation-collaborative-carpentries-28-mars",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Présentation de la documentation collaborative Carpentries (28 mars)",
    "text": "Présentation de la documentation collaborative Carpentries (28 mars)\nPour rappel, Kate Burnett-Isaacs, de Statistics Canada, nous présentera l’initiative Meta Academy / Carpentries le mardi 28 mars à 15h 📅. Plus de détails dans la Newsletter #11.\nInvitation Outlook ici."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#post-de-blog-sur-polars",
    "href": "infolettre/infolettre_12/index.html#post-de-blog-sur-polars",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Post de blog sur Polars",
    "text": "Post de blog sur Polars\n\n\n\n\n\nPour faire suite à la Newsletter #11 qui présentait l’écosystème autour du package Python Polars, Romain Tailhurat (Insee) nous propose un post de blog pour découvrir ce package.\nCelui-ci est accompagné par un tutoriel pas-à-pas pour découvrir les principales fonctionnalités de la librairie. Il est possible de tester le notebook en un seul clic sur le SSP Cloud ou sur Google Colab.\nVous pouvez également retrouver ce tutoriel depuis l’espace formation du SSP Cloud."
  },
  {
    "objectID": "infolettre/infolettre_14/index.html",
    "href": "infolettre/infolettre_14/index.html",
    "title": "Propositions de lecture estivale",
    "section": "",
    "text": "Note\n\n\n\nVous désirez intégrer la liste de diffusion ? L’inscription se fait ici.\nCette newsletter propose un petit cahier de vacances de la data science afin de profiter de la période estivale pour en apprendre plus sur les sujets en vogue et ainsi être fin prêt pour la rentrée 📖⛱️."
  },
  {
    "objectID": "infolettre/infolettre_14/index.html#lectures-estivales",
    "href": "infolettre/infolettre_14/index.html#lectures-estivales",
    "title": "Propositions de lecture estivale",
    "section": "Lectures estivales",
    "text": "Lectures estivales\nPour commencer, quelques conseils de lecture sur les grands modèles de langage :\n\nUn article du Washington Post pour en savoir plus sur le corpus d’entrainement des grands modèles de langage (large langage model, LLM) ;\nPour comprendre la manière dont ChatGPT et les grands modèles de langage traduisent vos phrases afin de vous répondre, consultez ce post de blog sur les tokenizers (décomposition d’une chaine de caractères en unité minimale comme un mot ou une syllabe);\nEntre deux épisodes de votre série préférée, vous pourrez alterner avec ce cours de 2 heures sur les grands modèles de langage ;\nVous serez ensuite prêts à approfondir le sujet avec ce cours complet d’Huggingface sur le traitement automatique du langage.\n\nN’hésitez pas à faire des pauses dessin virtuel pour stimuler votre créativité en créant des images avec StableDiffusion ou Dall-E 2.\nLa version Python de l’ouvrage de référence Introduction to Statistical Learning vient de sortir et est disponible, comme la version R, gratuitement. Si après la lecture de celui-ci vous désirez mieux comprendre la question de l’interprétabilité des modèles de machine learning, c’est-à-dire les méthodes statistiques permettant de mieux comprendre la manière dont les algorithmes d’apprentissage aboutissent à une décision, ce site web vous sera très utile.\nSi vous désirez en apprendre plus sur la question de la reproductibilité, les ressources suivantes vous seront utiles :\n\nBuilding reproducible analytical pipelines with R ;\nCoding for economists ;\nFormation aux bonnes pratiques R et Git par l’INSEE ;\nUn cours de l’ENSAE sur la mise en production de projets data science et, pour approfondir sur la mise en production de modèles de machine learning, une formation de l’Insee sur le MLOps, ensemble de techniques qui visent à faciliter la mise en production et la maintenance de modèles.\n\nSi les semestres de l’année scolaire ne vous ont pas suffi, vous pouvez aussi profiter de l’été pour compléter votre formation en data science en suivant le missing semester de votre cursus.\nVous pouvez également reprendre l’ensemble des ressources mises à disposition dans le cadre du Funathon 2023, un événement pour lequel les équipes innovation de l’Insee et du SSM Agriculture et alimentation ont mis à disposition de nombreuses ressources R ou Python sur six thèmes concernant l’alimentation et la production agricole. Ces ressources couvrent un large éventail de niveaux de difficulté et de techniques pour permettre à la fois aux débutants en code et aux data scientists plus aguerris d’y trouver leur compte.\nEt pour un parcours complet de formation, rien de mieux que d’explorer de fond en comble le portail de formation du SSP Cloud.\nAprès avoir lu tout ceci, vous serez prêts pour les événements data science de la rentrée comme le hackathon du mobidatalab sur le thème de l’amélioration des services de mobilité urbaine (15 et 16 septembre).\nBonnes vacances ! Et n’oubliez pas de profiter pleinement des vacances pour oublier ChatGPT quelques temps ! 🌞🌊⛰️"
  },
  {
    "objectID": "infolettre/infolettre_16/index.html",
    "href": "infolettre/infolettre_16/index.html",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "",
    "text": "Noël approchant, avant d’ouvrir vos cadeaux, de dévorer une bûche ou tout autre met délicieux, nous vous proposons de nous retourner sur la progression de l’audience du réseau durant l’année 2023. Cette newsletter commencera ainsi par une rétrospective du réseau, en écho à celle de l’année 2022. Une partie consacrée aux actualités de la data science suivra."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#rétrospective-du-réseau-en-2023",
    "href": "infolettre/infolettre_16/index.html#rétrospective-du-réseau-en-2023",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "Rétrospective du réseau en 2023",
    "text": "Rétrospective du réseau en 2023\n\nUne audience en progression\n\nhtml`&lt;div&gt;&lt;span class = \"underline-big\"&gt;${start_count}&lt;/span&gt; personnes faisaient partie de la liste de diffusion en début d'année 2023.&lt;/div&gt;&lt;br&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${plot_bar_participants}&lt;/div&gt;&lt;br&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${message}&lt;/div&gt;&lt;br&gt;`\n\n\n\n\n\n\n\n\n\n\nDécouvrir le code\n\nVoir plus bas la définition des objets Javascript 👇️\nhtml`&lt;div&gt;&lt;span class = \"underline-big\"&gt;${start_count}&lt;/span&gt; personnes faisaient parti de la liste de diffusion en début d'année.&lt;/div&gt;&lt;br&gt;`\nhtml`&lt;div&gt;${plot_bar_participants}&lt;/div&gt;&lt;br&gt;`\nhtml`&lt;div&gt;${message}&lt;/div&gt;`\n\nA l’exception du mois d’août (pause estivale oblige), la progression de l’audience a été assez régulière grâce aux événements et contenus publiés sur le site du SSP Hub.\n\nhtml`&lt;div&gt;Pendant l'année 2023, le réseau a ainsi connu &lt;span class=\"underline-big\"&gt;${events.length}&lt;/span&gt; événements et publications de contenu (${countEvents(\"Infolettre\")}, ${countEvents(\"Post de blog\")}, ${countEvents(\"Evénement virtuel ou présentiel\")}, ${countEvents(\"Masterclass\")}).&lt;/div&gt;&lt;br&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${lineplot}&lt;/div&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${warm_strip}&lt;/div&gt;`\n\n\n\n\n\n\n\nmd`__Choisir les événements du réseau à afficher 👇️__`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${viewof events_chosen_figure1}&lt;/div&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${table_events}&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\nCode pour générer les différents blocs de cette figure\n\n\n\nCode pour générer la figure principale\n\n\n// Voir plus bas 👇️ les arrays utilisés\n// Animation faite avec le Scrubber ci-dessous\nlineplot = Plot.plot({\n    y: {\n        grid: true,\n        label: \"Nombre d'inscrits\"\n    },\n    x: {\n        label: \"Date\",\n        domain: [new Date(\"2023-01-10\"), new Date(\"2023-12-09\")]\n    },\n    color: {\n        range: Object.values(color_mapping_events),\n        domain: Object.keys(color_mapping_events),\n        label: \"Type d'événement\"\n    }, \n    marginLeft: 50,\n    marks: [\n        Plot.line(\n          serie_contacts, {\n            x: \"date\", y: \"mail\",\n            stroke: \"#6886bb\",\n            tip: \"xy\"\n            }),\n1        Plot.crosshairX(serie_contacts_complete, {\n            x: (d) =&gt; new Date(d.date), y: \"mail\", stroke: \"red\"\n            }),\n        Plot.dot(\n          serie_contacts_complete,\n2          Plot.pointerX({x: (d) =&gt; new Date(d.date), y: \"mail\", stroke: \"red\"})),\n        Plot.dot(serie_contacts, {\n            x: \"date\", y: \"mail\",\n            stroke: \"#6886bb\",\n            fill: \"#6886bb\",\n            title: \"Effectif\"\n            }),\n        Plot.tickX(events_data_figure1_b, {\n            x: (d) =&gt; new Date(d.date), text: html``,\n            stroke: \"type\",\n            opacity: 0.1,\n            color: \"x\",\n            tip: true\n            }),            \n        Plot.axisX(events_data_figure1_b, {\n            x: (d) =&gt; new Date(d.date),\n            text: \"\",\n            color: \"type\"\n            }),\n    ]\n})\n\n\n1\n\nElément de réactivité lorsque la souris passe sur la figure.\n\n2\n\nElément de réactivité lorsque la souris passe sur la figure.\n\n\n\n\n\n\n\n\n\n\n\n\nCode pour générer la bande sous la figure principale\n\n\n1warm_strip = Plot.plot({\n  height: 40,\n  marginLeft: 50,\n  color: {\n    scheme: \"ylorrd\",\n    },\n  marks: [\n    Plot.crosshair(serie_contacts_complete, {\n      x: (d) =&gt; new Date(d.date),\n      strokeOpacity: 0.2,\n      fill: \"mail\",\n      interval: d3.utcDay.every(3),\n      inset: 0 // no gaps\n    }),\n    Plot.barX(serie_contacts_complete, {\n      x: (d) =&gt; new Date(d.date),\n      strokeOpacity: 0.2,\n      fill: \"mail\",\n      interval: d3.utcDay.every(3),\n      inset: 0 // no gaps\n    })\n  ]\n})\n\n\n1\n\nInspiration : https://observablehq.com/@observablehq/plot-warming-stripes\n\n\n\n\n\n\n\n\n\n\n\n\nCode pour générer le sélecteur d’événements\n\n\nfunction underline_event(x){\n    const x_underlined = `&lt;span style=\"text-transform: capitalize; border-bottom: solid 4px ${color_mapping_events[x]}; margin-bottom: -2px;\"&gt;${x}&lt;/span&gt;` ;\n    return x_underlined\n}\n\nviewof events_chosen_figure1 = Inputs.checkbox(\n    unique(events.map(d =&gt; d.type)),\n    {\n        value: unique(events.map(d =&gt; d.type)),\n        format: x =&gt; html`${underline_event(x)}`\n    }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode pour générer la table interactive\n\n\ntable_events = Inputs.table(\n    events_data_figure1_b,\n    {\n        columns: [\"date\", \"event\", \"type\"],\n        header: {\n            date: \"Date\",\n            event: \"Evénement du réseau\",\n            type: \"Type d'événement\"\n        },\n        format: {\n            type: (x) =&gt; html`\n            &lt;span style=\"text-transform: capitalize; display: inline-flex; align-items: center;\"&gt;\n    &lt;span style=\"border-bottom: solid 1px ${color_mapping_events[x]}; margin-bottom: -2px;\"&gt;${x}&lt;/span&gt;\n    &lt;span style=\"width: 10px; height: 10px; margin-left: 5px; background-color: ${color_mapping_events[x]};\"&gt;&lt;/span&gt;\n    &lt;/span&gt;\n            `,\n        event: (x) =&gt; html`&lt;a ${links_website_ssphub[x] !== undefined ? `href=\"${links_website_ssphub[x]}\" target=\"_blank\"` : ''}&gt;${x}&lt;/a&gt;`\n        }\n    })\n\n\n\n\n\n\n\n\n\n\nDu contenu qui intéresse au-delà des statisticiens publics\nSi les statistiques concernant la composition du réseau parmi les organismes de la statistique publique sont relativement stables par rapport à l’an dernier, le changement principal ayant eu lieu en 2023 est l’ouverture progressive à des publics hors de la statistique publique (administrations hors du SSP, chercheurs et étudiants…)\n\nhtml`&lt;div&gt;${barplot_ssp}&lt;/div&gt;`\n\n\n\n\n\n\n\n\nCode pour générer la figure\n\n\nbarplot_ssp = Plot.plot({\n  marginLeft: 60,\n  marginRight: 100,\n  x: {label: \"Frequency\"},\n  y: {label: null},\n  color: {\n    domain: [\"Hors du SSP\", \"Service Statistique Public (SSP)\"],\n    range: [\"forestgreen\", \"#6886bb\"]\n    },\n  marks: [\n    Plot.barX(hors_ssp_data,\n    {\n      y: \"SSP\",\n      fy: (d) =&gt; new Date(d.date).toLocaleString(\"fr\", {\n        \"month\": \"long\",\n        \"year\": \"numeric\"\n      }),\n      x: \"mail\", inset: 0.5, fill: \"SSP\", sort: \"mail\",\n      tip: true, channels: {share: (d)  =&gt; `${100*d.share.toFixed(2)}%`}\n    }),\n    Plot.axisY({textAnchor: \"start\", fill: \"black\", dx: 14}),\n    Plot.ruleX([0])\n  ]\n})\n\n\n\n\n\n\n\nLes deux dernières publications sur le site du réseau, à savoir l’infolettre #15 sur le réentrainement des modèles de langage et, surtout, le post de blog sur la publication du recensement de la population au format Parquet ont connu un écho important hors des cercles de data scientist du service statistique public et ont amené de nouveaux publics à suivre les contenus proposés par le réseau."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#la-startup-mistral-ai-publie-un-modèle-à-létat-de-lart",
    "href": "infolettre/infolettre_16/index.html#la-startup-mistral-ai-publie-un-modèle-à-létat-de-lart",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "La startup Mistral AI publie un modèle à l’état de l’art",
    "text": "La startup Mistral AI publie un modèle à l’état de l’art\nMistral AI, une startup française spécialisée dans l’intelligence artificielle, vient de publier un modèle nommé Mixtral qui repose sur le principe du mixture of experts. Cette technique consiste à privilégier une architecture construite à partir de sous-modèles spécialisés plutôt qu’un modèle généraliste qui s’adapte en fin de procédure à une question spécialisée. Dans ce type de modèles, l’enjeu est ainsi d’interpréter la question pour diriger la réponse vers l’expert adéquat : si une question porte sur un sujet de cuisine, un.e expert.e spécialisé.e en code sera de peu de secours.\nD’après les premières évaluations publiées, ce modèle surpasserait les capacités des autres modèles ouverts (notamment Llama 2) et s’approcherait des performances de GPT 3.5, le modèle derrière la version gratuite de ChatGPT. Cette annonce a eu lieu en pleine période de levée de fonds pour Mistral AI qui aurait obtenu un financement de 385 millions d’euros.\n\n\nTableau des performances (source: Mistral AI)\n\n\n\n\nTableau des performances publié par Mistral AI\n\n\n\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nUn article du Monde sur l’entreprise Mistral AI ;\nLe modèle Mixtral sur Huggingface ;\nLe principe des architectures mixture of experts (article Wikipedia)."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#leurope-parvient-à-un-accord-sur-les-premières-règles-au-monde-en-matière-dia",
    "href": "infolettre/infolettre_16/index.html#leurope-parvient-à-un-accord-sur-les-premières-règles-au-monde-en-matière-dia",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "L’Europe parvient à un accord sur les premières règles au monde en matière d’IA",
    "text": "L’Europe parvient à un accord sur les premières règles au monde en matière d’IA\nDans un accord provisoire signé le 9 décembre 2023 et nommé “Artificial Intelligence Act”, les États Membres et le Parlement européen ont établi une proposition relative à des règles harmonisées concernant l’intelligence artificielle (IA).\nDébutées en 2018, avant que les IA génératives ne deviennent si populaires, ces discussions dépassent le cadre exclusif de ces dernières. Néanmoins, concernant celles-ci, le compromis prévoit une approche différenciée suivant le contexte de développement et l’usage de ces modèles. Outre le respect des règles européennes de propriété intellectuelle, les développeurs de modèles génératifs devront s’assurer que les produits diffusés sont bien identifiés comme artificiels, afin de limiter la diffusion de deepfakes. Les développeurs de ces modèles devront également communiquer sur la qualité des données utilisées pour entraîner les modèles et sur le coût énergétique de ceux-ci. Les modèles open source et ceux construits à des fins de recherche bénéficient d’exemptions de ces règles.\nDes contraintes renforcées s’appliqueront aux systèmes jugés à “haut risque” dans des domaines comme la défense, l’éducation, les ressources humaines ou encore la santé. Pour ces systèmes, il sera nécessaire de réaliser une analyse d’impact avant la mise sur le marché. Par ailleurs, une obligation de transparence et d’explicabilité des modèles est mise en place afin d’être en mesure de comprendre les règles de décision de ces IA.\nL’accord provisoire interdit également l’utilisation de l’IA dans quelques domaines, jugés trop sensibles. Par exemple, la reconnaissance faciale de masse est interdite, hormis lorsque celle-ci est justifiée par des motifs de sécurité nationale. D’autres utilisations, qui peuvent amener à des dérives, comme la notation sociale basée sur le comportement ou des caractéristiques personnelles, sont interdits. Les travaux se poursuivront maintenant au niveau technique dans les semaines à venir afin de mettre au point les détails du nouveau règlement. Une fois ces travaux terminés, la présidence présentera le texte de compromis aux représentants des États membres pour approbation.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLa présentation de l’accord sur le site web du Conseil de l’Europe."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#nouveau-post-de-blog-diffusion-du-recensement-de-la-population-au-format-parquet",
    "href": "infolettre/infolettre_16/index.html#nouveau-post-de-blog-diffusion-du-recensement-de-la-population-au-format-parquet",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "Nouveau post de blog: diffusion du recensement de la population au format Parquet",
    "text": "Nouveau post de blog: diffusion du recensement de la population au format Parquet\nChaque année, l’Insee diffuse des statistiques construites à partir du recensement de la population, l’une des enquêtes phares de l’institut. Pour accompagner ces résultats et permettre à de nombreux acteurs de creuser ces données très riches dans des dimensions qui les intéressent, l’Insee diffuse également des bases de données détaillées construites après anonymisation de près de 20 millions de données individuelles.\nCes données, d’une extrême richesse, étaient historiquement complexes à manipuler du fait de leur volumétrie. La diffusion de celles-ci sous le format Parquet, une première mondiale parmi les instituts de statistique publique, vise à simplifier leur exploitation. Pour accompagner cette innovation, en partenariat avec les services de diffusion de l’Insee, le dernier post de blog du réseau présente un guide pratique d’utilisation de ces données dans plusieurs langages de traitement ( , Python  et Observable ) par le biais de DuckDB.\nCombien d’habitants de Toulouse ont changé de logement sur l’année ? Quels sont les départements avec le plus de centenaires ? Le post de blog vous montrera comment calculer ces statistiques. Et si vous désirez découvrir ce format avec des exemples additionnels, ce post d’Eric Mauvière vous intéressera également.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLe post de blog ;\nUn article sur le format Parquet dans le Courrier des stats n°9 écrit par Alexis Dondon et Pierre Lamarche ;\nLe blog d’Eric Mauvière qui présente une série d’articles sur le format Parquet;\nLa présentation de Romain Lesur sur le sujet pour l’atelier Modernisation of Official Statistics de l’UNECE."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#le-sspcloud-se-dote-dun-explorateur-de-fichiers-basé-sur-duckdb",
    "href": "infolettre/infolettre_16/index.html#le-sspcloud-se-dote-dun-explorateur-de-fichiers-basé-sur-duckdb",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "Le SSPCloud se dote d’un explorateur de fichiers basé sur DuckDB",
    "text": "Le SSPCloud se dote d’un explorateur de fichiers basé sur DuckDB\nDuckDB est un outil extrêmement efficace pour lire des fichiers Parquet et CSV. Outre son efficacité, DuckDB présente l’avantage d’être disponible par le biais de plusieurs clients: , Python  mais aussi un navigateur web grâce à Javascript  . Des acteurs majeurs de l’écosystème de la data science, notamment Observable, ont fait de DuckDB une pierre angulaire de leurs explorateurs de données. L’avantage de cette approche, typique du web assembly (approche visant à mettre à disposition des logiciels de calculs scientifiques par le biais d’un simple navigateur), est que seul Javascript , qui est disponible sur tout navigateur, est nécessaire pour visualiser et effectuer des traitements analytiques sur des données.\nLe SSPCloud, la plateforme moderne de traitement de données développée par l’Insee et mise à disposition de près de 3000 agents de l’administration ou étudiants, vient de mettre en oeuvre un explorateur aux fonctionnalités similaires 🚀.\n\n\n\nUn exemple d’utilisation de cet explorateur sur les données détaillées du recensement 👆️\n\n\nCelui-ci s’appuie sur DuckDB et permet de visualiser de manière très fluide les fichiers aux formats Parquet et CSV. Il ne se restreint pas aux données disponibles sur les espaces de stockage personnels du SSPCloud: n’importe quel fichier, au format adéquat et disponible sur internet, peut être lu avec ce visualiseur. Il n’est d’ailleurs pas nécessaire d’avoir un compte sur le SSPCloud pour l’utiliser, il suffit que le fichier que l’on souhaite lire soit un fichier open data  😍 !\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nDes éléments sur le web assembly ;\nL’explorateur de fichier du SSPCloud ;\nL’explorateur de fichiers de data.gouv basé sur la même approche technologique."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#laccessibilité-de-jupyter-améliorée-avec-le-concours-de-linsee",
    "href": "infolettre/infolettre_16/index.html#laccessibilité-de-jupyter-améliorée-avec-le-concours-de-linsee",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "L’accessibilité de Jupyter améliorée avec le concours de l’Insee",
    "text": "L’accessibilité de Jupyter améliorée avec le concours de l’Insee\nAfin de ne pas pénaliser certains publics, les logiciels doivent respecter des critères d’accessibilité. Ils doivent notamment avoir de nombreuses fonctionnalités accessibles sans souris, exclusivement par le biais du clavier. Cependant, Jupyter, logiciel bien connu des data scientists, par la structure complexe de son interface, présentait plusieurs défauts, comme la difficulté à naviguer dans la page pour trouver le menu nécessaire pour éditer du code.\nGrâce à une subvention de l’Insee, des travaux d’amélioration de l’accessibilité de Jupyter ont pu être menés. Les prochaines versions du logiciel devraient être plus accessibles, et, entre autres, plus pratiques d’usage pour les data scientists qui privilégient le clavier à la souris pour se déplacer dans un document.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nL’annonce sur le blog de Jupyter ;\nLe principe d’accessibilité clavier du W3C."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html",
    "href": "infolettre/infolettre_18/index.html",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "",
    "text": "Ce mois-ci, la première partie de la newsletter est consacrée à l’actualité dense dans le domaine des IA génératives et à l’annonce d’un nouveau générateur de site web pour les data scientists. Suivent les actualités du réseau, notamment une présentation de Quarto par Christophe Dervieux (Posit) et le replay de la présentation d’Eric Mauvière sur le sujet des bonnes pratiques de dataviz."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#sora-la-nouvelle-ia-dopenai-pour-générer-des-vidéos",
    "href": "infolettre/infolettre_18/index.html#sora-la-nouvelle-ia-dopenai-pour-générer-des-vidéos",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Sora, la nouvelle IA d’OpenAI pour générer des vidéos",
    "text": "Sora, la nouvelle IA d’OpenAI pour générer des vidéos\n\n Source : New York Times d’après OpenAI.\n\n\nInstruction utilisée par OpenAI pour générer cette vidéo\n\n“Animated scene features a close-up of a short fluffy monster kneeling beside a melting red candle. The art style is 3D and realistic, with a focus on lighting and texture. The mood of the painting is one of wonder and curiosity, as the monster gazes at the flame with wide eyes and open mouth. Its pose and expression convey a sense of innocence and playfulness, as if it is exploring the world around it for the first time. The use of warm colors and dramatic lighting further enhances the cozy atmosphere of the image.”\n\n\nAprès avoir révolutionné le champ de la génération d’image avec DallE (texte \\(\\to\\) image), de la génération de textes avec ChatGPT (texte \\(\\to\\) texte), OpenAI a rendu public les premières productions d’un modèle de génération de vidéos à partir d’instructions (texte \\(\\to\\) vidéo). Ce produit, nommé Sora, génère des vidéos d’un réalisme qui n’avait encore jamais été atteint par les IA génératrices de vidéos. Jusqu’à présent, les modèles de ce type généraient des images dont les formes étaient grossières, la résolution d’une qualité faible et dont les mouvements étaient peu vraisemblables.\n\n   \nSource : Le Monde\n\nSora n’est pas directement mis à disposition du grand public, contrairement aux autres services d’OpenAI. Ce produit n’est partagé qu’à des utilisateurs identifiés par OpenAI comme pouvant représenter le public cible - des réalisateurs par exemple - ou ayant une expertise sur des sujets comme la désinformation, les biais, la connaissance des algorithmes de recommandation, etc. Cette diffusion restreinte vise à recevoir des retours de la part de potentiels clients ou d’experts sur les risques de ces technologies. La communication par le biais de quelques vidéos choisies par OpenAI permet, dans le même temps, de créer une attente du grand public avant la mise à disposition plus large.\nComme Dall-E, Midjourney et consorts qui généraient des mains avec trop de doigts, le réseau de neurones derrière Sora a encore des difficultés à respecter certaines règles élémentaires de vraisemblance. Par exemple, dans la vidéo ci-dessous, les événements liés à un bris de verre s’enchaînent dans un ordre incohérent.\n Source : OpenAI\nOpenAI a déjà prévu de nombreuses applications à ce modèle. Outre la génération de vidéos à partir d’instructions verbales, Sora est capable d’animer une image, de compléter une vidéo déjà existante avec une vidéo fictionnelle, d’éditer une vidéo déjà existante pour changer des éléments… Les secteurs de la communication, de la création et de la diffusion de contenu sont concernés au premier chef mais la richesse des fonctionnalités possibles et la simplicité d’usage des produits d’OpenAI laissent penser que les applications iront bien au-delà de ces secteurs économiques ; la vidéo occupe maintenant une place prédominante sur internet et sur les réseaux sociaux pour de multiples usages.\nCe modèle soulève, comme Dall-E ou ChatGPT avant lui, des enjeux de propriété intellectuelle puisqu’il a aussi été entraîné sur des corpus massifs collectés depuis internet. Le réalisme des vidéos générées peut également laisser craindre, sans marque d’identification claire du fait que la vidéo est générée numériquement (principe du watermark), des dérives autour de la mésinformation, notamment des vidéos malveillantes et réalistes de personnes dans des situations inventées (des deepfakes) ou la prolifération de contenus choquants si les garde-fous dans la génération de contenus sont outrepassés.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLa présentation de Sora sur le site d’OpenAI ;\nUn article plus technique d’OpenAI sur les fonctionnalités de Sora ;\nLes 10mn de vidéos de présentation de Sora par OpenAI ;\nUn article du New York Times présentant Sora\nUn article sur le site The Conversation sur les enjeux pour certains secteurs économiques."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#le-chat-un-concurrent-à-chatgpt-par-mistral-ai",
    "href": "infolettre/infolettre_18/index.html#le-chat-un-concurrent-à-chatgpt-par-mistral-ai",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "“Le Chat” : un concurrent à ChatGPT par Mistral AI 🐱",
    "text": "“Le Chat” : un concurrent à ChatGPT par Mistral AI 🐱\nFin février, la startup française Mistral AI a rendu public, en accès libre, une IA conversationnelle aux fonctionnalités similaires à ChatGPT nommée “Le Chat”.\nCe service utilise le grand modèle de langage (LLM) Mistral Large, dernier né des LLM multilangues entraînés par Mistral AI. Contrairement à d’autres modèles de Mistral AI, celui-ci n’est pas ouvert ; l’accès n’y est possible que par le biais des services de Mistral ou par le biais du cloud Microsoft Azure, suite à un partenariat entre l’entreprise américaine et la startup française (tarification en fonction du volume de requêtes).\nSelon les évaluations réalisées fin février, avant la sortie de Claude 3 (voir plus bas 👇️), ce modèle présentait des performances supérieures à celles des modèles open source, notamment LLaMa-2, sur une série d’évaluations de la véracité des réponses proposées par une IA et sur les capacités de raisonnement de celle-ci à partir de tests standardisés. Sur des questions d’un niveau de premier cycle universitaire (métrique MMLU proposée par Hendrycks et al. (2021)), Mistral Large propose la bonne réponse dans 81% des cas, ce qui l’amène presque au niveau de GPT-4 (86%) et bien au-dessus de Llama-2 (70%), le meilleur modèle opensource à l’heure actuelle.\n\n\nClassement des principaux modèles de langage lors de la sortie de Mistral Large\n\n\n\n\nPerformance des principaux LLM sur la métrique MMLU, une série de 57 tests sur la fiabilité des réponses et les capacités de raisonnement des IA conversationnelles. Source : Mistral AI, fin février\n\n\n\n\n\n\nExemple de questions de niveau licence posées pour évaluer la qualité d’un modèle selon la métrique MMLU proposée par Hendrycks et al. (2021) (accéder à l’article de recherche)\n\n\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nhttps://chat.mistral.ai/, l’IA conversationnelle proposée par Mistral AI ;\nLe post de blog par Mistral AI annonçant Mistral Large ;\nLa newsletter d’Andrew Ng consacrée à Mistral Large ;\nL’article d’Hendrycks et al. (2021) à l’origine de la métrique MMLU utilisée pour classer les modèles."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#les-performances-de-gpt-4-dépassées-pour-la-première-fois",
    "href": "infolettre/infolettre_18/index.html#les-performances-de-gpt-4-dépassées-pour-la-première-fois",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Les performances de GPT-4 dépassées pour la première fois",
    "text": "Les performances de GPT-4 dépassées pour la première fois\nQuelques jours seulement après la sortie de Mistral Large, un autre modèle de langage est venu concurrencer le modèle d’OpenAI GPT-4. Ce modèle nommé Claude 3 est le premier à obtenir des performances supérieures à GPT-4 (le modèle derrière la version Pro de ChatGPT) sur les principaux tests de qualité des modèles. Ce modèle, créé par Anthropic et disponible en trois versions plus ou moins puissantes (Haiku, Sonnet et Opus), n’est pas encore disponible pour les utilisateurs résidant dans l’Union Européenne.\n\n\nLes trois modèles Claude-3 disponibles\n\n\n\n\nSource : Anthropic\n\n\n\n\n\nComparaison des performances des LLM\n\n \n\nLes modèles Claude sont développés par l’entreprise Anthropic, créée par des anciens employés d’OpenAI considérant que la problématique de la sécurité des IA n’était pas assez mise en avant par OpenAI. Valorisée autour de 18 milliards d’euros en ce début d’année 2024, elle a bénéficié de financements importants d’Amazon et de Google, ces deux entreprises ayant investi respectivement 4 et 2 milliards de dollars. Les modèles Claude sont disponibles pour les utilisateurs des cloud d’Amazon (AWS) ou de Google (GCP) à l’instar des modèles GPT disponibles aux utilisateurs du cloud de Microsoft (Azure). La concurrence entre OpenAI et Anthropic est ainsi l’occasion d’un affrontement entre les trois principaux acteurs du cloud. Au-delà de la concurrence entre leurs investisseurs, les modèles économiques d’Anthropic et d’OpenAI diffèrent. Anthropic vise plutôt à proposer des services à des entreprises accessibles par le biais d’API là où OpenAI propose plutôt des outils grands publics avec des fonctionnalités supplémentaires pour les acteurs spécialisés. Parmi les partenaires principaux d’Anthropic, on retrouve Gitlab, Quora ou Salesforce (l’éditeur de logiciel derrière Slack). A l’instar des modèles Mistral Large ou GPT-4, le modèle Claude 3 n’est pas open source.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nL’annonce de Claude 3 par Anthropic ;\nUn article sur Anthropic par le New York Times et un autre par Forbes."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#observable-propose-un-constructeur-de-sites-statiques-pour-sabstraire-des-notebooks",
    "href": "infolettre/infolettre_18/index.html#observable-propose-un-constructeur-de-sites-statiques-pour-sabstraire-des-notebooks",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Observable propose un constructeur de sites statiques, pour s’abstraire des notebooks",
    "text": "Observable propose un constructeur de sites statiques, pour s’abstraire des notebooks\nAfin de démocratiser l’utilisation de Javascript au-delà du cercle des développeurs web, Mike Bostock, ancien responsable des dataviz du New York Times, la référence en la matière, a créé il y a quelques années Observable.\nEn plus d’être une extension du langage Javascript à la grammaire familière aux connaisseurs de Python et R, Observable vise à créer une communauté d’utilisateurs de Javascript à l’interface entre data scientists et développeurs web. Pour cela, le site observablehq.com se propose d’être un réseau social de notebooks en Javascript, un peu comme Github faisant office de réseau social du code. Les notebooks Observable permettent de rapidement prendre en main du code Javascript pour créer des analyses de données interactives qui peuvent ensuite être facilement partagées par le biais du site observablehq.com pour simplifier les réutilisations du code proposé ou des données sous-jacentes.\nCependant, si les notebooks sont un terrain fertile pour l’expérimentation, ils montrent rapidement leurs limites dès qu’on désire s’abstraire de l’hébergement sur observablehq.com. Pour mettre à disposition des visualisations interactives sur d’autres sites, les sites statiques sont plus simples d’usage. Historiquement, l’écosystème Javascript est construit autour d’imposants frameworks comme React, bien connus des développeurs web mais méconnus des data scientists qui sont néanmoins amenés à livrer de plus en plus d’applications interactives pour valoriser des données.\nL’annonce d’Observable Framework, un constructeur de sites statiques, représente un changement d’approche. Observable Framework vise à être un framework permettant aux data scientists de construire des sites web en mélangeant des étapes de préparation de données en R, Python ou SQL (via DuckDB), du formattage de texte en Markdown et de l’interactivité grâce au langage Observable. L’approche est ainsi similaire à celle de Quarto, la référence pour les data scientists désirant construire des publications reproductibles (voir la section événements 👇️ pour en apprendre plus). Ce dernier écosystème permet déjà depuis quelques temps de compléter du travail de données en R ou Python avec des traitements en Observable pour obtenir un site web interactif sans besoin de solutions techniques complexes comme Shiny ou Streamlit.\nLes évolutions à venir d’Observable Framework sont donc à surveiller, cet écosystème pouvant être amené, s’il rencontre du succès, à rentrer dans la boîte à outil standard des data scientists comme Quarto est déjà en train de le faire. Le site observablehq.com ne va pas pour autant disparaître : celui-ci restera un lieu où on peut tirer avantage de la simplicité des notebooks pour l’expérimentation ou pour la mise à disposition de tutoriels pédagogiques. Ce virage est similaire à celui pris par Python dans la communauté des data scientists où les notebooks, après avoir connu une phase hégémonique, sont revenus à leur fonction initiale : des carnets pour expérimenter servant de brouillon avant l’écriture de scripts ou alors de belles pages, mêlant texte et code, pour présenter une démarche de manière pédagogique.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nL’annonce d’Observable Framework ;\nL’interactivité dans Quarto grâce aux cellules Observable ;\nLe cours de “Mise en production de projets data science” de l’ENSAE où les enjeux techniques et humains de la mise à disposition de tels sites sont évoqués."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#chistophe-dervieux-quarto-une-évolution-de-r-markdown-pour-des-travaux-statistiques-reproductibles-2-mai",
    "href": "infolettre/infolettre_18/index.html#chistophe-dervieux-quarto-une-évolution-de-r-markdown-pour-des-travaux-statistiques-reproductibles-2-mai",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Chistophe Dervieux, “Quarto : Une évolution de R Markdown pour des travaux statistiques reproductibles” (📅 2 mai)",
    "text": "Chistophe Dervieux, “Quarto : Une évolution de R Markdown pour des travaux statistiques reproductibles” (📅 2 mai)\nPour fiabiliser la production de documents construits en valorisant des données (tableaux, graphiques, etc.), RStudio (devenu Posit depuis) a construit il y a quelques années l’écosystème R Markdown permettant de produire du document en mélangeant code et texte.\nCette problématique des publications reproductibles est devenue incontournable dans l’écosystème R et la solution R Markdown est dorénavant largement utilisée. Pour étendre les vertus de cette approche à d’autres langages, Posit a commencé à développer Quarto, un écosystème reprenant le principe de R Markdown mais étendant ces fonctionnalités à d’autres langages de programmation, notamment Python et Observable.\nLe 2 mai de 15h à 16h, Christophe Dervieux (Posit) nous présentera Quarto, l’écosystème de publications reproductibles qui succède à R Markdown. Cet événement est proposé de manière hybride : par le biais de Zoom ou, pour les agents en poste à la Direction Générale de l’Insee, en salle 4-C-458.\n\n👉️ Ajouter cet événement à votre agenda Outlook\n👉️ Lien zoom"
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#vos-besoins-de-formation",
    "href": "infolettre/infolettre_18/index.html#vos-besoins-de-formation",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Vos besoins de formation",
    "text": "Vos besoins de formation\nL’an dernier, nous avions organisé un questionnaire pour connaître les besoins de formations des membres du réseau. Ce questionnaire est utile pour que les événements organisés dans le cadre du réseau répondent au mieux aux besoins.\nAfin de connaître les attentes et centres d’intérêt en cette année 2024, nous vous proposons un nouveau questionnaire. Celui-ci est également l’occasion d’accueillir vos retours sur les masterclass menées en 2023 en collaboration avec Datascientest si vous avez participé à celles-ci.\n\n👉️ Questionnaire"
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#replay-de-la-présentation-deric-mauvière-la-dataviz-pour-donner-du-sens-aux-données-et-communiquer-un-message",
    "href": "infolettre/infolettre_18/index.html#replay-de-la-présentation-deric-mauvière-la-dataviz-pour-donner-du-sens-aux-données-et-communiquer-un-message",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Replay de la présentation d’Eric Mauvière “La dataviz pour donner du sens aux données et communiquer un message”",
    "text": "Replay de la présentation d’Eric Mauvière “La dataviz pour donner du sens aux données et communiquer un message”\n\nLa présentation d’Eric Mauvière sur les bonnes pratiques de dataviz a rencontré un réel succès avec près de 150 participants. Le replay et les slides de cette présentation essentielle sont disponibles ci-dessous :\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-02-09-mauviere/conf_ssphub_item7-1.pdf\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Télécharger les slides\n  &lt;/a&gt;\n&lt;/p&gt;`"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html",
    "href": "infolettre/infolettre_20/index.html",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "",
    "text": "Bienvenue à la vingtième infolettre ! Il est temps de se mettre au défi de respecter ce qui a été dit il y a quelques semaines : une newsletter par mois ga-ran-tie je vous ai dit. Bon, à l’époque, je ne savais pas qu’un mois ça voulait dire 4 semaines et que 4, c’est pas beaucoup 🙃. Allez, on y va !\nBonne lecture 📔 !"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#la-troisième-journée-du-réseau-1-décembre---la-tréso-malakoff",
    "href": "infolettre/infolettre_20/index.html#la-troisième-journée-du-réseau-1-décembre---la-tréso-malakoff",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "La troisième journée du réseau 📅 1 décembre - La Tréso (Malakoff)",
    "text": "La troisième journée du réseau 📅 1 décembre - La Tréso (Malakoff)\nLes inscriptions pour la troisième journée du réseau le 1er décembre 2025 sont ouvertes. Si vous souhaitez présenter un sujet, n’hésitez pas à me contacter !"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#présentation-de-cartographia---13-janvier-2026---format-mixte-montrouge-et-en-ligne",
    "href": "infolettre/infolettre_20/index.html#présentation-de-cartographia---13-janvier-2026---format-mixte-montrouge-et-en-ligne",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "Présentation de Cartographia - 📅 13 janvier 2026 - format mixte (Montrouge et en ligne)",
    "text": "Présentation de Cartographia - 📅 13 janvier 2026 - format mixte (Montrouge et en ligne)\nFrançoise Bahoken et Nicolas Lambert vont venir nous parler de leur livre Cartographia et des questions de cartographie (!) passionnantes qu’ils y abordent. Cela se passera le 13 janvier 2026 en début d’après-midi, en format mixte : présentiel (à la DG de l’Insee) et à distance. Nicolas Lambert était déjà intervenu pour nous présenter Observable, une librairie Javascript très pratique pour faire des dataviz (ici, pour rappel). Si cela vous intéresse, réservez donc votre début d’après-midi !"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#datavisualisation",
    "href": "infolettre/infolettre_20/index.html#datavisualisation",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "Datavisualisation",
    "text": "Datavisualisation\n\nUne belle carte de migration simulée au niveau infra-national en zone endémique paludéenne africaine. Françoise Bahoken, son autrice, sera notre invitée début 2026.\n\nPour les passionnés d’espace, voici une carte en 3D et dynamique du système solaire, comprenant les planètes mais aussi les astéroïdes! Vous pouvez jouer avec le temps pour savoir où sera la Terre quand vous aurez 30/40/50/60/70 ans (spoiler alert : c’est cyclique), et vérifier aussi qu’aucun astéroïde ne passe sur la Terre d’ici là.\nPour les autres candidats à l’infographie du mois :\n\npour voir l’ensemble des livres publiés en une seule carte interactive, cela donne cela.\nla cheffe de l’infographie d’Epsiloon (magazine de sciences) a publié les étapes avant / après de ses infographies. Je vous recommande celle sur les habitudes de sommeil par pays.\n\nEnfin, un petit jeu à la SUTOM en version graphique : un graphique par jour, devinez le pays !"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#cartographie",
    "href": "infolettre/infolettre_20/index.html#cartographie",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "Cartographie",
    "text": "Cartographie\n\nDes chercheurs de l’Université de Charles (en Tchéquie) et de Freiburg ont publié une (très belle) taxonomie des bâtiments urbains, disponible en ligne. Tout est ici et vous pouvez explorer le bâti urbain de six pays d’Europe centrale et de l’est.\n\nL’IGN a publié une cartographie très détaillée des risques d’inondation et de submersion sur tout le territoire national, notamment grâce aux images LIDAR. Par exemple, vous habitez à Saint-Maur-des-Fossés? Vous pouvez y voir la simulation d’une inondation majeure chez vous.\n\nUtiliser l’IA pour faire des fonds de carte à partir d’image satellite ? Cela semble bien fonctionner d’après ce post.\nA partir des données d’utilisateurs de Facebook, des chercheurs ont bâti des indicateurs mensuels de flux migratoires couvrant 181 pays.\nAprès la prévisualisation de données Parquet en ligne, cette fois ce site permet de visualiser très facilement des fichiers de données géographiques types STAC (dont Geoparquet). Explications sur la plomberie ici.\nLa communauté Apache Sedona publie SedonaDB, un moteur de base de données analytique open source et pensé nativement aussi pour des données spatiales."
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#le-reste-cest-le-r",
    "href": "infolettre/infolettre_20/index.html#le-reste-cest-le-r",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "Le reste c’est le R",
    "text": "Le reste c’est le R\n\nLe package R tinytable, permet de faire des tableaux de qualité en de multiples formats. L’ambition de ce package est d’être simple, léger (0 dépendance à des packages externes), flexible et de différencier les données de l’affichage.\nLe package R redoc devrait être mis à jour. Il permet notamment de faire le lien entre Quarto, et la suite Office, par exemple si vous générez des documents qui seront relus par des personnes utilisant Word comme outil de travail 😉.\n\nEn machine learning, quels sont les problèmes posés par des classes d’apprentissage de tailles très différentes ? Selon cet article, il s’agit surtout 1/ d’avoir des métriques de performance adéquates, 2/ d’avoir un nombre absolu d’observations dans la classe minoritaire suffisant (et non une part dans le total) et enfin 3/ de faire attention à la fongibilité entre classes."
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#ia",
    "href": "infolettre/infolettre_20/index.html#ia",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "IA",
    "text": "IA\n\nQu’est-ce que les paramètres des modèles publiés disent des données d’entraînement sous-jacentes, confidentielles ? Des chercheurs sont allés fouiller ce que cache GPT-5.\nDu côté de l’entraînement des modèles, la gamme des données disponibles s’enrichit pour que ses données soient mieux reprises par l’IA. C’est un nouvel épisode de la course à ne pas finir page 15 des résultats Google ou page 259 des blogs Myspace.\n\nWikidata, qui stocke les données structurées de Wikipédia et consorts, propose maintenant ses données sous format de données vectorielles.\nUn programme d’harmonisation des metadata et d’API, l’Open Semantic Interchange Initiative, a été lancé pour enrichir les données d’apprentissage et la précision des IA. L’idée est d’avoir un langage YAML commun pour permettre aux IA d’échanger des données de manière robuste, par exemple par API et sans perte de sens au fur et à mesure de leur traitement par des agents d’IA.\nUne fois que l’on a les données, on cherche ensuite à savoir de combien de GPU j’ai besoin pour entraîner mon modèle. La Dinum a développé un outil pour estimer ses besoins pour cette étape.\n\nDes faiblesses des modèles de LLM sont, comme toujours, remontées, notamment sur l’importance des données d’apprentissage et d’un prompt propre. Cela met notamment en valeur le fait que les modèles ne sont pour le moment pas très efficaces quand il s’agit de trier des CV ou des articles.\n\nLe “prompt injection” : c’est l’idée de truquer l’IA dans son CV/projet d’article scientifique/sur Linkedin, par exemple avec des instructions écrites en « blanc sur blanc », pour avoir plus de chance d’être sélectionné/de détecter les messages écrits par l’IA. Cela marche à presque 100 % des cas selon cet article, d’autant plus que l’IA a un biais positif naturel.\nUne étude confirme que l’IA est particulièrement sensible à la qualité de la donnée d’apprentissage. Ainsi, 250 observations fausses suffisent pour empoisonner durablement un LLM, et ce quel que soit sa taille. Une présentation intéressante à une conférence PyData présentait par ailleurs ce qu’il se passait quand on entraîne une IA sur des images générés par une IA, avec des résultats similaires.\n\n\nLe site AI Darwin Awards rassemble des exemples d’usage raté d’IA. Globalement, les cas d’usage où l’IA est mis directement face à un client peuvent rapidement mal tourner… Peut-être que Diella, la première et seule ministre à être une IA dans le monde, rejoindra cette liste ?\nSi vous voulez comparer le résultat de deux LLM et leur empreinte carbone, n’oubliez pas l’outil ComparIA créé par une start-up d’État du ministère de la Culture."
  },
  {
    "objectID": "project/2020_donnees_caisse/index.html",
    "href": "project/2020_donnees_caisse/index.html",
    "title": "Classification des données de caisse à partir de machine learning",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\n\nDétail du projet\nLes données de caisse sont utilisées à l’Insee pour le calcul de l’IPC depuis 2010 (cf le document de travail sur le sujet). Les données de caisses donnent en effet pour chaque code-barres, chaque jour et chaque point de ventes les quantités vendues ainsi que le chiffre d’affaires et/ou le prix auquel le produit est vendu. Pour exploiter ces données, il est toutefois nécessaire de savoir quel produit se trouve derrière un code-barres. Actuellement, l’IPC se fonde sur un référentiel de codes-barres, acheté à un prestataire et qui donne une information très détaillée et structurée des caractéristiques de ces produits. Cette information est payante et ne couvre pas l’ensemble des produits. L’expérimentation vise à identifier les étapes de traitement textuel des libellés, ainsi que les méthodes de classification ou autres, permettant de coder automatiquement les libellés, sans passer par le référentiel, dans la nomenclature Coicop pour l’IPC et sur les regroupements utilisés pour Emagsa dans le cadre du projet Nosica qui vise à intégrer notamment les données de caisse dans la production des indicateurs d’activité de court-terme. Elle teste aussi leur performance sur des jeux de données tests.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLes données de caisse sont aujourd’hui utilisées en production pour le calcul de l’inflation et pour le calcul d’indicateurs d’activité conjoncturelle.  note point d’étape été 2020  note point d’étape hiver 2020-2021 \n\n\nCode du projet\n- https://github.com/InseeFrLab/predicat : API pour classification des libellés de caisse  - https://github.com/InseeFrLab/product-labelling : Application de labellisation des données de caisse"
  },
  {
    "objectID": "project/2021_codif_PCS/index.html",
    "href": "project/2021_codif_PCS/index.html",
    "title": "Codification automatique des professions dans la nomenclature PCS 2020",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\n\n\nDétail du projet\nLa rénovation de la nomenclature des PCS en 2020 s’accompagne de la promotion d’un outil d’autocomplétion des libellés de profession dans une liste de libellés enrichis permettant le codage direct dans la case de la nomenclature ou des regroupements ad hoc complémentaires de la nomenclature. Or, l’outil d’autocomplétion ne sera disponible que pour des collectes informatisées et par ailleurs il comprend la possibilité de répondre “hors-liste”. Pour pouvoir intégrer la nouvelle PCS 2020 dans le recensement de population à la collecte 2024 et être en mesure de coder aussi les bulletins papier tout comme les réponses informatisées “hors-liste”, un algorithme de codification automatique en PCS 2020 de ces bulletins doit être créé. Suite au report de l’enquête de recensement 2021, des gestionnaires ont annoté en PCS 2020, avec double codage et arbitrage, 119 000 bulletins issus de l’EAR 2020. Le but de l’expérimentation est de tester et comparer différents modèles d’apprentissage statistique et méthodes de prétraitement pour le codage des professions dans la nomenclature PCS 2020 à partir du libellé de profession ainsi que des variables annexes utilisées lors de la phase d’annotation (statut de l’employeur, etc.) et d’en retenir le plus performant. L’objectif est de maintenir le taux de codifications correctes ainsi que le taux d’envoi en reprise manuelle à des niveaux similaires à l’existant.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\n- note bilan de l’expérimentation et résultats associés (décembre 2021)  - article et communication aux journées de méthodologie statistiques 2022 “Application de techniques de machine learning pour coder les professions dans la nomenclature des professions et catégories socio-professionnelles 2020”  article et slides présentés à la Conférence Q2022 “Machine learning for coding occupations in the Census: first lessons from experiments to production”.\n\n\nCode du projet"
  },
  {
    "objectID": "project/2022_Enquete_Budget_Famille/index.html",
    "href": "project/2022_Enquete_Budget_Famille/index.html",
    "title": "Travaux méthodologiques sur l’enquête Budget de Famille",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\n\nDétail du projet\nLa collecte de la prochaine enquête Budget de Famille en 2026 sera largement différente de celle de l’enquête précédente (2017).  - En effet, une partie des données de consommation des ménages sera remontée à travers une application mobile mise à disposition des ménages enquêtés.  - De plus, l’enregistrement en base de données des produits issus des données de consommation ne se fera plus par les enquêteurs mais par un prestataire de saisie ou d’extraction automatique.  Deux axes existent dans cette expérimentation :  - Le premier axe consiste à tester les solutions open-source d’extraction automatique du contenu de tickets de caisse, afin de pouvoir mieux dialoguer avec des partenaires comme Teklia et potentiellement pouvoir procéder à une partie des extractions en interne ;  - Le second axe (principal) est autour de la codification automatique des produits de consommation dans la nomenclature COICOP : entraînement d’un modèle de classification sur les données de l’enquête précédente et élaboration d’une stratégie de reprise optimale.\n\n\nActeurs\nInsee (DSDS)\n\n\nRésultats du projet\nProjet en cours\n\n\nCode du projet\n- https://git.lab.sspcloud.fr/ssplab/bdf  - https://git.lab.sspcloud.fr/ssplab/experimentation-bdf"
  },
  {
    "objectID": "project/2022_codif_ape/cards/cards14.html",
    "href": "project/2022_codif_ape/cards/cards14.html",
    "title": "PyData Paris",
    "section": "",
    "text": "Faciliter l’entraînement mais pas forcément –&gt;"
  },
  {
    "objectID": "project/2022_codif_ape/index.html",
    "href": "project/2022_codif_ape/index.html",
    "title": "Codification automatique de l’activité principale des entreprises",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\n\n\nDétail du projet\nLa codification de l’activité principale exercée (APE) des entreprises à partir de descriptions d’activité (sous forme de texte libre) dans le répertoire Sirene était auparavant réalisée grâce à 6 environnements de codification déterministes mobilisant un nombre énorme de règles de décision. Le but de l’expérimentation est de tester les performances des modèles d’apprentissage statistique pour prédire la catégorie d’APE dans le cadre de la refonte du répertoire Sirene et de la mise en place d’un guichet unique. \n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLe modèle développé présente des performances similaires aux modèles précédents, en les automatisant, et propose par ailleurs une aide à la décision. Le modèle a par ailleurs été mis en production, en appliquant quand cela était possible les principes MLOps.  Les présentations et supports écrits concernant le projet sont accessibles sur ce site.\n\n\nCode du projet\nDépôts de code accessibles ici. Ils comprennent :  - Code relatif à l’annotation de données à l’aide de Label Studio ;  - Code d’une API web de codification déployée sur le SSP Cloud ;  - Code qui implémente un dashboard de visualisation permettant de surveiller l’activité d’un modèle de codification en production et accessible via une API web ;  - Code pour l’entraînement de modèles de classification de l’APE. \n\n\n\n\n\nDocuments relatifs au projet\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Échange avec Statec - Luxembourg\n            \n\n            \n              Description : Cas d'usage de Label Studio à la DSE de l'Insee\n\n            \n\n            \n            \n              \n                Nathan Randriamanana\n                23 sept. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            PyData Paris\n            \n\n            \n              Description : Presentation at PyData Paris 2025\n\n            \n\n            \n            \n              \n                Cédric Couralet, Meilame Tayebjee\n                1 oct. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            WP10 Project Presentation\n            \n\n            \n              Description : Presentation given during the WP10\n\n            \n\n            \n            \n              \n                Thomas Faria, Meilame Tayebjee\n                22 mai 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Conférence UNECE\n            \n\n            \n              Description : Presentation given during the UNECE Generative AI and Official Statistics Workshop 2025\n\n            \n\n            \n            \n              \n                Thomas Faria\n                12 mai 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Ateliers concepteurs\n            \n\n            \n              Description : Présentation non technique de la codification automatique de la NAF\n\n            \n\n            \n            \n              \n                Meilame Tayebjee\n                4 avr. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Groupe de veille Codification Automatique n°10\n            \n\n            \n              Description : Présentation du package torchFastText et ses performances sur la codification de l'APE\n\n            \n\n            \n            \n              \n                Meilame Tayebjee\n                5 mars 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Groupe de veille Codification Automatique n°9\n            \n\n            \n              Description : Travaux sur le passage de la nomenclature NAF2008 Rev2.1 à la NAF2025\n\n            \n\n            \n            \n              \n                Thomas Faria, Nathan Randriamanana\n                6 nov. 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            European Conference for Quality in Official Statistics 2024\n            \n\n            \n              Description : Retraining strategies for an economic activity classification model\n\n            \n\n            \n            \n              \n                Thomas Faria, Nathan Randriamanana, Tom Seimandi\n                24 mai 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Échange avec l'Inserm\n            \n\n            \n              Description : Codification automatique de l'APE à l'Insee\n\n            \n\n            \n            \n              \n                Thomas Faria et Nathan Randriamanana\n                7 mai 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Eurostat webinar\n            \n\n            \n              Description : Presentation given during the NACE implementation webinar\n\n            \n\n            \n            \n              \n                Nathan Randriamanana\n                30 avr. 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            MLOps exhange with StatCan\n            \n\n            \n              Description : Slides used for an exchange with StatCan on MLOps\n\n            \n\n            \n            \n              \n                Romain Avouac, Thomas Faria et Tom Seimandi\n                19 mars 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Formation BCEAO 2023\n            \n\n            \n              Description : Présentation pour la formation Big Data donné à la BCEAO à Dakar\n\n            \n\n            \n            \n              \n                Romain Avouac, Thomas Faria et Tom Seimandi\n                27 sept. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Conférence UNECE\n            \n\n            \n              Description : Presentation given during the UNECE Machine Learning for Official Statistics Workshop 2023\n\n            \n\n            \n            \n              \n                Thomas Faria and Tom Seimandi\n                5 juin 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Journée SSP Hub\n            \n\n            \n              Description : Présentation lors de l'évènement de lancement du SSP Hub\n\n            \n\n            \n            \n              \n                Thomas Faria et Tom Seimandi\n                17 avr. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Séminaire DMS\n            \n\n            \n              Description : Séminaire interne du département des méthodes statistiques de l'Insee\n\n            \n\n            \n            \n              \n                Thomas Faria et Tom Seimandi\n                1 sept. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n\n\nAucun article correspondant"
  }
]