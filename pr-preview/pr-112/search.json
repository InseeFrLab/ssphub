[
  {
    "objectID": "project/2024_scanr/index.html",
    "href": "project/2024_scanr/index.html",
    "title": "scanR, une application pour observer le paysage de la recherche et de l’innovation en France",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nscanR, explorer le monde de la recherche et de l’innovation française\n\n\n\n\nDétail du projet\nscanR est une application web pour aider à caractériser les structures publiques (unités de recherche de tous types, institutions publiques) et privées (entreprises) qui participent à la recherche et à l’innovation en France. scanR aide également à identifier les orientations des travaux des chercheuses et chercheurs actifs en France depuis le début des années 1990.  scanR combine les données structurées sous licence libre (Publications et thèses, participation à des projets de recherche collaboratifs, spin-off, brevets, etc.) et des informations ouvertes directement extraites des sites web des acteurs de la recherche et de l’innovation. Ces informations proviennent ainsi d’environ 13 sources différentes (theses.fr, le baromètre de la science ouverte, HAL, Commission européenne, INPI, ANR, l’office européen des brevets …).  A partir de cette information, scanR produit des fiches présentant la contribution à la recherche et à l’innovation en France de plusieurs dizaines de milliers de chercheuses et chercheurs et de près de 60 000 entreprises, organismes, établissements d’enseignement supérieur et laboratoires publics.  Enfin, à travers son moteur de recherche basé sur ElasticSearch, scanR aide ses utilisateurs à identifier les acteurs et chercheurs actifs en France associés à un problème de recherche.\n\n\nActeurs\nService statistique du Ministère de l’Enseignement Supérieur et de la Recherche (SIES)\n\n\nRésultats du projet\nLe projet met à disposition des fiches par structure et auteurs présentant leur organisation, activité, domaines de recherche, source de financement. Il propose par ailleurs un moteur de recherche sur les structures, auteurs, financements, publications, brevets de la recherche en France. Il met aussi à disposition des outils d’analyse des résultats, notamment des visualisations de graphs. Le projet met aussi à disposition plusieurs API pour récupérer des données et les données.  Le projet scanR est en production depuis 2016. Une v2 a été mise en production en 2020 et une v3 en 2024.\n\n\nProduits et documentation du projet\nscanR est un site scanr.enseignementsup-recherche.gouv.fr. Ce site comprend :  - un moteur de recherche parmi les différents produits de la recherche et de l’innovation en France  - la documentation pour accéder aux quatre API disponibles ;  - les différentes sources de données utilisées pour le projet.\n\n\nCode du projet\n- Le code est disponible sur GitHub  https://github.com/dataesr/scanr-ui\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\n\nwebscraping\n\nen production\n\ndatavisualisation\n\nSIES\n\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\n\nwebscraping\n\ndatavisualisation\n\nopen-data\n\nen production\n\nSIES\n\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2023_pynsee/index.html",
    "href": "project/2023_pynsee/index.html",
    "title": "pynsee, un package Python  pour récupérer les données de l’Insee",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\npynsee, un package Python pour récupérer les données de l’Insee\n\n\n\n\nDétail du projet\npynsee donne un accès rapide à plus de 150 000 séries macroéconomiques, une dizaine d’ensembles de données locales, de nombreuses sources disponibles sur insee.fr ainsi que des métadonnées clés et une base de données SIRENE contenant des données sur toutes les entreprises françaises. Consultez la page détaillée de l’API portail-api.insee.fr.  Ce paquet est une contribution à la recherche reproductible et à la transparence des données publiques. Il bénéficie des développements réalisés par les équipes travaillant sur les API à l’Insee et à l’IGN.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLe package pynsee est en production et téléchargeable depuis GitHub. La documentation est aussi publiée ici.\n\n\nCode du projet\n- Le code est disponible sur GitHub  https://github.com/InseeFrLab/pynsee\n\n\n\n\n\nProjets similaires ayant abouti à la création de package\n\n\n\n\n\n\n\n\n\n\nDoremifasol\n\n\nLe package  R Doremifasol facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur…\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\nUtiliser les images satellites pour améliorer le recensement de la population dans les territoire ultra-marins\n\n\n\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_satellites/index.html",
    "href": "project/2022_satellites/index.html",
    "title": "Utilisation des images satellites pour la statistique publique",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\n\n\nDétail du projet\nFin 2022, un groupe de travail expérimental sur l’utilisation des données satellites pour la statistique publique a été lancé. Cette initiative visait à améliorer l’organisation des enquêtes cartographiques dans les départements d’outre-mer, particulièrement en Guyane et à Mayotte. Ces enquêtes servent à mettre à jour le répertoire individus / logement (RIL) chaque année, et facilitent l’identification en amont des enquêtes de nouvelles zones d’habitations spontanées ou temporaires. Des algorithmes appliqués à des photographies satellites permettent d’identifier de telles zones, qui à une année d’exercice donnée nécessiteront plus d’enquêteurs qu’à l’exercice précédent.  Le projet est présenté plus précisément ici.\n\n\nActeurs\nInsee (DG et Direction interrégionale Réunion-Mayotte)\n\n\nRésultats du projet\nLes résultats du projet sont présentés plus précisément sur le site dédié. La phase d’expérimentation s’est achevée avec:  - La transmission de statistiques d’évolution de bâti par îlot à Mayotte. Ces statistiques ont servi à l’organisation de l’enquête cartographique complémentaire 2024 à Mayotte ;  - Le développement d’une application de visualisation intégrant des fonds de carte construits à partir de tuiles Pleiades et les bâtiments détectés sur ces tuiles avec nos algorithmes, ainsi que des couches de détection de changement ;  - La création du package Python de traitement d’images satellites Astrovision\n\n\nCode du projet\n- https://github.com/InseeFrLab/astrovision : package Python pour travailler avec des données satellites:  - https://inseefrlab.github.io/satellite-images-webapp/: code de l’application de visualisation  - https://github.com/InseeFrLab/satellite-images-preprocess: code de pré-processing des données  - https://github.com/InseeFrLab/satellite-images-train : code pour l’entraînement des modèles  - https://github.com/InseeFrLab/satellite-images-inference : code pour l’inférence\n\n\n\n\n\nDocuments relatifs au projet sur les données satellites à l’Insee\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            TODO List\n            \n\n            \n              Description : TODO\n\n            \n\n            \n            \n              \n                Raya Berova, Thomas Faria et Clément Guillo\n                19 nov. 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Présentation JMS 2025\n            \n\n            \n              Description : Utilisation des images satellites pour améliorer le repérage des logements à Mayotte.\n\n            \n\n            \n            \n              \n                Maëlys Bernard, Raya Berova et Thomas Faria\n                25 nov. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Article JMS 2025\n            \n\n            \n              Description : Utilisation des images satellites pour améliorer le repérage des logements à Mayotte.\n\n            \n\n            \n            \n              \n                Maëlys Bernard, Raya Berova et Thomas Faria\n                25 nov. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Utilisation des images satellites pour améliorer le repérage des logements dans les départements d'outre-mer\n            \n\n            \n              Description : Slides pour le séminaire de Méthodologie Statistique & Sciences des données de la DMCSI - La terre vue d’en haut : images satellites, images aériennes et statistique publique.\n\n            \n\n            \n            \n              \n                Raya Berova et Thomas Faria\n                3 juil. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Conference on New Techniques and Technologies for official Statistics (NTTS 2025)\n            \n\n            \n              Description : Slide deck for the NTTS 2025 conference.\n\n            \n\n            \n            \n              \n                Raya Berova, Thomas Faria et Clément Guillo\n                12 mars 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            AIML4OS Workshop on Earth Observation\n            \n\n            \n              Description : Slideshow presentation of the project Detecting changes in buildings in French overseas departments for Work Package 7,\nArtificial Intelligence and Machine Learning for Official Statistics, organized by Eurostat.\n\n            \n\n            \n            \n              \n                Raya Berova, Thomas Faria et Clément Guillo\n                13 nov. 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Séminaire à la Dirag\n            \n\n            \n              Description : Ce séminaire marque la fin des travaux d’expérimentation. À cette occasion, nous présenterons l’intégralité de la chaîne de traitement, de la récupération des images jusqu’à l’application d’aide à la décision, en mettant l’accent sur les difficultés rencontrées. La poursuite de ces travaux dépendra des besoins réels de l’Insee et nécessitera des compétences variées et une technicité élevée pour maintenir et/ou améliorer la chaîne de traitement.\n\n            \n\n            \n            \n              \n                Raya Berova, Gaëtan Carrere, Thomas Faria, Clément Guillo et Tom Seimandi\n                28 mai 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Document de travail\n            \n\n            \n              Description : Document retraçant les avancées et résultats obtenus sur les travaux exploratoires utilisant des images satellitaires\n\n            \n\n            \n            \n              \n                Raya Berova, Gaëtan Carrere, Thomas Faria, Clément Guillo et Tom Seimandi\n                16 mai 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Application web\n            \n\n            \n              Description : Application web à destination des agents facilitant l'utilisation des résultats fournis par le modèle de segmentation.\n\n            \n\n            \n            \n              \n                \n                5 févr. 2026\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Documentation\n            \n\n            \n              Description : Ici est rassemblée toute la documentation technique sur le travail effectué au cours de l'expérimentation sur les données satellite\n\n            \n\n            \n            \n              \n                Raya Berova, Gaëtan Carrere, Thomas Faria, Clément Guillo et Tom Seimandi\n                5 févr. 2026\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n\n\nAucun article correspondant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoremifasol\n\n\nLe package  R Doremifasol facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur…\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\npynsee, un package Python  pour récupérer les données de l’Insee\n\n\nLe package  pynsee facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur Github.\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_codif_ape/index.html",
    "href": "project/2022_codif_ape/index.html",
    "title": "Codification automatique de l’activité principale des entreprises",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\n\n\nDétail du projet\nLa codification de l’activité principale exercée (APE) des entreprises à partir de descriptions d’activité (sous forme de texte libre) dans le répertoire Sirene était auparavant réalisée grâce à 6 environnements de codification déterministes mobilisant un nombre énorme de règles de décision. Le but de l’expérimentation est de tester les performances des modèles d’apprentissage statistique pour prédire la catégorie d’APE dans le cadre de la refonte du répertoire Sirene et de la mise en place d’un guichet unique. \n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLe modèle développé présente des performances similaires aux modèles précédents, en les automatisant, et propose par ailleurs une aide à la décision. Le modèle a par ailleurs été mis en production, en appliquant quand cela était possible les principes MLOps.  Les présentations et supports écrits concernant le projet sont accessibles sur ce site.\n\n\nCode du projet\nDépôts de code accessibles sur Github . Ils comprennent :  - Code relatif à l’annotation de données à l’aide de Label Studio ;  - Code d’une API web de codification déployée sur le SSP Cloud ;  - Code qui implémente un dashboard de visualisation permettant de surveiller l’activité d’un modèle de codification en production et accessible via une API web ;  - Code pour l’entraînement de modèles de classification de l’APE. \n\n\n\n\n\nDocuments relatifs au projet\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            PyData Global\n            \n\n            \n              Description : Presentation at PyData Global 2025\n\n            \n\n            \n            \n              \n                Cédric Couralet, Meilame Tayebjee\n                9 déc. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Présentation JMS\n            \n\n            \n              Description : Présentation sur la génération de données d'entraînement via LLM pour un classifieur dans un contexte de changement de nomenclature\n\n            \n\n            \n            \n              \n                Nathan Randriamanana\n                25 nov. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Présentation JMS\n            \n\n            \n              Description : Présentation de la nouvelle mouture de TTC (torchTextClassifiers)\n\n            \n\n            \n            \n              \n                Meilame Tayebjee\n                12 nov. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            JMS2025 - réentraînement lié au changement de NAF\n            \n\n            \n              Description : Papier JMS sur la méthodologie basée sur LLM\n\n            \n\n            \n            \n              \n                Nathan Randriamanana\n                25 nov. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            JMS2025 - fastText, en production et déjà obsolète\n            \n\n            \n              Description : Papier JMS sur le package\n\n            \n\n            \n            \n              \n                Meilame Tayebjee\n                23 sept. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Échange avec Statec - Luxembourg\n            \n\n            \n              Description : Cas d'usage de Label Studio à la DSE de l'Insee\n\n            \n\n            \n            \n              \n                Nathan Randriamanana\n                23 sept. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            PyData Paris\n            \n\n            \n              Description : Presentation at PyData Paris 2025\n\n            \n\n            \n            \n              \n                Cédric Couralet, Meilame Tayebjee\n                1 oct. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            WP10 Project Presentation\n            \n\n            \n              Description : Presentation given during the WP10\n\n            \n\n            \n            \n              \n                Thomas Faria, Meilame Tayebjee\n                22 mai 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Conférence UNECE\n            \n\n            \n              Description : Presentation given during the UNECE Generative AI and Official Statistics Workshop 2025\n\n            \n\n            \n            \n              \n                Thomas Faria\n                12 mai 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Ateliers concepteurs\n            \n\n            \n              Description : Présentation non technique de la codification automatique de la NAF\n\n            \n\n            \n            \n              \n                Meilame Tayebjee\n                4 avr. 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Groupe de veille Codification Automatique n°10\n            \n\n            \n              Description : Présentation du package torchFastText et ses performances sur la codification de l'APE\n\n            \n\n            \n            \n              \n                Meilame Tayebjee\n                5 mars 2025\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Groupe de veille Codification Automatique n°9\n            \n\n            \n              Description : Travaux sur le passage de la nomenclature NAF2008 Rev2.1 à la NAF2025\n\n            \n\n            \n            \n              \n                Thomas Faria, Nathan Randriamanana\n                6 nov. 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            European Conference for Quality in Official Statistics 2024\n            \n\n            \n              Description : Retraining strategies for an economic activity classification model\n\n            \n\n            \n            \n              \n                Thomas Faria, Nathan Randriamanana, Tom Seimandi\n                24 mai 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Échange avec l'Inserm\n            \n\n            \n              Description : Codification automatique de l'APE à l'Insee\n\n            \n\n            \n            \n              \n                Thomas Faria et Nathan Randriamanana\n                7 mai 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Eurostat webinar\n            \n\n            \n              Description : Presentation given during the NACE implementation webinar\n\n            \n\n            \n            \n              \n                Nathan Randriamanana\n                30 avr. 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            MLOps exhange with StatCan\n            \n\n            \n              Description : Slides used for an exchange with StatCan on MLOps\n\n            \n\n            \n            \n              \n                Romain Avouac, Thomas Faria et Tom Seimandi\n                19 mars 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Formation BCEAO 2023\n            \n\n            \n              Description : Présentation pour la formation Big Data donné à la BCEAO à Dakar\n\n            \n\n            \n            \n              \n                Romain Avouac, Thomas Faria et Tom Seimandi\n                27 sept. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Conférence UNECE\n            \n\n            \n              Description : Presentation given during the UNECE Machine Learning for Official Statistics Workshop 2023\n\n            \n\n            \n            \n              \n                Thomas Faria and Tom Seimandi\n                5 juin 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Journée SSP Hub\n            \n\n            \n              Description : Présentation lors de l'évènement de lancement du SSP Hub\n\n            \n\n            \n            \n              \n                Thomas Faria et Tom Seimandi\n                17 avr. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Séminaire DMS\n            \n\n            \n              Description : Séminaire interne du département des méthodes statistiques de l'Insee\n\n            \n\n            \n            \n              \n                Thomas Faria et Tom Seimandi\n                1 sept. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n\n\nAucun article correspondant\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nDoremifasol\n\n\nLe package  R Doremifasol facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur…\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\npynsee, un package Python  pour récupérer les données de l’Insee\n\n\nLe package  pynsee facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur Github.\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\nUtiliser les images satellites pour améliorer le recensement de la population dans les territoire ultra-marins\n\n\n\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_alimentation/index.html",
    "href": "project/2022_alimentation/index.html",
    "title": "Quel effet de la qualité de la nourriture sur notre santé ?",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\nDétail du projet\nAu-delà des multiples dimensions largement documentées d’inégalités (revenu, patrimoine, éducation, logement, accès aux soins et aux services publics en général), des disparités dans la consommation de biens alimentaires sont également susceptibles d’être la source d’inégalités de santé, ainsi que des marqueurs sociaux et territoriaux. Les données de caisse pourraient offrir une description très riche de la consommation locale, à condition que les identifiants des produits permettent des enrichissement avec des sources externes, comme des informations nutritionnelles. Ce projet a pour but d’enrichir des données de la grande distribution avec des informations nutritionnelles extraites d’Open Food Facts, complétées par les données Ciqual de l’Anses.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nPour compenser l’appariement partiel via le code-barre, une méthode pour apparier efficacement des libellés courts est mise en place. Après une étape de prétraitement pour normaliser des libellés courts, des techniques d’appariement floue sont mises en place. Elles sont basées sur plusieurs tokenizers (y compris les n-grammes) en interrogeant un index personnalisé ElasticSearch et en validant les échos candidats avec une distance de Levenstein. Le pipeline est composé de plusieurs étapes relâchant successivement les contraintes pour trouver des candidats pertinents. Enfin, l’appariement final est évalué par une mesure de similarité basée sur un word embedding obtenu en entraînant un réseau siamois sur l’appariement exact via les code-barres. Les données mobilisées, portant sur la période 2015-2018, sont celles de plusieurs enseignes d’un même groupe de la grande distribution (relevanC).\n\n\nProduits et documentation du projet\n- Enrichissement de données de caisses à partir d’informations nutritionnelles : une approche par appariement flou sur données de grande dimension, Journées de méthodologie statistique 2022\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées CB\n\nInsee\n\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\ncodification automatique\n\nextraction de données\n\ndonnées de caisse\n\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\n\nwebscraping\n\nen production\n\ncodification automatique\n\nDARES\n\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\n\nPython\n\ncodification automatique\n\npackage\n\nen production\n\nMLFlow\n\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nModélisation de l’appartenance au parc des véhicules routiers et de son utilisation\n\n\n\nen production\n\nappariement\n\nSDES\n\ndonnées administratives\n\n\n\nAppariement de bases administratives et modélisation pour estimer le nombre de véhicules routiers en France\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\n\nInsee\n\nprévisions\n\ndonnées comptes bancaires\n\nexpérimentation\n\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparaison des méthodes d’appariement et apport du machine learning\n\n\n\nappariement\n\ndonnées administratives\n\nen production\n\n\n\nTester et comparer différentes méthodes d’appariements afin de dégager des recommandations pour les travaux nécessaires à la construction des répertoires, notamment dans le…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\n\nPython\n\ncodification automatique\n\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\n\nprévisions\n\nexpérimentation\n\nInsee\n\ndonnées CB\n\ndonnées de téléphonie mobile\n\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\n\nInsee\n\nprévisions\n\ndonnées privées\n\nexpérimentation\n\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\n\ndatavisualisation\n\nmachine learning\n\nInsee\n\nexpérimentation\n\nopen-data\n\ndonnées de téléphonie mobile\n\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\nPython\n\ncodification automatique\n\ndonnées de caisse\n\nCOICOP\n\nIPC\n\nen production\n\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\n\nen production\n\nInsee\n\ncodification automatique\n\nmachine learning\n\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\nInsee\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées administratives\n\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_Enquete_Budget_Famille/index.html",
    "href": "project/2022_Enquete_Budget_Famille/index.html",
    "title": "Travaux méthodologiques sur l’enquête Budget de Famille",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\n\nDétail du projet\nLa collecte de la prochaine enquête Budget de Famille en 2026 sera largement différente de celle de l’enquête précédente (2017).  - En effet, une partie des données de consommation des ménages sera remontée à travers une application mobile mise à disposition des ménages enquêtés.  - De plus, l’enregistrement en base de données des produits issus des données de consommation ne se fera plus par les enquêteurs mais par un prestataire de saisie ou d’extraction automatique.  Deux axes existent dans cette expérimentation :  - Le premier axe consiste à tester les solutions open-source d’extraction automatique du contenu de tickets de caisse, afin de pouvoir mieux dialoguer avec des partenaires comme Teklia et potentiellement pouvoir procéder à une partie des extractions en interne ;  - Le second axe (principal) est autour de la codification automatique des produits de consommation dans la nomenclature COICOP : entraînement d’un modèle de classification sur les données de l’enquête précédente et élaboration d’une stratégie de reprise optimale.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nProjet en cours\n\n\nCode du projet\n- https://git.lab.sspcloud.fr/ssplab/bdf  - https://git.lab.sspcloud.fr/ssplab/experimentation-bdf\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2021_webscraping_ipc_hotel/index.html",
    "href": "project/2021_webscraping_ipc_hotel/index.html",
    "title": "Indices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nWebscraping des prix de nuitées hôtelières pour construire l’indice des prix à la consommation\n\n\n\n\nDétail du projet\nL’indice des prix à la consommation pour les nuitées hôtelières est calculé à partir de relevés effectués sur le terrain par des enquêtrices et enquêteurs de l’Insee qui relèvent le prix d’une chambre pour une nuitée le jour même pour 2 personnes avec petit-déjeuner. Afin d’améliorer l’indice et de s’émanciper de certaines limites de la méthode actuelle, ce projet explore une méthode de collecte innovante, le webscraping depuis un site de réservation.  Après récolte des données en ligne, celles-ci sont brutes et nécessitent un nettoyage : par exemple la valeur pour une caractéristique n’est pas forcément décrite de la même manière entre deux observations. Pour palier les problèmes liés à un indice à panier fixe, l’indice final est construit à partir de classes homogènes. Enfin, les résultats de l’indice calculé avec les données de la plateforme de réservation en ligne avec l’indice publié.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLa nouvelle méthodologie de récolte des prix est maintenant en production.\n\n\nProduits et documentation du projet\n- Indices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne, Journées de méthodologie statistique 2022\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées CB\n\nInsee\n\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\n\nwebscraping\n\nen production\n\ndatavisualisation\n\nSIES\n\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\ncodification automatique\n\nextraction de données\n\ndonnées de caisse\n\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\n\nwebscraping\n\nen production\n\ncodification automatique\n\nDARES\n\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\n\nmachine learning\n\nInsee\n\nCOICOP\n\ndonnées de caisse\n\nappariement\n\nexpérimentation arrêtée\n\ncodification automatique\n\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\n\nwebscraping\n\ndatavisualisation\n\nopen-data\n\nen production\n\nSIES\n\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrévoir la croissance en lisant le journal\n\n\n\nInsee\n\nmachine learning\n\nexpérimentation\n\nprévisions\n\nwebscraping\n\n\n\nUtiliser les articles de presse en continu pour construire un indicateur aidant à prévoir la croissance\n\n\n\n\n\n\n1 mars 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\n\nprévisions\n\nexpérimentation\n\nInsee\n\ndonnées CB\n\ndonnées de téléphonie mobile\n\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\n\ndatavisualisation\n\nmachine learning\n\nInsee\n\nexpérimentation\n\nopen-data\n\ndonnées de téléphonie mobile\n\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation\n\n\n\nen production ??\n\nInsee\n\nIPC\n\nwebscraping\n\nforêt aléatoire\n\n\n\nCollecter sur le web les caractéristiques des produits pour améliorer la prise en compte des effets qualité dans l’indice des prix à la consommation\n\n\n\n\n\n\n1 juin 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\nPython\n\ncodification automatique\n\ndonnées de caisse\n\nCOICOP\n\nIPC\n\nen production\n\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\nInsee\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées administratives\n\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2021_gdp_media/index.html",
    "href": "project/2021_gdp_media/index.html",
    "title": "Prévoir la croissance en lisant le journal",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nConstruction d’un indicateur de sentiment économique à partir d’articles de presse pour prévoir la croissance économique\n\n\n\n\nDétail du projet\nL’information produite par les media est réactive, fournie et aborde de nombreux domaines économiques. En outre les techniques d’analyse textuelle (text mining) et de collecte automatisée des données en ligne (webscraping) permettent d’élaborer des indicateurs synthétisant cette richesse. Le but de l’expérimentation est d’estimer la plus-value d’un indicateur médiatique associé à l’activité économique par sa complémentarité avec les enquêtes de conjoncture existantes. Cette expérimentation complète les travaux pré-existants, qui cherchent à construire un indicateur aidant à prédire le PIB sur la base des archives du journal Le Monde, en mobilisant des sources journalistiques complémentaires et revoyant les modèles utilisés.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nUn indicateurs indicateur de sentiment médiatique conjoncturel a été construit à partir des tonalités des articles de presse.\n\n\nProduits et documentation du projet\n- Les messages issus des articles de presse peuvent contribuer à prédire l’activité économique en temps réel, Notes et points de conjoncture de l’Insee de l’année 2020 ;  - Éclairage - L’activité économique française au travers d’articles de presse, Note de conjoncture de l’Insee de mars 2021 ;  - Prédire l’activité économique à partir d’articles de presse, Journées de méthodologie statistique 2022\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne\n\n\nExploration des apports du webscraping pour mesurer le prix des nuitées hôtelières dans l’IPC\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation\n\n\nCollecter sur le web les caractéristiques des produits pour améliorer la prise en compte des effets qualité dans l’indice des prix à la consommation\n\n\n\n\n\n\n1 juin 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2021_comptes_bancaires_conj/index.html",
    "href": "project/2021_comptes_bancaires_conj/index.html",
    "title": "En 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\n\n\nDétail du projet\nLes données de comptes bancaires constituent une source d’information avancée sur la consommation et l’épargne des ménages en 2020, aussi bien à l’échelle microéconomique qu’infra-annuelle. À travers l’exploitation de données anonymisées mises à disposition par le Crédit Mutuel Alliance Fédérale, il est possible d’étudier comment la crise sanitaire a pu modifier la situation financière des ménages clients de cette banque, selon leur niveau de revenu, leur âge ou encore leur catégorie socio-professionnelle.  Les données bancaires utilisées ne permettent pas d’identifier directement les revenus des ménages mais peuvent tout de même être mobilisées pour en déduire une approximation, grâce à l’ensemble des virements et chèques entrant sur les comptes. Ces flux entrants chutent lors du premier confinement avant de rebondir en juin. Le deuxième confinement n’aurait pas provoqué de baisse de ces flux entrants, en moyenne.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nCette étude se situe dans le prolongement des travaux menés sur les ménages à partir des données de compte bancaire.  Pendant les deux confinements de 2020, tous les groupes de ménages étudiés, quels que soient leurs niveaux de revenus, auraient diminué leur consommation, laquelle s’est recentrée, en particulier en avril, aux biens de première nécessité. Les ménages qui consommaient le plus avant la crise, essentiellement des cadres et des hauts revenus, auraient donc davantage diminué leur consommation.  La chute de la consommation a provoqué un surcroît d’épargne qui a alimenté les comptes courants des ménages et leurs comptes sur livrets. Le patrimoine financier brut des ménages (épargne liquide, comptes-titres et assurances- vie, crédits exclus) aurait fortement augmenté en 2020. Cette hausse s’observe chez tous les groupes de ménages, quel que soit leur niveau de patrimoine financier. Elle est plus élevée en euros chez les ménages à hauts patrimoines financiers, qui ont pu épargner davantage en diminuant leur consommation. Les ménages à faibles patrimoines, ont également mis de l’argent de côté notamment pendant le premier confinement. Cependant, les montants en jeu pour ces ménages, quelques dizaines ou centaines d’euros en général, demeurent faibles bien qu’ils représentent une part importante de leur patrimoine initial. Parmi les ménages actifs, certains auraient été davantage touchés que d’autres par une baisse de leurs revenus et auraient donc moins augmenté leur épargne : c’est le cas des artisans et commerçants, ou encore des salariés du secteur privé, par contraste avec ceux du secteur public.\n\n\nProduits et documentation du projet\n- En 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires, Note de conjoncture de l’Insee - mars 2021\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées CB\n\nInsee\n\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\ncodification automatique\n\nextraction de données\n\ndonnées de caisse\n\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\n\nmachine learning\n\nInsee\n\nCOICOP\n\ndonnées de caisse\n\nappariement\n\nexpérimentation arrêtée\n\ncodification automatique\n\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\n\nprévisions\n\nexpérimentation\n\nInsee\n\ndonnées CB\n\ndonnées de téléphonie mobile\n\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\n\nInsee\n\nprévisions\n\ndonnées privées\n\nexpérimentation\n\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\n\ndatavisualisation\n\nmachine learning\n\nInsee\n\nexpérimentation\n\nopen-data\n\ndonnées de téléphonie mobile\n\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\nPython\n\ncodification automatique\n\ndonnées de caisse\n\nCOICOP\n\nIPC\n\nen production\n\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\nInsee\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées administratives\n\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant\n\n  \n\n\n\n\nAutres études menées grâces aux données bancaires\nPar ailleurs, d’autres études ont été menées par l’Insee en utilisant des données de compte bancaires. Elles sont disponibles sur le site de l’Insee :\n\n\n\n\n\n\n\n\n\n\nL’économie racontée par les données bancaires - Ce que nos relevés de comptes disent de nous\n\n\nCourrier des statistiques n°12, Insee, Décembre 2024\n\n\n\n1 déc. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchats transfrontaliers de carburant à la frontière franco-allemande\n\n\nDocuments de travail de l’Insee n°2024-08, mai 2024\n\n\n\n1 mai 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa situation financière des ménages au jour le jour\n\n\nInsee Analyses n°90, décembre 2023\n\n\n\n1 déc. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvec l’inflation, une précarité financière en légère hausse, mais inférieure en août 2022 à son niveau d’avant-crise sanitaire\n\n\nInsee Analyses n°76, octobre 2022\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nUne mesure de la réponse en consommation à des chocs de revenus à partir des données bancaires\n\n\nJournées de méthodologie statistique 2022\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact de la crise sanitaire sur un panel anonymisé de clients de La Banque Postale\n\n\nInsee Analyses n°69, novembre 2021\n\n\n\n1 nov. 2021\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2021_Extraction_CS/index.html",
    "href": "project/2021_Extraction_CS/index.html",
    "title": "Extraction automatique du tableau des filiales et participations des comptes sociaux des entreprises",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nExtraction automatique du tableau des filiales et participations des comptes sociaux des entreprises\n\n\n\n\nDétail du projet\nUne expérimentation autour de l’extraction automatisée du tableau des filiales et participations des comptes sociaux des entreprises a été démarrée en 2022 à la suite d’un stage dans la division PTGU. Ce tableau est exploité à la main au cours des opérations de profilage des entreprises, ce qui constitue un travail fastidieux. L’expérimentation a été menée avec la Banque de France qui est intéressé par le même cas d’usage que l’Insee, avec deux objectifs principaux :   - développer un prototype d’application permettant à des utilisateurs de récupérer un tableau des filiales et participations automatiquement pour un numéro Siren et une année donnés ;  - comparer les performances de différentes méthodes d’extraction automatique de tableaux, basées d’une part sur des outils open-source et d’autre part sur des solutions commerciales.\n\n\nActeurs\nInsee, Inpi, Banque de France\n\n\nRésultats du projet\n- Une API expérimentale et une interface expérimentale ont été mises en place pour répondre au besoin métier.\n\n\nProduits et documentation du projet\n- Le projet a été présenté le 27 juin 2024 à un séminaire interne Insee slides  - Extraction automatique de données issues d’images scannées : Une illustration par les comptes sociaux d’entreprises, Journées de méthodologie statistique 2022\n\n\nCode du projet\n- https://github.com/InseeFrLab/ca-document-querier/ : wrapper Python autour de l’API entreprises de l’Inpi  - https://github.com/InseeFrLab/extraction-comptes-sociaux : dépôt « core » où se trouvent les éléments pour la détection de page/extraction de tableaux  - https://github.com/InseeFrLab/extract-table-ui : dépôt de code de l’interface expérimentale  - https://github.com/InseeFrLab/extraction-comptes-sociaux-llm : code source de l’architecture globale, comprenant des microservices conteneurisés et orchestrés par Kubernetes. Il combine des appels à des API externes (INPI), le traitement de PDF, et l’utilisation de modèles de langage (LLM) pour l’analyse et l’extraction de données."
  },
  {
    "objectID": "project/2020_webscraping_ipc/index.html",
    "href": "project/2020_webscraping_ipc/index.html",
    "title": "Webscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation",
    "section": "",
    "text": "Webscraping du prix et des caractéristiques des ordinateurs portables, estimation de modèles hédoniques pour améliorer la qualité statistique de l’indice des prix à la consommation\n\n\n\n\nDétail du projet\nL’IPC mesure l’évolution « pure » des prix, à qualité constante. Il suit un certain nombre de produits identiques dans le temps. Lorsque ceux-ci disparaissent, ils sont remplacés par des produits qui peuvent ne pas être équivalents. Il convient alors de distinguer, dans l’évolution des prix des remplaçants par rapport aux produits remplacés, un effet qualité (différence de prix pour un mois fixé) d’un effet inflation.  Les méthodes « hédoniques » estiment cet effet qualité à partir de coefficients correspondant aux prix sous-jacents des différentes caractéristiques techniques du produit (par exemple la marque de l’ordinateur, la mémoire vive d’un ordinateur, le modèle et la fréquence du processeur, etc.). Le but du projet est de renforcer ces méthodes d’estimation de l’effet qualité par l’extension de la base de données et l’usage de méthode d’apprentissage statistique.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nL’étude a permis d’augmenter les échantillons utilisés (en termes de relevés de prix et de caractéristiques) par une collecte de données en ligne (webscraping) et de mettre en place une procédure de sélection automatique des caractéristiques expliquant les prix par apprentissage automatique (forêt aléatoire, régression de type Lasso).  In fine, 15 caractéristiques ont été retenues pour estimer l’effet qualité, comme par exemple la marque, la RAM, la capacité de stockage, la marque du processeur, sa fréquence, la résolution de l’écran … Les formulaires utilisés par les enquêteurs ont été amendés pour qu’ils relèvent désormais les caractéristiques pertinentes déterminant le prix des ordinateurs."
  },
  {
    "objectID": "project/2020_webscraping_ipc/index.html#en-lien-avec-le-webscraping-et-lipc",
    "href": "project/2020_webscraping_ipc/index.html#en-lien-avec-le-webscraping-et-lipc",
    "title": "Webscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation",
    "section": "En lien avec le webscraping et l’IPC",
    "text": "En lien avec le webscraping et l’IPC\nLa récolte de données en ligne (webscraping) n’est pas utilisée que dans le cadre de la production de l’inflation. Elle est aussi utilisée dans d’autres domaines et par d’autres entités que l’Insee au sein du service statistique public. L’Insee utilise par ailleurs depuis 2020 les données de caisse dans la définition de l’IPC, comme rappelé dans l’article Utiliser les données de caisses pour le calcul de l’indice des prix à la consommation du courrier des statistiques de 2019.\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne\n\n\nExploration des apports du webscraping pour mesurer le prix des nuitées hôtelières dans l’IPC\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrévoir la croissance en lisant le journal\n\n\nUtiliser les articles de presse en continu pour construire un indicateur aidant à prévoir la croissance\n\n\n\n\n\n\n1 mars 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2020_webscraping_ipc/index.html#en-lien-avec-lusage-de-nouvelles-sources-de-données",
    "href": "project/2020_webscraping_ipc/index.html#en-lien-avec-lusage-de-nouvelles-sources-de-données",
    "title": "Webscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation",
    "section": "En lien avec l’usage de nouvelles sources de données",
    "text": "En lien avec l’usage de nouvelles sources de données\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2020_webscraping_ipc/index.html#en-lien-avec-les-problématiques-de-classification-automatique",
    "href": "project/2020_webscraping_ipc/index.html#en-lien-avec-les-problématiques-de-classification-automatique",
    "title": "Webscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation",
    "section": "En lien avec les problématiques de classification automatique",
    "text": "En lien avec les problématiques de classification automatique\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2020_electricite_conj/index.html",
    "href": "project/2020_electricite_conj/index.html",
    "title": "Que disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\n\n\nDétail du projet\nLes données de production et de consommation d’électricité quotidiennes françaises sont une source utile pour suivre en temps réel les évolutions de l’activité des entreprises et des ménages. L’électricité étant l’une des formes d’énergie les plus utilisées dans le processus de production, les variations de sa production et de sa consommation reflètent les grandes évolutions du produit intérieur brut. La consommation d’électricité, en particulier, reflète la modification des comportements induite par la crise du coronavirus - qu’il s’agisse de la baisse de production dans des secteurs intensifs en électricité, comme les transports, ou du mode de vie transformé des ménages confinés.  Le climat et la saisonnalité affectant fortement la production et la consommation d’électricité, les comparaisons temporelles sont effectuées après correction des effets des variations de température, des jours ouvrés et des mois de l’année.  Selon les données de RTE (réseau de transport d’électricité), la consommation totale pendant la période du 23 mars au 26 avril 2020 est inférieure de 14 % à celle d’une période normale. De manière relativement cohérente avec l’ordre de grandeur des baisses d’activité estimées par ailleurs, la consommation des entreprises directement raccordées à RTE (pour la plupart de gros industriels) est inférieure de 24 %. Pendant la période du 23 mars au 3 avril, selon les données issues d’Enedis, la consommation hors résidentiel (entreprises – hors celles directement raccordées à RTE – et secteur public) est inférieure d’environ 27 % ; tandis que celle des ménages est supérieure d’environ 4 % à la normale.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLes données d’électricité ont été utilisées pour prédire l’activité économique en période de crise.\n\n\nProduits et documentation du projet\n- Que disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?, Note de conjoncture - décembre 2020\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées CB\n\nInsee\n\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\ncodification automatique\n\nextraction de données\n\ndonnées de caisse\n\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\n\nmachine learning\n\nInsee\n\nCOICOP\n\ndonnées de caisse\n\nappariement\n\nexpérimentation arrêtée\n\ncodification automatique\n\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\n\nInsee\n\nprévisions\n\ndonnées comptes bancaires\n\nexpérimentation\n\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\n\nprévisions\n\nexpérimentation\n\nInsee\n\ndonnées CB\n\ndonnées de téléphonie mobile\n\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\n\ndatavisualisation\n\nmachine learning\n\nInsee\n\nexpérimentation\n\nopen-data\n\ndonnées de téléphonie mobile\n\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\nPython\n\ncodification automatique\n\ndonnées de caisse\n\nCOICOP\n\nIPC\n\nen production\n\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\nInsee\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées administratives\n\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2020_cb_conj/index.html",
    "href": "project/2020_cb_conj/index.html",
    "title": "Utilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\n\n\nDétail du projet\nLa Note de conjoncture du 24 mars 2020, préparée par l’Insee pendant le premier trimestre 2020, n’aura jamais été publiée en l’état. La crise sanitaire et les mesures de limitation de propagation de la pandémie, en premier lieu le confinement, ont bouleversé les prédictions conjoncturelles standards et nécessité l’utilisation de sources alternatives plus réactives. Les données traditionnelles, disponibles pour la plupart mensuellement, n’étaient pas à même de fournir, dans les délais impartis, des informations fiables et pertinentes sur la situation économique : par exemple, les enquêtes de conjoncture les plus récentes auprès des entreprises, qui constituent une source privilégiée pour la prévision, avaient été collectées entre fin février et début mars principalement, donc avant l’annonce du premier confinement. Les premiers indicateurs quantitatifs sur mars, basés sur des enquêtes ou des sources administratives, n’ont été quant à eux disponibles qu’en avril ou mai. L’Insee a donc mobilisé d’autres sources de données, de nature diverse bien qu’essentiellement à haute fréquence, comme les données de caisse des grandes surfaces, le fret ferroviaire, les activations de réseaux de téléphonie mobile, la consommation électrique, l’utilisation d’internet, ou les données de transactions par carte bancaire CB.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nL’utilisation des données de transactions de paiement par carte bancaire CB agrégées et anonymisées pour la mesure du suivi de l’activité économique et de la consommation des ménages s’est révélée particulièrement appropriée et pertinente en 2020, lors de la mise en place des mesures de freinage de la propagation du Covid-19. Elles permettent de documenter en temps réel un choc exceptionnel pour lequel les outils de mesure standards apparaissaient tardifs ou parfois inadaptés. La richesse des informations conduit à des analyses particulièrement diverses. Toutefois, ces sources présentent des différences conceptuelles importantes avec les statistiques usuelles de suivi de l’activité économique. Ne pas les prendre en compte conduirait à des analyses faussées, en niveau et en évolution. Les données de téléphonie mobile ont aussi été utilisées pour suivre le retour de l’activité lors du déconfinement de juin 2020. La pertinence de ces sources privées pour les études publiques dépend donc grandement du degré de granularité de l’information disponible et de la capacité du producteur et de l’utilisateur à converger vers un cadre commun.\n\n\nProduits et documentation du projet\n- Que disent les données de transactions par carte bancaire sur les comportements de consommateurs « confinés » ?, Note et point de conjoncture de l’Insee - avril 2020  - Que disent les données de transactions par carte bancaire sur l’évolution récente de la consommation des ménages ?, Note et point de conjoncture de l’Insee - mai 2020  - Disparités territoriales de consommation : que disent les données de transaction par carte bancaire ?, Note et point de conjoncture de l’Insee - mai 2020  - Fin mai, les trajets matinaux n’atteignaient que 60 % de leur volume habituel, Note et point de conjoncture, juin 2020  - Apports, limites et perspectives des données de transactions carte bancaire (CB) dans le suivi de l’activité économique, Journées de méthodologie statistique 2022\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées CB\n\nInsee\n\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\ncodification automatique\n\nextraction de données\n\ndonnées de caisse\n\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\n\nmachine learning\n\nInsee\n\nCOICOP\n\ndonnées de caisse\n\nappariement\n\nexpérimentation arrêtée\n\ncodification automatique\n\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\n\nInsee\n\nprévisions\n\ndonnées comptes bancaires\n\nexpérimentation\n\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\n\nInsee\n\nprévisions\n\ndonnées privées\n\nexpérimentation\n\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\n\ndatavisualisation\n\nmachine learning\n\nInsee\n\nexpérimentation\n\nopen-data\n\ndonnées de téléphonie mobile\n\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\nPython\n\ncodification automatique\n\ndonnées de caisse\n\nCOICOP\n\nIPC\n\nen production\n\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\nInsee\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées administratives\n\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2019_classification_asso/index.html",
    "href": "project/2019_classification_asso/index.html",
    "title": "Codification automatique de l’activité des associations",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\n\n\nDétail du projet\nL’enjeu de l’expérimentation est d’attribuer un domaine d’activité aux associations pour améliorer le tirage de l’échantillon de l’enquête associations. En effet, une partie des associations est répertoriée dans Sirène (employeuses, subventionnées…) mais 50 % d’entre elles ont un code APE 9499Z ne permettant pas de déterminer leur domaine d’activité de façon précise. Par ailleurs, les associations loi 1901 sont enregistrées dans le répertoire national des associations (RNA), géré par le ministère de l’intérieur. Dans ce répertoire, un champ “objet social” rempli en clair décrit rapidement les activités de chaque association. L’expérimentation vise à analyser textuellement ce champ pour en prédire l’activité en 10 modalités.  En pratique, après prétraitements et exploration des données textuelles et de ses thématiques (allocation latente de Dirichlet), un dictionnaire des mots a été constitué, avec des variantes pour en réduire la dimension. La prédiction proprement dite s’est appuyée sur divers modèles d’apprentissage supervisé (forêts aléatoires, support vector machine, modèle linéaires généralisés pénalisés GLMnet et Extrem Gradient Boosting (XGBoost)), le jeu d’apprentissage (et les jeux de tests) étant fournis par la précédente enquête (2014) appariée avec le RNA. Le modèle XGboost a montré des performances plus élevés que les autres (avec une précision et un rappel de l’ordre de 69%). Le meilleur modèle s’obtient en combinant les différents modèles testés.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nL’échantillon de l’enquête Association a été tiré selon une stratification tirant profit de la prédiction du secteur d’activité par machine learning mené dans le cadre de cette expérimentation.\n\n\n\n\n\nProjets similaires liés à de la codification automatique\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2018_outlier_dsn/index.html",
    "href": "project/2018_outlier_dsn/index.html",
    "title": "Détecter et traiter les valeurs aberrantes ou manquantes, application à la Déclaration Sociale Nominative",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nUtilisation des méthodes de machine learning pour la détection et le traitement des valeurs aberrantes ou manquantes, application à la Déclaration Sociale Nominative (DSN)\n\n\n\n\nDétail du projet\nÀ l’occasion de la modernisation des processus internes à l’Insee suite à l’arrivée de la DSN, ce projet vise à repenser la détection des anomalies et le redressement des salaires. En effet, la gestion de nouvelles données mensuelles DSN incite à tester des méthodes d’apprentissage statistique (machine learning) pour détecter automatiquement les anomalies sur le triplet de variables (salaire brut, salaire net et nombre d’heures).\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLes travaux réalisés ont permis de comparer les caractéristiques des anomalies identifiées par trois méthodes issues de l’apprentissage statistique. Les trois algorithmes détectent en grande parte des anomalies différentes, en fonction de la manière dont ils définissent et identifient les erreurs présumées. L’utilisation combinée de plusieurs algorithmes de détection d’erreurs permettrait ainsi de couvrir un spectre plus large d’erreurs potentielles.  Les travaux ont par ailleurs permis d’approfondir la connaissance métier des bases utilisées et d’affiner les règles mises en place pour leur usage, permettant ainsi de renforcer la qualité des statistiques produites.\n\n\nProduits et documentation du projet\n- 5 324 euros de l’heure : outlier ou footballeur ? Méthodes d’apprentissage non supervisé pour la détection d’anomalies : application au cas de la Déclaration Sociale Nominative, Journées de méthodologie statistique 2018"
  },
  {
    "objectID": "infolettre/infolettre_22/index.html",
    "href": "infolettre/infolettre_22/index.html",
    "title": "La première infographie",
    "section": "",
    "text": "Bienvenue en 2026, l’année pendant laquelle les gens né(e)s en 2000 vont avoir 26 ans."
  },
  {
    "objectID": "infolettre/infolettre_22/index.html#présentation-de-cartographia",
    "href": "infolettre/infolettre_22/index.html#présentation-de-cartographia",
    "title": "La première infographie",
    "section": "Présentation de Cartographia",
    "text": "Présentation de Cartographia\nLe 13 janvier, nous avons reçu Françoise Bahoken et Nicolas Lambert. Ils nous ont présenté leur passionnant livre Cartographia, qui explore ludiquement les grandes questions de la cartographie. Le livre rappelle aussi que toute cartographie est une simplification de la réalité et qu’elle comprend ainsi forcément un parti pris, dont il faut être conscient. L’enjeu est alors de ne pas chercher à manipuler l’information à travers une cartographie et de tenter d’adopter le point de vue le plus neutre possible.\n\n\n\nComment des données identiques peuvent véhiculer un message totalement différent par les choix de visualisation effectués\n\n\nLe replay et leur présentation sont en ligne."
  },
  {
    "objectID": "infolettre/infolettre_22/index.html#journée-contribution-open-source-16-17-juin-2026---paris",
    "href": "infolettre/infolettre_22/index.html#journée-contribution-open-source-16-17-juin-2026---paris",
    "title": "La première infographie",
    "section": "Journée contribution open source 📅 16 & 17 juin 2026 - Paris",
    "text": "Journée contribution open source 📅 16 & 17 juin 2026 - Paris\nA vos agendas ! Le SSPLab organise une journée autour de la contribution open-source les 16 et 17 juin 2026. Cela aura lieu au Lieu de la transformation publique à Paris. Le but de cette journée est de démystifier l’open source, d’expliquer comment y contribuer, et d’encourager chacun à soutenir les projets que nous utilisons largement en datascience.\nLe programme est en cours d’élaboration donc si vous avez une idée de projets open-source d’intérêt auquel vous souhaitez contribuer, n’hésitez pas à nous contacter !"
  },
  {
    "objectID": "infolettre/infolettre_22/index.html#llm",
    "href": "infolettre/infolettre_22/index.html#llm",
    "title": "La première infographie",
    "section": "LLM",
    "text": "LLM\nDans la série agents, cet article de Posit compare différents modèles de LLM pour coder en R (sans les modèles de Mistral 😮🇫🇷😮). Résultat : les modèles les mieux évalués à date, mais attention les classements évoluent très vite, sont Claude Sonnet 4.5, Claude Opus 4.5 et OpenAI GPT-5."
  },
  {
    "objectID": "infolettre/infolettre_22/index.html#outils",
    "href": "infolettre/infolettre_22/index.html#outils",
    "title": "La première infographie",
    "section": "Outils",
    "text": "Outils\nDeux outils open-source pour synthétiser des données ou les valoriser.\n\nSi vous cherchez à générer des données synthétiques, il y a NeMo. Open-source, l’objectif de ce package Python est de permettre de générer des données synthétiques facilement, en se branchant au LLM de votre choix.\nCôté valorisation des données, il y a Marmot, un outil open-source pour parcourir plus facilement vos différents jeux de données.\nUn peu plus d’interactivité dans des cellules de code en Quarto ? Quarto-live permet de faire des cellules de code réactives dans une page HTML Quarto.\nDu côté des publications en ligne reproductibles et interactives, l’OFCE a publié depuis plusieurs années son package maison OFCE. Il permet de publier sur Internet les études de l’OFCE, en insérant des graphiques interactifs et en gérant aussi la charte graphique de l’OFCE."
  },
  {
    "objectID": "infolettre/infolettre_22/index.html#formation",
    "href": "infolettre/infolettre_22/index.html#formation",
    "title": "La première infographie",
    "section": "Formation",
    "text": "Formation\nQuelques ressources intéressantes de formation ou articles de blog.\n\nCôté Python, le célèbre livre Python datascience Handbook est disponible aussi en version web ;\nPour comprendre la manière dont fonctionnent les filtres Bloom dans la technologie Parquet, je vous recommande ce très utile post de blog d’Éric Mauvière."
  },
  {
    "objectID": "infolettre/infolettre_22/index.html#fun",
    "href": "infolettre/infolettre_22/index.html#fun",
    "title": "La première infographie",
    "section": "Fun",
    "text": "Fun\n\nUne excellente BD permet d’expliquer les statistiques facilement (aussi pour les enfants). Dans Les statistiques en BD, Laurence Dujourdy et Mathieu Bartoletti vulgarisent simplement les concepts que l’on a appris il y a un certain temps. Le tout est dessiné par Thibault Roy et est porté par l’institut Agro de Dijon.\nPour avoir une lecture sociale des évolutions de Paris, n’oubliez pas d’aller voir l’analyse Ce que la carte des kebabs révèle de Paris publiée dans Les Echos. A travers les kebabs, Jules Grandin analyse la géographie de Paris et son évolution. Il retrouve ainsi les grandes avenues, les gares, les quartiers densément peuplés et enfin les inégalités d’âge et de revenus."
  },
  {
    "objectID": "infolettre/infolettre_20/index.html",
    "href": "infolettre/infolettre_20/index.html",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "",
    "text": "Bienvenue à la vingtième infolettre ! Il est temps de se mettre au défi de respecter ce qui a été dit il y a quelques semaines : une newsletter par mois ga-ran-tie je vous ai dit. Bon, à l’époque, je ne savais pas qu’un mois ça voulait dire 4 semaines et que 4, c’est pas beaucoup 🙃. Allez, on y va !\nBonne lecture 📔 !"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#la-troisième-journée-du-réseau-1-décembre---la-tréso-malakoff",
    "href": "infolettre/infolettre_20/index.html#la-troisième-journée-du-réseau-1-décembre---la-tréso-malakoff",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "La troisième journée du réseau 📅 1 décembre - La Tréso (Malakoff)",
    "text": "La troisième journée du réseau 📅 1 décembre - La Tréso (Malakoff)\nLes inscriptions pour la troisième journée du réseau le 1er décembre 2025 sont ouvertes. Si vous souhaitez présenter un sujet, n’hésitez pas à me contacter !"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#présentation-de-cartographia---13-janvier-2026---format-mixte-montrouge-et-en-ligne",
    "href": "infolettre/infolettre_20/index.html#présentation-de-cartographia---13-janvier-2026---format-mixte-montrouge-et-en-ligne",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "Présentation de Cartographia - 📅 13 janvier 2026 - format mixte (Montrouge et en ligne)",
    "text": "Présentation de Cartographia - 📅 13 janvier 2026 - format mixte (Montrouge et en ligne)\nFrançoise Bahoken et Nicolas Lambert vont venir nous parler de leur livre Cartographia et des questions de cartographie (!) passionnantes qu’ils y abordent. Cela se passera le 13 janvier 2026 en début d’après-midi, en format mixte : présentiel (à la DG de l’Insee) et à distance. Nicolas Lambert était déjà intervenu pour nous présenter Observable, une librairie Javascript très pratique pour faire des dataviz (ici, pour rappel). Si cela vous intéresse, réservez donc votre début d’après-midi !"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#datavisualisation",
    "href": "infolettre/infolettre_20/index.html#datavisualisation",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "Datavisualisation",
    "text": "Datavisualisation\n\nUne belle carte de migration simulée au niveau infra-national en zone endémique paludéenne africaine. Françoise Bahoken, son autrice, sera notre invitée début 2026.\n\nPour les passionnés d’espace, voici une carte en 3D et dynamique du système solaire, comprenant les planètes mais aussi les astéroïdes! Vous pouvez jouer avec le temps pour savoir où sera la Terre quand vous aurez 30/40/50/60/70 ans (spoiler alert : c’est cyclique), et vérifier aussi qu’aucun astéroïde ne passe sur la Terre d’ici là.\nPour les autres candidats à l’infographie du mois :\n\npour voir l’ensemble des livres publiés en une seule carte interactive, cela donne cela.\nla cheffe de l’infographie d’Epsiloon (magazine de sciences) a publié les étapes avant / après de ses infographies. Je vous recommande celle sur les habitudes de sommeil par pays.\n\nEnfin, un petit jeu à la SUTOM en version graphique : un graphique par jour, devinez le pays !"
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#cartographie",
    "href": "infolettre/infolettre_20/index.html#cartographie",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "Cartographie",
    "text": "Cartographie\n\nDes chercheurs de l’Université de Charles (en Tchéquie) et de Freiburg ont publié une (très belle) taxonomie des bâtiments urbains, disponible en ligne. Tout est ici et vous pouvez explorer le bâti urbain de six pays d’Europe centrale et de l’est.\n\nL’IGN a publié une cartographie très détaillée des risques d’inondation et de submersion sur tout le territoire national, notamment grâce aux images LIDAR. Par exemple, vous habitez à Saint-Maur-des-Fossés? Vous pouvez y voir la simulation d’une inondation majeure chez vous.\n\nUtiliser l’IA pour faire des fonds de carte à partir d’image satellite ? Cela semble bien fonctionner d’après ce post.\nA partir des données d’utilisateurs de Facebook, des chercheurs ont bâti des indicateurs mensuels de flux migratoires couvrant 181 pays.\nAprès la prévisualisation de données Parquet en ligne, cette fois ce site permet de visualiser très facilement des fichiers de données géographiques types STAC (dont Geoparquet). Explications sur la plomberie ici.\nLa communauté Apache Sedona publie SedonaDB, un moteur de base de données analytique open source et pensé nativement aussi pour des données spatiales."
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#le-reste-cest-le-r",
    "href": "infolettre/infolettre_20/index.html#le-reste-cest-le-r",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "Le reste c’est le R",
    "text": "Le reste c’est le R\n\nLe package R tinytable, permet de faire des tableaux de qualité en de multiples formats. L’ambition de ce package est d’être simple, léger (0 dépendance à des packages externes), flexible et de différencier les données de l’affichage.\nLe package R redoc devrait être mis à jour. Il permet notamment de faire le lien entre Quarto, et la suite Office, par exemple si vous générez des documents qui seront relus par des personnes utilisant Word comme outil de travail 😉.\n\nEn machine learning, quels sont les problèmes posés par des classes d’apprentissage de tailles très différentes ? Selon cet article, il s’agit surtout 1/ d’avoir des métriques de performance adéquates, 2/ d’avoir un nombre absolu d’observations dans la classe minoritaire suffisant (et non une part dans le total) et enfin 3/ de faire attention à la fongibilité entre classes."
  },
  {
    "objectID": "infolettre/infolettre_20/index.html#ia",
    "href": "infolettre/infolettre_20/index.html#ia",
    "title": "De belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA",
    "section": "IA",
    "text": "IA\n\nQu’est-ce que les paramètres des modèles publiés disent des données d’entraînement sous-jacentes, confidentielles ? Des chercheurs sont allés fouiller ce que cache GPT-5.\nDu côté de l’entraînement des modèles, la gamme des données disponibles s’enrichit pour que ses données soient mieux reprises par l’IA. C’est un nouvel épisode de la course à ne pas finir page 15 des résultats Google ou page 259 des blogs Myspace.\n\nWikidata, qui stocke les données structurées de Wikipédia et consorts, propose maintenant ses données sous format de données vectorielles.\nUn programme d’harmonisation des metadata et d’API, l’Open Semantic Interchange Initiative, a été lancé pour enrichir les données d’apprentissage et la précision des IA. L’idée est d’avoir un langage YAML commun pour permettre aux IA d’échanger des données de manière robuste, par exemple par API et sans perte de sens au fur et à mesure de leur traitement par des agents d’IA.\nUne fois que l’on a les données, on cherche ensuite à savoir de combien de GPU j’ai besoin pour entraîner mon modèle. La Dinum a développé un outil pour estimer ses besoins pour cette étape.\n\nDes faiblesses des modèles de LLM sont, comme toujours, remontées, notamment sur l’importance des données d’apprentissage et d’un prompt propre. Cela met notamment en valeur le fait que les modèles ne sont pour le moment pas très efficaces quand il s’agit de trier des CV ou des articles.\n\nLe “prompt injection” : c’est l’idée de truquer l’IA dans son CV/projet d’article scientifique/sur Linkedin, par exemple avec des instructions écrites en « blanc sur blanc », pour avoir plus de chance d’être sélectionné/de détecter les messages écrits par l’IA. Cela marche à presque 100 % des cas selon cet article, d’autant plus que l’IA a un biais positif naturel.\nUne étude confirme que l’IA est particulièrement sensible à la qualité de la donnée d’apprentissage. Ainsi, 250 observations fausses suffisent pour empoisonner durablement un LLM, et ce quel que soit sa taille. Une présentation intéressante à une conférence PyData présentait par ailleurs ce qu’il se passait quand on entraîne une IA sur des images générés par une IA, avec des résultats similaires.\n\n\nLe site AI Darwin Awards rassemble des exemples d’usage raté d’IA. Globalement, les cas d’usage où l’IA est mis directement face à un client peuvent rapidement mal tourner… Peut-être que Diella, la première et seule ministre à être une IA dans le monde, rejoindra cette liste ?\nSi vous voulez comparer le résultat de deux LLM et leur empreinte carbone, n’oubliez pas l’outil ComparIA créé par une start-up d’État du ministère de la Culture."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html",
    "href": "infolettre/infolettre_18/index.html",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "",
    "text": "Ce mois-ci, la première partie de la newsletter est consacrée à l’actualité dense dans le domaine des IA génératives et à l’annonce d’un nouveau générateur de site web pour les data scientists. Suivent les actualités du réseau, notamment une présentation de Quarto par Christophe Dervieux (Posit) et le replay de la présentation d’Eric Mauvière sur le sujet des bonnes pratiques de dataviz."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#sora-la-nouvelle-ia-dopenai-pour-générer-des-vidéos",
    "href": "infolettre/infolettre_18/index.html#sora-la-nouvelle-ia-dopenai-pour-générer-des-vidéos",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Sora, la nouvelle IA d’OpenAI pour générer des vidéos",
    "text": "Sora, la nouvelle IA d’OpenAI pour générer des vidéos\n\n Source : New York Times d’après OpenAI.\n\n\nInstruction utilisée par OpenAI pour générer cette vidéo\n\n“Animated scene features a close-up of a short fluffy monster kneeling beside a melting red candle. The art style is 3D and realistic, with a focus on lighting and texture. The mood of the painting is one of wonder and curiosity, as the monster gazes at the flame with wide eyes and open mouth. Its pose and expression convey a sense of innocence and playfulness, as if it is exploring the world around it for the first time. The use of warm colors and dramatic lighting further enhances the cozy atmosphere of the image.”\n\n\nAprès avoir révolutionné le champ de la génération d’image avec DallE (texte \\(\\to\\) image), de la génération de textes avec ChatGPT (texte \\(\\to\\) texte), OpenAI a rendu public les premières productions d’un modèle de génération de vidéos à partir d’instructions (texte \\(\\to\\) vidéo). Ce produit, nommé Sora, génère des vidéos d’un réalisme qui n’avait encore jamais été atteint par les IA génératrices de vidéos. Jusqu’à présent, les modèles de ce type généraient des images dont les formes étaient grossières, la résolution d’une qualité faible et dont les mouvements étaient peu vraisemblables.\n\n   \nSource : Le Monde\n\nSora n’est pas directement mis à disposition du grand public, contrairement aux autres services d’OpenAI. Ce produit n’est partagé qu’à des utilisateurs identifiés par OpenAI comme pouvant représenter le public cible - des réalisateurs par exemple - ou ayant une expertise sur des sujets comme la désinformation, les biais, la connaissance des algorithmes de recommandation, etc. Cette diffusion restreinte vise à recevoir des retours de la part de potentiels clients ou d’experts sur les risques de ces technologies. La communication par le biais de quelques vidéos choisies par OpenAI permet, dans le même temps, de créer une attente du grand public avant la mise à disposition plus large.\nComme Dall-E, Midjourney et consorts qui généraient des mains avec trop de doigts, le réseau de neurones derrière Sora a encore des difficultés à respecter certaines règles élémentaires de vraisemblance. Par exemple, dans la vidéo ci-dessous, les événements liés à un bris de verre s’enchaînent dans un ordre incohérent.\n Source : OpenAI\nOpenAI a déjà prévu de nombreuses applications à ce modèle. Outre la génération de vidéos à partir d’instructions verbales, Sora est capable d’animer une image, de compléter une vidéo déjà existante avec une vidéo fictionnelle, d’éditer une vidéo déjà existante pour changer des éléments… Les secteurs de la communication, de la création et de la diffusion de contenu sont concernés au premier chef mais la richesse des fonctionnalités possibles et la simplicité d’usage des produits d’OpenAI laissent penser que les applications iront bien au-delà de ces secteurs économiques ; la vidéo occupe maintenant une place prédominante sur internet et sur les réseaux sociaux pour de multiples usages.\nCe modèle soulève, comme Dall-E ou ChatGPT avant lui, des enjeux de propriété intellectuelle puisqu’il a aussi été entraîné sur des corpus massifs collectés depuis internet. Le réalisme des vidéos générées peut également laisser craindre, sans marque d’identification claire du fait que la vidéo est générée numériquement (principe du watermark), des dérives autour de la mésinformation, notamment des vidéos malveillantes et réalistes de personnes dans des situations inventées (des deepfakes) ou la prolifération de contenus choquants si les garde-fous dans la génération de contenus sont outrepassés.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLa présentation de Sora sur le site d’OpenAI ;\nUn article plus technique d’OpenAI sur les fonctionnalités de Sora ;\nLes 10mn de vidéos de présentation de Sora par OpenAI ;\nUn article du New York Times présentant Sora\nUn article sur le site The Conversation sur les enjeux pour certains secteurs économiques."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#le-chat-un-concurrent-à-chatgpt-par-mistral-ai",
    "href": "infolettre/infolettre_18/index.html#le-chat-un-concurrent-à-chatgpt-par-mistral-ai",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "“Le Chat” : un concurrent à ChatGPT par Mistral AI 🐱",
    "text": "“Le Chat” : un concurrent à ChatGPT par Mistral AI 🐱\nFin février, la startup française Mistral AI a rendu public, en accès libre, une IA conversationnelle aux fonctionnalités similaires à ChatGPT nommée “Le Chat”.\nCe service utilise le grand modèle de langage (LLM) Mistral Large, dernier né des LLM multilangues entraînés par Mistral AI. Contrairement à d’autres modèles de Mistral AI, celui-ci n’est pas ouvert ; l’accès n’y est possible que par le biais des services de Mistral ou par le biais du cloud Microsoft Azure, suite à un partenariat entre l’entreprise américaine et la startup française (tarification en fonction du volume de requêtes).\nSelon les évaluations réalisées fin février, avant la sortie de Claude 3 (voir plus bas 👇️), ce modèle présentait des performances supérieures à celles des modèles open source, notamment LLaMa-2, sur une série d’évaluations de la véracité des réponses proposées par une IA et sur les capacités de raisonnement de celle-ci à partir de tests standardisés. Sur des questions d’un niveau de premier cycle universitaire (métrique MMLU proposée par Hendrycks et al. (2021)), Mistral Large propose la bonne réponse dans 81% des cas, ce qui l’amène presque au niveau de GPT-4 (86%) et bien au-dessus de Llama-2 (70%), le meilleur modèle opensource à l’heure actuelle.\n\n\nClassement des principaux modèles de langage lors de la sortie de Mistral Large\n\n\n\n\nPerformance des principaux LLM sur la métrique MMLU, une série de 57 tests sur la fiabilité des réponses et les capacités de raisonnement des IA conversationnelles. Source : Mistral AI, fin février\n\n\n\n\n\n\nExemple de questions de niveau licence posées pour évaluer la qualité d’un modèle selon la métrique MMLU proposée par Hendrycks et al. (2021) (accéder à l’article de recherche)\n\n\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nhttps://chat.mistral.ai/, l’IA conversationnelle proposée par Mistral AI ;\nLe post de blog par Mistral AI annonçant Mistral Large ;\nLa newsletter d’Andrew Ng consacrée à Mistral Large ;\nL’article d’Hendrycks et al. (2021) à l’origine de la métrique MMLU utilisée pour classer les modèles."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#les-performances-de-gpt-4-dépassées-pour-la-première-fois",
    "href": "infolettre/infolettre_18/index.html#les-performances-de-gpt-4-dépassées-pour-la-première-fois",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Les performances de GPT-4 dépassées pour la première fois",
    "text": "Les performances de GPT-4 dépassées pour la première fois\nQuelques jours seulement après la sortie de Mistral Large, un autre modèle de langage est venu concurrencer le modèle d’OpenAI GPT-4. Ce modèle nommé Claude 3 est le premier à obtenir des performances supérieures à GPT-4 (le modèle derrière la version Pro de ChatGPT) sur les principaux tests de qualité des modèles. Ce modèle, créé par Anthropic et disponible en trois versions plus ou moins puissantes (Haiku, Sonnet et Opus), n’est pas encore disponible pour les utilisateurs résidant dans l’Union Européenne.\n\n\nLes trois modèles Claude-3 disponibles\n\n\n\n\nSource : Anthropic\n\n\n\n\n\nComparaison des performances des LLM\n\n \n\nLes modèles Claude sont développés par l’entreprise Anthropic, créée par des anciens employés d’OpenAI considérant que la problématique de la sécurité des IA n’était pas assez mise en avant par OpenAI. Valorisée autour de 18 milliards d’euros en ce début d’année 2024, elle a bénéficié de financements importants d’Amazon et de Google, ces deux entreprises ayant investi respectivement 4 et 2 milliards de dollars. Les modèles Claude sont disponibles pour les utilisateurs des cloud d’Amazon (AWS) ou de Google (GCP) à l’instar des modèles GPT disponibles aux utilisateurs du cloud de Microsoft (Azure). La concurrence entre OpenAI et Anthropic est ainsi l’occasion d’un affrontement entre les trois principaux acteurs du cloud. Au-delà de la concurrence entre leurs investisseurs, les modèles économiques d’Anthropic et d’OpenAI diffèrent. Anthropic vise plutôt à proposer des services à des entreprises accessibles par le biais d’API là où OpenAI propose plutôt des outils grands publics avec des fonctionnalités supplémentaires pour les acteurs spécialisés. Parmi les partenaires principaux d’Anthropic, on retrouve Gitlab, Quora ou Salesforce (l’éditeur de logiciel derrière Slack). A l’instar des modèles Mistral Large ou GPT-4, le modèle Claude 3 n’est pas open source.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nL’annonce de Claude 3 par Anthropic ;\nUn article sur Anthropic par le New York Times et un autre par Forbes."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#observable-propose-un-constructeur-de-sites-statiques-pour-sabstraire-des-notebooks",
    "href": "infolettre/infolettre_18/index.html#observable-propose-un-constructeur-de-sites-statiques-pour-sabstraire-des-notebooks",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Observable propose un constructeur de sites statiques, pour s’abstraire des notebooks",
    "text": "Observable propose un constructeur de sites statiques, pour s’abstraire des notebooks\nAfin de démocratiser l’utilisation de Javascript au-delà du cercle des développeurs web, Mike Bostock, ancien responsable des dataviz du New York Times, la référence en la matière, a créé il y a quelques années Observable.\nEn plus d’être une extension du langage Javascript à la grammaire familière aux connaisseurs de Python et R, Observable vise à créer une communauté d’utilisateurs de Javascript à l’interface entre data scientists et développeurs web. Pour cela, le site observablehq.com se propose d’être un réseau social de notebooks en Javascript, un peu comme Github faisant office de réseau social du code. Les notebooks Observable permettent de rapidement prendre en main du code Javascript pour créer des analyses de données interactives qui peuvent ensuite être facilement partagées par le biais du site observablehq.com pour simplifier les réutilisations du code proposé ou des données sous-jacentes.\nCependant, si les notebooks sont un terrain fertile pour l’expérimentation, ils montrent rapidement leurs limites dès qu’on désire s’abstraire de l’hébergement sur observablehq.com. Pour mettre à disposition des visualisations interactives sur d’autres sites, les sites statiques sont plus simples d’usage. Historiquement, l’écosystème Javascript est construit autour d’imposants frameworks comme React, bien connus des développeurs web mais méconnus des data scientists qui sont néanmoins amenés à livrer de plus en plus d’applications interactives pour valoriser des données.\nL’annonce d’Observable Framework, un constructeur de sites statiques, représente un changement d’approche. Observable Framework vise à être un framework permettant aux data scientists de construire des sites web en mélangeant des étapes de préparation de données en R, Python ou SQL (via DuckDB), du formattage de texte en Markdown et de l’interactivité grâce au langage Observable. L’approche est ainsi similaire à celle de Quarto, la référence pour les data scientists désirant construire des publications reproductibles (voir la section événements 👇️ pour en apprendre plus). Ce dernier écosystème permet déjà depuis quelques temps de compléter du travail de données en R ou Python avec des traitements en Observable pour obtenir un site web interactif sans besoin de solutions techniques complexes comme Shiny ou Streamlit.\nLes évolutions à venir d’Observable Framework sont donc à surveiller, cet écosystème pouvant être amené, s’il rencontre du succès, à rentrer dans la boîte à outil standard des data scientists comme Quarto est déjà en train de le faire. Le site observablehq.com ne va pas pour autant disparaître : celui-ci restera un lieu où on peut tirer avantage de la simplicité des notebooks pour l’expérimentation ou pour la mise à disposition de tutoriels pédagogiques. Ce virage est similaire à celui pris par Python dans la communauté des data scientists où les notebooks, après avoir connu une phase hégémonique, sont revenus à leur fonction initiale : des carnets pour expérimenter servant de brouillon avant l’écriture de scripts ou alors de belles pages, mêlant texte et code, pour présenter une démarche de manière pédagogique.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nL’annonce d’Observable Framework ;\nL’interactivité dans Quarto grâce aux cellules Observable ;\nLe cours de “Mise en production de projets data science” de l’ENSAE où les enjeux techniques et humains de la mise à disposition de tels sites sont évoqués."
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#chistophe-dervieux-quarto-une-évolution-de-r-markdown-pour-des-travaux-statistiques-reproductibles-2-mai",
    "href": "infolettre/infolettre_18/index.html#chistophe-dervieux-quarto-une-évolution-de-r-markdown-pour-des-travaux-statistiques-reproductibles-2-mai",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Chistophe Dervieux, “Quarto : Une évolution de R Markdown pour des travaux statistiques reproductibles” (📅 2 mai)",
    "text": "Chistophe Dervieux, “Quarto : Une évolution de R Markdown pour des travaux statistiques reproductibles” (📅 2 mai)\nPour fiabiliser la production de documents construits en valorisant des données (tableaux, graphiques, etc.), RStudio (devenu Posit depuis) a construit il y a quelques années l’écosystème R Markdown permettant de produire du document en mélangeant code et texte.\nCette problématique des publications reproductibles est devenue incontournable dans l’écosystème R et la solution R Markdown est dorénavant largement utilisée. Pour étendre les vertus de cette approche à d’autres langages, Posit a commencé à développer Quarto, un écosystème reprenant le principe de R Markdown mais étendant ces fonctionnalités à d’autres langages de programmation, notamment Python et Observable.\nLe 2 mai de 15h à 16h, Christophe Dervieux (Posit) nous présentera Quarto, l’écosystème de publications reproductibles qui succède à R Markdown. Cet événement est proposé de manière hybride : par le biais de Zoom ou, pour les agents en poste à la Direction Générale de l’Insee, en salle 4-C-458.\n\n👉️ Ajouter cet événement à votre agenda Outlook\n👉️ Lien zoom"
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#vos-besoins-de-formation",
    "href": "infolettre/infolettre_18/index.html#vos-besoins-de-formation",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Vos besoins de formation",
    "text": "Vos besoins de formation\nL’an dernier, nous avions organisé un questionnaire pour connaître les besoins de formations des membres du réseau. Ce questionnaire est utile pour que les événements organisés dans le cadre du réseau répondent au mieux aux besoins.\nAfin de connaître les attentes et centres d’intérêt en cette année 2024, nous vous proposons un nouveau questionnaire. Celui-ci est également l’occasion d’accueillir vos retours sur les masterclass menées en 2023 en collaboration avec Datascientest si vous avez participé à celles-ci.\n\n👉️ Questionnaire"
  },
  {
    "objectID": "infolettre/infolettre_18/index.html#replay-de-la-présentation-deric-mauvière-la-dataviz-pour-donner-du-sens-aux-données-et-communiquer-un-message",
    "href": "infolettre/infolettre_18/index.html#replay-de-la-présentation-deric-mauvière-la-dataviz-pour-donner-du-sens-aux-données-et-communiquer-un-message",
    "title": "Sora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks",
    "section": "Replay de la présentation d’Eric Mauvière “La dataviz pour donner du sens aux données et communiquer un message”",
    "text": "Replay de la présentation d’Eric Mauvière “La dataviz pour donner du sens aux données et communiquer un message”\n\nLa présentation d’Eric Mauvière sur les bonnes pratiques de dataviz a rencontré un réel succès avec près de 150 participants. Le replay et les slides de cette présentation essentielle sont disponibles ci-dessous :\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-02-09-mauviere/conf_ssphub_item7-1.pdf\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Télécharger les slides\n  &lt;/a&gt;\n&lt;/p&gt;`"
  },
  {
    "objectID": "infolettre/infolettre_16/index.html",
    "href": "infolettre/infolettre_16/index.html",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "",
    "text": "Noël approchant, avant d’ouvrir vos cadeaux, de dévorer une bûche ou tout autre met délicieux, nous vous proposons de nous retourner sur la progression de l’audience du réseau durant l’année 2023. Cette newsletter commencera ainsi par une rétrospective du réseau, en écho à celle de l’année 2022. Une partie consacrée aux actualités de la data science suivra."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#rétrospective-du-réseau-en-2023",
    "href": "infolettre/infolettre_16/index.html#rétrospective-du-réseau-en-2023",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "Rétrospective du réseau en 2023",
    "text": "Rétrospective du réseau en 2023\n\nUne audience en progression\n\nhtml`&lt;div&gt;&lt;span class = \"underline-big\"&gt;${start_count}&lt;/span&gt; personnes faisaient partie de la liste de diffusion en début d'année 2023.&lt;/div&gt;&lt;br&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${plot_bar_participants}&lt;/div&gt;&lt;br&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${message}&lt;/div&gt;&lt;br&gt;`\n\n\n\n\n\n\n\n\n\n\nDécouvrir le code\n\nVoir plus bas la définition des objets Javascript 👇️\nhtml`&lt;div&gt;&lt;span class = \"underline-big\"&gt;${start_count}&lt;/span&gt; personnes faisaient parti de la liste de diffusion en début d'année.&lt;/div&gt;&lt;br&gt;`\nhtml`&lt;div&gt;${plot_bar_participants}&lt;/div&gt;&lt;br&gt;`\nhtml`&lt;div&gt;${message}&lt;/div&gt;`\n\nA l’exception du mois d’août (pause estivale oblige), la progression de l’audience a été assez régulière grâce aux événements et contenus publiés sur le site du SSP Hub.\n\nhtml`&lt;div&gt;Pendant l'année 2023, le réseau a ainsi connu &lt;span class=\"underline-big\"&gt;${events.length}&lt;/span&gt; événements et publications de contenu (${countEvents(\"Infolettre\")}, ${countEvents(\"Post de blog\")}, ${countEvents(\"Evénement virtuel ou présentiel\")}, ${countEvents(\"Masterclass\")}).&lt;/div&gt;&lt;br&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${lineplot}&lt;/div&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${warm_strip}&lt;/div&gt;`\n\n\n\n\n\n\n\nmd`__Choisir les événements du réseau à afficher 👇️__`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${viewof events_chosen_figure1}&lt;/div&gt;`\n\n\n\n\n\n\n\nhtml`&lt;div&gt;${table_events}&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\nCode pour générer les différents blocs de cette figure\n\n\n\nCode pour générer la figure principale\n\n\n// Voir plus bas 👇️ les arrays utilisés\n// Animation faite avec le Scrubber ci-dessous\nlineplot = Plot.plot({\n    y: {\n        grid: true,\n        label: \"Nombre d'inscrits\"\n    },\n    x: {\n        label: \"Date\",\n        domain: [new Date(\"2023-01-10\"), new Date(\"2023-12-09\")]\n    },\n    color: {\n        range: Object.values(color_mapping_events),\n        domain: Object.keys(color_mapping_events),\n        label: \"Type d'événement\"\n    }, \n    marginLeft: 50,\n    marks: [\n        Plot.line(\n          serie_contacts, {\n            x: \"date\", y: \"mail\",\n            stroke: \"#6886bb\",\n            tip: \"xy\"\n            }),\n1        Plot.crosshairX(serie_contacts_complete, {\n            x: (d) =&gt; new Date(d.date), y: \"mail\", stroke: \"red\"\n            }),\n        Plot.dot(\n          serie_contacts_complete,\n2          Plot.pointerX({x: (d) =&gt; new Date(d.date), y: \"mail\", stroke: \"red\"})),\n        Plot.dot(serie_contacts, {\n            x: \"date\", y: \"mail\",\n            stroke: \"#6886bb\",\n            fill: \"#6886bb\",\n            title: \"Effectif\"\n            }),\n        Plot.tickX(events_data_figure1_b, {\n            x: (d) =&gt; new Date(d.date), text: html``,\n            stroke: \"type\",\n            opacity: 0.1,\n            color: \"x\",\n            tip: true\n            }),            \n        Plot.axisX(events_data_figure1_b, {\n            x: (d) =&gt; new Date(d.date),\n            text: \"\",\n            color: \"type\"\n            }),\n    ]\n})\n\n\n1\n\nElément de réactivité lorsque la souris passe sur la figure.\n\n2\n\nElément de réactivité lorsque la souris passe sur la figure.\n\n\n\n\n\n\n\n\n\n\n\n\nCode pour générer la bande sous la figure principale\n\n\n1warm_strip = Plot.plot({\n  height: 40,\n  marginLeft: 50,\n  color: {\n    scheme: \"ylorrd\",\n    },\n  marks: [\n    Plot.crosshair(serie_contacts_complete, {\n      x: (d) =&gt; new Date(d.date),\n      strokeOpacity: 0.2,\n      fill: \"mail\",\n      interval: d3.utcDay.every(3),\n      inset: 0 // no gaps\n    }),\n    Plot.barX(serie_contacts_complete, {\n      x: (d) =&gt; new Date(d.date),\n      strokeOpacity: 0.2,\n      fill: \"mail\",\n      interval: d3.utcDay.every(3),\n      inset: 0 // no gaps\n    })\n  ]\n})\n\n\n1\n\nInspiration : https://observablehq.com/@observablehq/plot-warming-stripes\n\n\n\n\n\n\n\n\n\n\n\n\nCode pour générer le sélecteur d’événements\n\n\nfunction underline_event(x){\n    const x_underlined = `&lt;span style=\"text-transform: capitalize; border-bottom: solid 4px ${color_mapping_events[x]}; margin-bottom: -2px;\"&gt;${x}&lt;/span&gt;` ;\n    return x_underlined\n}\n\nviewof events_chosen_figure1 = Inputs.checkbox(\n    unique(events.map(d =&gt; d.type)),\n    {\n        value: unique(events.map(d =&gt; d.type)),\n        format: x =&gt; html`${underline_event(x)}`\n    }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode pour générer la table interactive\n\n\ntable_events = Inputs.table(\n    events_data_figure1_b,\n    {\n        columns: [\"date\", \"event\", \"type\"],\n        header: {\n            date: \"Date\",\n            event: \"Evénement du réseau\",\n            type: \"Type d'événement\"\n        },\n        format: {\n            type: (x) =&gt; html`\n            &lt;span style=\"text-transform: capitalize; display: inline-flex; align-items: center;\"&gt;\n    &lt;span style=\"border-bottom: solid 1px ${color_mapping_events[x]}; margin-bottom: -2px;\"&gt;${x}&lt;/span&gt;\n    &lt;span style=\"width: 10px; height: 10px; margin-left: 5px; background-color: ${color_mapping_events[x]};\"&gt;&lt;/span&gt;\n    &lt;/span&gt;\n            `,\n        event: (x) =&gt; html`&lt;a ${links_website_ssphub[x] !== undefined ? `href=\"${links_website_ssphub[x]}\" target=\"_blank\"` : ''}&gt;${x}&lt;/a&gt;`\n        }\n    })\n\n\n\n\n\n\n\n\n\n\nDu contenu qui intéresse au-delà des statisticiens publics\nSi les statistiques concernant la composition du réseau parmi les organismes de la statistique publique sont relativement stables par rapport à l’an dernier, le changement principal ayant eu lieu en 2023 est l’ouverture progressive à des publics hors de la statistique publique (administrations hors du SSP, chercheurs et étudiants…)\n\nhtml`&lt;div&gt;${barplot_ssp}&lt;/div&gt;`\n\n\n\n\n\n\n\n\nCode pour générer la figure\n\n\nbarplot_ssp = Plot.plot({\n  marginLeft: 60,\n  marginRight: 100,\n  x: {label: \"Frequency\"},\n  y: {label: null},\n  color: {\n    domain: [\"Hors du SSP\", \"Service Statistique Public (SSP)\"],\n    range: [\"forestgreen\", \"#6886bb\"]\n    },\n  marks: [\n    Plot.barX(hors_ssp_data,\n    {\n      y: \"SSP\",\n      fy: (d) =&gt; new Date(d.date).toLocaleString(\"fr\", {\n        \"month\": \"long\",\n        \"year\": \"numeric\"\n      }),\n      x: \"mail\", inset: 0.5, fill: \"SSP\", sort: \"mail\",\n      tip: true, channels: {share: (d)  =&gt; `${100*d.share.toFixed(2)}%`}\n    }),\n    Plot.axisY({textAnchor: \"start\", fill: \"black\", dx: 14}),\n    Plot.ruleX([0])\n  ]\n})\n\n\n\n\n\n\n\nLes deux dernières publications sur le site du réseau, à savoir l’infolettre #15 sur le réentrainement des modèles de langage et, surtout, le post de blog sur la publication du recensement de la population au format Parquet ont connu un écho important hors des cercles de data scientist du service statistique public et ont amené de nouveaux publics à suivre les contenus proposés par le réseau."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#la-startup-mistral-ai-publie-un-modèle-à-létat-de-lart",
    "href": "infolettre/infolettre_16/index.html#la-startup-mistral-ai-publie-un-modèle-à-létat-de-lart",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "La startup Mistral AI publie un modèle à l’état de l’art",
    "text": "La startup Mistral AI publie un modèle à l’état de l’art\nMistral AI, une startup française spécialisée dans l’intelligence artificielle, vient de publier un modèle nommé Mixtral qui repose sur le principe du mixture of experts. Cette technique consiste à privilégier une architecture construite à partir de sous-modèles spécialisés plutôt qu’un modèle généraliste qui s’adapte en fin de procédure à une question spécialisée. Dans ce type de modèles, l’enjeu est ainsi d’interpréter la question pour diriger la réponse vers l’expert adéquat : si une question porte sur un sujet de cuisine, un.e expert.e spécialisé.e en code sera de peu de secours.\nD’après les premières évaluations publiées, ce modèle surpasserait les capacités des autres modèles ouverts (notamment Llama 2) et s’approcherait des performances de GPT 3.5, le modèle derrière la version gratuite de ChatGPT. Cette annonce a eu lieu en pleine période de levée de fonds pour Mistral AI qui aurait obtenu un financement de 385 millions d’euros.\n\n\nTableau des performances (source: Mistral AI)\n\n\n\n\nTableau des performances publié par Mistral AI\n\n\n\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nUn article du Monde sur l’entreprise Mistral AI ;\nLe modèle Mixtral sur Huggingface ;\nLe principe des architectures mixture of experts (article Wikipedia)."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#leurope-parvient-à-un-accord-sur-les-premières-règles-au-monde-en-matière-dia",
    "href": "infolettre/infolettre_16/index.html#leurope-parvient-à-un-accord-sur-les-premières-règles-au-monde-en-matière-dia",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "L’Europe parvient à un accord sur les premières règles au monde en matière d’IA",
    "text": "L’Europe parvient à un accord sur les premières règles au monde en matière d’IA\nDans un accord provisoire signé le 9 décembre 2023 et nommé “Artificial Intelligence Act”, les États Membres et le Parlement européen ont établi une proposition relative à des règles harmonisées concernant l’intelligence artificielle (IA).\nDébutées en 2018, avant que les IA génératives ne deviennent si populaires, ces discussions dépassent le cadre exclusif de ces dernières. Néanmoins, concernant celles-ci, le compromis prévoit une approche différenciée suivant le contexte de développement et l’usage de ces modèles. Outre le respect des règles européennes de propriété intellectuelle, les développeurs de modèles génératifs devront s’assurer que les produits diffusés sont bien identifiés comme artificiels, afin de limiter la diffusion de deepfakes. Les développeurs de ces modèles devront également communiquer sur la qualité des données utilisées pour entraîner les modèles et sur le coût énergétique de ceux-ci. Les modèles open source et ceux construits à des fins de recherche bénéficient d’exemptions de ces règles.\nDes contraintes renforcées s’appliqueront aux systèmes jugés à “haut risque” dans des domaines comme la défense, l’éducation, les ressources humaines ou encore la santé. Pour ces systèmes, il sera nécessaire de réaliser une analyse d’impact avant la mise sur le marché. Par ailleurs, une obligation de transparence et d’explicabilité des modèles est mise en place afin d’être en mesure de comprendre les règles de décision de ces IA.\nL’accord provisoire interdit également l’utilisation de l’IA dans quelques domaines, jugés trop sensibles. Par exemple, la reconnaissance faciale de masse est interdite, hormis lorsque celle-ci est justifiée par des motifs de sécurité nationale. D’autres utilisations, qui peuvent amener à des dérives, comme la notation sociale basée sur le comportement ou des caractéristiques personnelles, sont interdits. Les travaux se poursuivront maintenant au niveau technique dans les semaines à venir afin de mettre au point les détails du nouveau règlement. Une fois ces travaux terminés, la présidence présentera le texte de compromis aux représentants des États membres pour approbation.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLa présentation de l’accord sur le site web du Conseil de l’Europe."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#nouveau-post-de-blog-diffusion-du-recensement-de-la-population-au-format-parquet",
    "href": "infolettre/infolettre_16/index.html#nouveau-post-de-blog-diffusion-du-recensement-de-la-population-au-format-parquet",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "Nouveau post de blog: diffusion du recensement de la population au format Parquet",
    "text": "Nouveau post de blog: diffusion du recensement de la population au format Parquet\nChaque année, l’Insee diffuse des statistiques construites à partir du recensement de la population, l’une des enquêtes phares de l’institut. Pour accompagner ces résultats et permettre à de nombreux acteurs de creuser ces données très riches dans des dimensions qui les intéressent, l’Insee diffuse également des bases de données détaillées construites après anonymisation de près de 20 millions de données individuelles.\nCes données, d’une extrême richesse, étaient historiquement complexes à manipuler du fait de leur volumétrie. La diffusion de celles-ci sous le format Parquet, une première mondiale parmi les instituts de statistique publique, vise à simplifier leur exploitation. Pour accompagner cette innovation, en partenariat avec les services de diffusion de l’Insee, le dernier post de blog du réseau présente un guide pratique d’utilisation de ces données dans plusieurs langages de traitement ( , Python  et Observable ) par le biais de DuckDB.\nCombien d’habitants de Toulouse ont changé de logement sur l’année ? Quels sont les départements avec le plus de centenaires ? Le post de blog vous montrera comment calculer ces statistiques. Et si vous désirez découvrir ce format avec des exemples additionnels, ce post d’Eric Mauvière vous intéressera également.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLe post de blog ;\nUn article sur le format Parquet dans le Courrier des stats n°9 écrit par Alexis Dondon et Pierre Lamarche ;\nLe blog d’Eric Mauvière qui présente une série d’articles sur le format Parquet;\nLa présentation de Romain Lesur sur le sujet pour l’atelier Modernisation of Official Statistics de l’UNECE."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#le-sspcloud-se-dote-dun-explorateur-de-fichiers-basé-sur-duckdb",
    "href": "infolettre/infolettre_16/index.html#le-sspcloud-se-dote-dun-explorateur-de-fichiers-basé-sur-duckdb",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "Le SSPCloud se dote d’un explorateur de fichiers basé sur DuckDB",
    "text": "Le SSPCloud se dote d’un explorateur de fichiers basé sur DuckDB\nDuckDB est un outil extrêmement efficace pour lire des fichiers Parquet et CSV. Outre son efficacité, DuckDB présente l’avantage d’être disponible par le biais de plusieurs clients: , Python  mais aussi un navigateur web grâce à Javascript  . Des acteurs majeurs de l’écosystème de la data science, notamment Observable, ont fait de DuckDB une pierre angulaire de leurs explorateurs de données. L’avantage de cette approche, typique du web assembly (approche visant à mettre à disposition des logiciels de calculs scientifiques par le biais d’un simple navigateur), est que seul Javascript , qui est disponible sur tout navigateur, est nécessaire pour visualiser et effectuer des traitements analytiques sur des données.\nLe SSPCloud, la plateforme moderne de traitement de données développée par l’Insee et mise à disposition de près de 3000 agents de l’administration ou étudiants, vient de mettre en oeuvre un explorateur aux fonctionnalités similaires 🚀.\n\n\n\nUn exemple d’utilisation de cet explorateur sur les données détaillées du recensement 👆️\n\n\nCelui-ci s’appuie sur DuckDB et permet de visualiser de manière très fluide les fichiers aux formats Parquet et CSV. Il ne se restreint pas aux données disponibles sur les espaces de stockage personnels du SSPCloud: n’importe quel fichier, au format adéquat et disponible sur internet, peut être lu avec ce visualiseur. Il n’est d’ailleurs pas nécessaire d’avoir un compte sur le SSPCloud pour l’utiliser, il suffit que le fichier que l’on souhaite lire soit un fichier open data  😍 !\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nDes éléments sur le web assembly ;\nL’explorateur de fichier du SSPCloud ;\nL’explorateur de fichiers de data.gouv basé sur la même approche technologique."
  },
  {
    "objectID": "infolettre/infolettre_16/index.html#laccessibilité-de-jupyter-améliorée-avec-le-concours-de-linsee",
    "href": "infolettre/infolettre_16/index.html#laccessibilité-de-jupyter-améliorée-avec-le-concours-de-linsee",
    "title": "Rétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud",
    "section": "L’accessibilité de Jupyter améliorée avec le concours de l’Insee",
    "text": "L’accessibilité de Jupyter améliorée avec le concours de l’Insee\nAfin de ne pas pénaliser certains publics, les logiciels doivent respecter des critères d’accessibilité. Ils doivent notamment avoir de nombreuses fonctionnalités accessibles sans souris, exclusivement par le biais du clavier. Cependant, Jupyter, logiciel bien connu des data scientists, par la structure complexe de son interface, présentait plusieurs défauts, comme la difficulté à naviguer dans la page pour trouver le menu nécessaire pour éditer du code.\nGrâce à une subvention de l’Insee, des travaux d’amélioration de l’accessibilité de Jupyter ont pu être menés. Les prochaines versions du logiciel devraient être plus accessibles, et, entre autres, plus pratiques d’usage pour les data scientists qui privilégient le clavier à la souris pour se déplacer dans un document.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nL’annonce sur le blog de Jupyter ;\nLe principe d’accessibilité clavier du W3C."
  },
  {
    "objectID": "infolettre/infolettre_14/index.html",
    "href": "infolettre/infolettre_14/index.html",
    "title": "Propositions de lecture estivale",
    "section": "",
    "text": "Note\n\n\n\nVous désirez intégrer la liste de diffusion ? L’inscription se fait ici.\nCette newsletter propose un petit cahier de vacances de la data science afin de profiter de la période estivale pour en apprendre plus sur les sujets en vogue et ainsi être fin prêt pour la rentrée 📖⛱️."
  },
  {
    "objectID": "infolettre/infolettre_14/index.html#lectures-estivales",
    "href": "infolettre/infolettre_14/index.html#lectures-estivales",
    "title": "Propositions de lecture estivale",
    "section": "Lectures estivales",
    "text": "Lectures estivales\nPour commencer, quelques conseils de lecture sur les grands modèles de langage :\n\nUn article du Washington Post pour en savoir plus sur le corpus d’entrainement des grands modèles de langage (large langage model, LLM) ;\nPour comprendre la manière dont ChatGPT et les grands modèles de langage traduisent vos phrases afin de vous répondre, consultez ce post de blog sur les tokenizers (décomposition d’une chaine de caractères en unité minimale comme un mot ou une syllabe);\nEntre deux épisodes de votre série préférée, vous pourrez alterner avec ce cours de 2 heures sur les grands modèles de langage ;\nVous serez ensuite prêts à approfondir le sujet avec ce cours complet d’Huggingface sur le traitement automatique du langage.\n\nN’hésitez pas à faire des pauses dessin virtuel pour stimuler votre créativité en créant des images avec StableDiffusion ou Dall-E 2.\nLa version Python de l’ouvrage de référence Introduction to Statistical Learning vient de sortir et est disponible, comme la version R, gratuitement. Si après la lecture de celui-ci vous désirez mieux comprendre la question de l’interprétabilité des modèles de machine learning, c’est-à-dire les méthodes statistiques permettant de mieux comprendre la manière dont les algorithmes d’apprentissage aboutissent à une décision, ce site web vous sera très utile.\nSi vous désirez en apprendre plus sur la question de la reproductibilité, les ressources suivantes vous seront utiles :\n\nBuilding reproducible analytical pipelines with R ;\nCoding for economists ;\nFormation aux bonnes pratiques R et Git par l’INSEE ;\nUn cours de l’ENSAE sur la mise en production de projets data science et, pour approfondir sur la mise en production de modèles de machine learning, une formation de l’Insee sur le MLOps, ensemble de techniques qui visent à faciliter la mise en production et la maintenance de modèles.\n\nSi les semestres de l’année scolaire ne vous ont pas suffi, vous pouvez aussi profiter de l’été pour compléter votre formation en data science en suivant le missing semester de votre cursus.\nVous pouvez également reprendre l’ensemble des ressources mises à disposition dans le cadre du Funathon 2023, un événement pour lequel les équipes innovation de l’Insee et du SSM Agriculture et alimentation ont mis à disposition de nombreuses ressources R ou Python sur six thèmes concernant l’alimentation et la production agricole. Ces ressources couvrent un large éventail de niveaux de difficulté et de techniques pour permettre à la fois aux débutants en code et aux data scientists plus aguerris d’y trouver leur compte.\nEt pour un parcours complet de formation, rien de mieux que d’explorer de fond en comble le portail de formation du SSP Cloud.\nAprès avoir lu tout ceci, vous serez prêts pour les événements data science de la rentrée comme le hackathon du mobidatalab sur le thème de l’amélioration des services de mobilité urbaine (15 et 16 septembre).\nBonnes vacances ! Et n’oubliez pas de profiter pleinement des vacances pour oublier ChatGPT quelques temps ! 🌞🌊⛰️"
  },
  {
    "objectID": "infolettre/infolettre_12/index.html",
    "href": "infolettre/infolettre_12/index.html",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "",
    "text": "Note\n\n\n\nVous désirez intégrer la liste de diffusion ? L’inscription se fait ici.\nDu fait de la densité des actualités dans le monde de la data science et des multiples événements à venir dans le cadre de ce réseau, nous proposons d’accélérer le rythme de publication des newsletters."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#la-charte-graphique-de-linsee-sur-le-tapis-rouge",
    "href": "infolettre/infolettre_12/index.html#la-charte-graphique-de-linsee-sur-le-tapis-rouge",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "La charte graphique de l’Insee sur le tapis rouge",
    "text": "La charte graphique de l’Insee sur le tapis rouge\nLa semaine dernière avait lieu la cérémonie des Oscars. Grâce à un fil de Clara Dealberto, on peut mesurer l’influence des graphistes de l’Insee sur les stylistes des stars :\n\n\n\nTweet de Clara Dealberto\n\n\nLe fil, qui met à l’honneur l’Insee, vaut le détour ; n’hésitez pas à le consulter ! Ou à découvrir celui sur le Met Gala."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#chatgpt-encore-et-toujours",
    "href": "infolettre/infolettre_12/index.html#chatgpt-encore-et-toujours",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "ChatGPT encore et toujours",
    "text": "ChatGPT encore et toujours\n\n\n\n\n\nChatGPT continue de focaliser l’attention. Dans la veine de l’article désignant les modèles de langages sous le terme de “stochastic parrots” (“perroquets stochastiques”), Arthur Charpentier parle lui de “société du bullshit” pour désigner la manière dont ChatGPT offre, sous un raisonnement en apparence logique, de manière indifférenciée des absurdités et des vérités.\nEspérons que lorsque ChatGPT sera embarqué dans les voitures General Motors, il ne nous donnera pas de fausse indication pour changer un pneu ou ne se retournera pas contre le conducteur comme le ferait HAL 9000.\nUn article intéressant de Wired questionne d’ailleurs l’empreinte carbone que pourrait impliquer la généralisation des modèles de langage dans les moteurs de recherche, qui font face à des milliards de requêtes quotidiennes.\nAu moment où OpenAI rend public GPT-4, une version plus riche de son modèle GPT-3 qui servait de base à ChatGPT, l’un des cofondateurs d’OpenAI revient sur la stratégie d’ouverture (ou plutôt l’absence d’ouverture) d’OpenAI : “Nous avions tord” (voir The Verge). Hasard du calendrier, cette déclaration a eu lieu presque au même moment que la publication d’un robot conversationnel ouvert OpenChatKit. La concurrence est néanmoins âpre puisqu’une équipe de Microsoft a déjà proposé l’intégration à ChatGPT d’un module permettant d’interagir avec ChatGPT également par le biais d’images."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#des-turbulences-dans-la-silicon-valley",
    "href": "infolettre/infolettre_12/index.html#des-turbulences-dans-la-silicon-valley",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Des turbulences dans la Silicon Valley",
    "text": "Des turbulences dans la Silicon Valley\n\n\n\nUne infographie des faillites bancaires par Mike Bostock. Source: Notebook Observable\n\n\nL’autre actualité phare des quinze derniers jours est la faillite de la Silicon Valley Bank. Aux Etats-Unis, il s’agit de la plus principale faillite bancaire depuis 2008 aux Etats-Unis.\nLa Fed est rapidement intervenue pour endiguer la panique bancaire, même si la banque était en fait déjà dans ses radars bien avant sa faillite (voir NYT)."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#retour-sur-les-évolutions-récentes-du-monde-de-la-data-science",
    "href": "infolettre/infolettre_12/index.html#retour-sur-les-évolutions-récentes-du-monde-de-la-data-science",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Retour sur les évolutions récentes du monde de la data science",
    "text": "Retour sur les évolutions récentes du monde de la data science\nLe panorama technologique 2023 de Matt Turck confirme la tendance à la diversification des technologies à maîtriser pour mener un projet de data science. Cette complexification des outils et des rôles dans l’écosystème de la donnée, évoquée dans l’article “Is Data Scientist Still the Sexiest Job of the 21st Century?”, est ici confirmée.\nLes derniers sondages auprès des recruteurs américains montrent la popularité des data engineers, plus spécialisés que les data scientists dans la mise en oeuvre d’infrastructures techniques pour valoriser des données. Le profil de data engineer apparaît en deuxième place dans le classement des profils les plus recherchés par les recruteurs alors que les data scientists n’apparaissent plus dans les premières places du classement."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#le-big-data-nest-pas-mort",
    "href": "infolettre/infolettre_12/index.html#le-big-data-nest-pas-mort",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Le big data n’est pas mort",
    "text": "Le big data n’est pas mort\nUne réponse intéressante à l’article “Big data is dead” (voir Newsletter #11) revient sur l’intérêt de disposer de données historiques longues pour l’entrainement de modèles d’apprentissage."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#r-directement-dans-le-navigateur",
    "href": "infolettre/infolettre_12/index.html#r-directement-dans-le-navigateur",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "R directement dans le navigateur",
    "text": "R directement dans le navigateur\nAvec un peu de retard sur Python, il devient maintenant possible de faire du R directement depuis le navigateur web, c’est-à-dire sans installation du logiciel R, grâce à WebR. Cette approche est typique du Web Assembly où les langages de programmation sont directement utilisés depuis le navigateur, sans installation préalable."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#première-journée-du-réseau-en-avril-17-avril",
    "href": "infolettre/infolettre_12/index.html#première-journée-du-réseau-en-avril-17-avril",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Première journée du réseau en avril (17 avril)",
    "text": "Première journée du réseau en avril (17 avril)\n\n\n\n\n\nDéjà annoncée dans la Newsletter #11, nous rappelons la journée du réseau le 17 avril, en présentiel 📅.\nLe nombre de places dans l’espace à disposition étant limité, une invitation par mail et un lien d’inscription seront communiqués dans la semaine pour pouvoir participer à cet événement en présentiel dans le tiers-lieu la Tréso à Malakoff."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#bonnes-pratiques-en-python-présentation-lors-des-ateliers-du-programme-10-30-mars",
    "href": "infolettre/infolettre_12/index.html#bonnes-pratiques-en-python-présentation-lors-des-ateliers-du-programme-10-30-mars",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Bonnes pratiques en Python : présentation lors des ateliers du programme 10% (30 mars)",
    "text": "Bonnes pratiques en Python : présentation lors des ateliers du programme 10% (30 mars)\nDans le cadre du programme 10%, des présentations ont lieu avant certains ateliers de travail sur les projets communautaires.\nLa prochaine présentation, qui aura lieu le jeudi 30 mars de 14h à 15h 📅, sera donnée par des membres du réseau. Elle portera sur une présentation des outils favorisant les bonnes pratiques de développement en Python et de l’intérêt de ces bonnes pratiques pour faciliter la mise en production de projets de data science. Il s’agira d’une présentation succincte du contenu du cours de l’ENSAE “Bonnes pratiques et mise en production de projets data science”.\nPlus d’infos à venir par le biais du canal Tchap de notre réseau."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#un-événement-autour-de-locrisation-avec-christopher-kermorvant-29-mars",
    "href": "infolettre/infolettre_12/index.html#un-événement-autour-de-locrisation-avec-christopher-kermorvant-29-mars",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Un événement autour de l’OCRisation avec Christopher Kermorvant (29 mars)",
    "text": "Un événement autour de l’OCRisation avec Christopher Kermorvant (29 mars)\nLe mercredi 29 mars de 15h à 16h 📅 nous recevons Christopher Kermorvant, chercheur spécialisé en OCRisation et fondateur de Teklia. Christopher mène actuellement plusieurs projets de numérisation de textes anciens, notamment d’OCRisation de vieux recensements avec l’INED.\nPendant cet événement, Christopher nous fera un état de l’art de l’OCRisation puis nous présentera des projets qu’il a pu mener récemment avec Teklia.\nIl est possible de suivre la présentation via Zoom ou, pour les personnes présentes à l’Insee, en 2-C-496."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#présentation-de-la-documentation-collaborative-carpentries-28-mars",
    "href": "infolettre/infolettre_12/index.html#présentation-de-la-documentation-collaborative-carpentries-28-mars",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Présentation de la documentation collaborative Carpentries (28 mars)",
    "text": "Présentation de la documentation collaborative Carpentries (28 mars)\nPour rappel, Kate Burnett-Isaacs, de Statistics Canada, nous présentera l’initiative Meta Academy / Carpentries le mardi 28 mars à 15h 📅. Plus de détails dans la Newsletter #11.\nInvitation Outlook ici."
  },
  {
    "objectID": "infolettre/infolettre_12/index.html#post-de-blog-sur-polars",
    "href": "infolettre/infolettre_12/index.html#post-de-blog-sur-polars",
    "title": "Tapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley",
    "section": "Post de blog sur Polars",
    "text": "Post de blog sur Polars\n\n\n\n\n\nPour faire suite à la Newsletter #11 qui présentait l’écosystème autour du package Python Polars, Romain Tailhurat (Insee) nous propose un post de blog pour découvrir ce package.\nCelui-ci est accompagné par un tutoriel pas-à-pas pour découvrir les principales fonctionnalités de la librairie. Il est possible de tester le notebook en un seul clic sur le SSP Cloud ou sur Google Colab.\nVous pouvez également retrouver ce tutoriel depuis l’espace formation du SSP Cloud."
  },
  {
    "objectID": "infolettre/infolettre_10/index.html",
    "href": "infolettre/infolettre_10/index.html",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit"
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#un-événement-autour-des-packages-facilitant-laccès-à-lopen-data-de-linsee",
    "href": "infolettre/infolettre_10/index.html#un-événement-autour-des-packages-facilitant-laccès-à-lopen-data-de-linsee",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Un événement autour des packages facilitant l’accès à l’open data de l’Insee",
    "text": "Un événement autour des packages facilitant l’accès à l’open data de l’Insee\n\n\n\n\n\nAprès les présentations d’observable et de gridviz nous vous proposons un nouvel événement. Celui-ci sera autour de l’open data à travers la présentation des packages facilitant la récupération de données de l’Insee disponibles depuis le site web ou les API.\nDeux présentations sont prévues : - Pierre Lamarche présentera le package  doremifasol. C’est notamment grâce à ce package que la documentation utilitR peut s’appuyer sur des données bien connues des utilisateurs d’open data (Filosofi, recensement…) - Hadrien Leclerc nous présentera le package  Pynsee qui est utilisé depuis deux ans à l’ENSAE pour apprendre aux futurs data scientists à récupérer des données de cadrage.\nCes deux présentations seront suivies d’un temps d’échange.\nCet événement aura lieu le 13 février de 15h à 16h30 (📅 invitation Outlook). Si vous êtes utilisateurs de données, que vous veniez de l’Insee ou non, ces packages peuvent vous intéresser !"
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#masterclass-datascientest",
    "href": "infolettre/infolettre_10/index.html#masterclass-datascientest",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Masterclass datascientest",
    "text": "Masterclass datascientest\n\n\n\n\n\nLes masterclass organisées avec l’organisme de formation spécialisé datascientest reprennent ! Après une première masterclass au mois de décembre consacrée au MLOps, notre réseau va proposer de nouvelles séances.\nLes premières séances vont s’organiser autour de deux cursus parallèles, qui commenceront par des introductions pour monter graduellement en niveau et se rapprocher des cas d’usages que rencontrent nos data scientists.\nLe premier parcours sera orienté autour des problématiques de NLP. La première séance aura lieu le 10 février, de 10h à 12h et constituera une introduction au NLP avec un retour sur certains concepts centraux (preprocessing, tokenisation, lemmatisation…) et des exemples d’applications avec le package SpaCy. Une deuxième séance dans ce parcours est déjà programmée, le 24 mars de 10h à 12h, sur le thème de la similarité textuelle et de la classification de textes grâce aux méthodes d’embeddings.\nLe deuxième parcours cible la problématique de l’analyse d’images. Une première séance, qui aura lieu le 10 mars de 10h à 12h reviendra sur certains concepts centraux du deep learning (perceptron, convolution, transfer learning…). Les séances suivantes, dont les dates n’ont pas encore été arrêtées, s’intéresseront à des cas d’usages comme l’OCRisation ou la détection d’objets dans des images.\n\n\n\n\n\n\nNote\n\n\n\nPour vous inscrire, il suffit de remplir ce formulaire !"
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#questionnaire-sur-vos-besoins-en-formation-data-science",
    "href": "infolettre/infolettre_10/index.html#questionnaire-sur-vos-besoins-en-formation-data-science",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Questionnaire sur vos besoins en formation data science",
    "text": "Questionnaire sur vos besoins en formation data science\n\n\n\n\n\nEn cette période de recensement, le réseau propose également le sien ! Pour déterminer au mieux la répartition des besoins en formation sur les sujets data science et ainsi pouvoir proposer des événements pertinents, nous vous proposons un questionnaire sur vos besoins en formation."
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#utilitr-recherche-des-rédacteurs-dexercices",
    "href": "infolettre/infolettre_10/index.html#utilitr-recherche-des-rédacteurs-dexercices",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "utilitR recherche des rédacteurs d’exercices !",
    "text": "utilitR recherche des rédacteurs d’exercices !\n\n\n\n\n\nDans le but de continuer à développer utilitR, documentation collaborative et ouverte, l’équipe du projet souhaite encourager des contributions volontaires pour ajouter des exercices à chaque fiche thématique. L’objectif est de produire pour chaque chapitre un ensemble d’exercices, de difficulté graduelle, permettant de mettre en application les concepts présentés dans la fiche.\n\nCes exercices seraient accessibles depuis le site web mais aussi à travers le portail de formation du SSP Cloud, sous la forme de notebooks d’autoformation.\nL’équipe du projet utilitR est donc à la recherche des personnes motivées pour rédiger des exercices ou mettre à disposition des bouts de code ou des exercices déjà préparés. Si vous désirez apporter votre pierre à l’édifice, toute contribution, même modeste, sur cette page, sera appréciée par l’équipe utilitr.\nCette évolution de la documentation vise à prolonger l’effort continu pour construire une documentation vivante, interactive et originale. L’esthétique du site web book.utilitr.org a ainsi été revue récemment afin de rendre la documentation plus ergonomique tout en ajoutant des fonctionnalités utiles aux lecteurs, comme la possibilité de surligner ou de prendre des notes."
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#proposez-un-billet-de-blog",
    "href": "infolettre/infolettre_10/index.html#proposez-un-billet-de-blog",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Proposez un billet de blog !",
    "text": "Proposez un billet de blog !\n\n\n\n\n\nLe site web du réseau (https://ssphub.netlify.app/) propose depuis septembre une section blog. Vos idées et contributions sont les bienvenues pour l’enrichir !\nPour souligner l’aspect collectif de cette section, un guide des contributeurs vient de voir le jour. Celui-ci expose la démarche à suivre, de la phase de discussion pour définir le sujet du billet aux outils proposés pour faciliter la rédaction et la soumission de celui-ci depuis Github ."
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#la-saison-2-du-programme-10-arrive",
    "href": "infolettre/infolettre_10/index.html#la-saison-2-du-programme-10-arrive",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "La saison 2 du programme 10% arrive",
    "text": "La saison 2 du programme 10% arrive\n\n\n\n\n\nL’attente était insoutenable mais la nouvelle saison de 10% est enfin là ! Rejoignez ce programme, issu des recommandations du rapport de l’Inspection Générale de l’Insee et de la DINUM, où des data scientists proposent de consacrer jusqu’à 10% de leur temps de travail à des projets transversaux !\nAu-delà de la participation à ces projets, le programme 10% est également l’opportunité d’échanger des idées avec des data scientists d’autres administrations et de bénéficier de formations.\nAprès un webinaire d’information sur le programme le 31 janvier (inscription via eventbrite), la journée de lancement de la saison 2 se tiendra le 14 février au Bercy Lab (plus d’infos à venir).\nCette saison, plus longue que la première, permettra de pérenniser certains des projets de la saison 1 mais aussi de lancer de nouveaux projets.\nInscrivez-vous dès maintenant pour ne pas manquer la saison 2 de 10%!\n\n\n\n\n\n\nNote\n\n\n\nPour plus d’information sur le programme : lab-ia@data.gouv.fr"
  },
  {
    "objectID": "infolettre/infolettre_10/index.html#report-de-la-journée-de-la-donnée",
    "href": "infolettre/infolettre_10/index.html#report-de-la-journée-de-la-donnée",
    "title": "DoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images",
    "section": "Report de la journée de la donnée",
    "text": "Report de la journée de la donnée\nLa journée de la donnée organisée par l’Administrateur Ministériel des Données, Algorithmes et Codes sources (AMDAC) du Ministère de la Santé, initialement prévue le 31 janvier, est reportée à une date ultérieure."
  },
  {
    "objectID": "infolettre/infolettre_08/index.html",
    "href": "infolettre/infolettre_08/index.html",
    "title": "L’année 2022 dans le monde de la data science : IA, transformation de RStudio, Observable",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit\nLa fin de l’année est généralement synonyme de bétisiers, best of ou rétrospectives personnalisées qui nous permettent de nous rappeler les événements marquants de l’année.\nPour célébrer la fin de l’année 2022, la newsletter de janvier adopte un format un peu spécial pour proposer, en deux temps, deux rétrospectives.\nCette première newsletter revient sur les principaux événements de l’année 2022 dans le monde de la data science. La seconde newsletter proposera une rétrospective quantitative sur le réseau des data scientists de la statistique publique, à la manière des rétrospectives personnalisées de nos applications préférées.\n\nLes IA créatrices de contenu à l’honneur\nSi l’année 2022 a été particulièrement riche dans le domaine de la data science, c’est principalement grâce à deux coups médiatiques d’OpenAI, à savoir Dall-E et ChatGPT.\nCes deux outils ont beaucoup fait parler d’eux, au-delà de la sphère traditionnelle de la data science. Le buzz a été intense sur Twitter ou sur Mastodon, le réseau social dont le nombre d’utilisateurs a nettement augmenté en réaction au rachat de Twitter par Elon Musk en fin d’année.\n\nCes innovations, parce qu’elles pourraient avoir des effets à long terme sur la manière dont le grand public appréhende l’intelligence artificielle, ont beaucoup intéressé les médias traditionnels, notamment Le Monde, The Economist et sa “Nouvelle Frontière” ou le Guardian qui s’interroge sur la nature des tâches que l’intelligence artificielle pourra remplacer à terme : procédurales et régies par des règles bien définies ou bien également des activités nécessitant de la créativité et des capacités d’analyse ?\nPour une fois, il ne s’agit donc pas de souligner exclusivement les limites de ces modèles voire leurs dérives (deep fake, biais racistes…) mais aussi de s’enthousiasmer sur leur potentiel créatif. Il est difficile de rester insensible à certaines des créations artistiques des modèles Dall-E, Stable Diffusion, Midjourney et consorts ou de résister à la tentation de tester la capacité de ChatGPT à répondre à des questions complexes Les chercheurs, et pas des moindres (notamment Andrew Ng ou Gaël Varoquaux) se sont également saisis de cette question et ont souligné les biais de raisonnement et excès de confiance de ces IA.\n\n\n\nhttps://github.com/Stability-AI/stablediffusion\n\n\nSi vous désirez utiliser Python de manière créative pour générer du contenu avec Stable Diffusion, vous pouvez consulter ce tutoriel qui fonctionne sur le SSPCloud ou sur Google Colab.\n\n\n\nLe succès des modèles de diffusion\nCes IA génératrices de contenu reposent toutes, à plusieurs niveaux, sur des réseaux de neurone.\nLe premier étage de la fusée est un modèle de langage (large language model) qui synthétise un langage en un ensemble complexe de paramètres. Les plus connus sont BERT et GPT-3. L’inflation dans le nombre de paramètres n’est pas prête de s’arrêter. Si les ressources nécessaires à entraîner en 2018 le modèle BERT (110 millions de paramètres) avaient déjà été critiquées en raison de leur coût financier et environnemental, cette complexité a encore augmenté depuis. Le modèle GPT-3, sorti en 2020, et qui sert de base à Dall-E et ChatGPT intègre 175 milliards de paramètres. Un chiffre qui apparaît minime par rapport aux 17O trillions de paramètres attendus pour le modèle GPT-4 en 2023.\nEn ce qui concerne les IA créatrices de contenu visuel, le deuxième étage de la fusée est un modèle d’analyse d’image qui apprend à associer des images à une description textuelle afin de détecter des structures communes entre des mots ou des séquences de mots et des formes sur des images. Il s’agit de déconstruire une forme en une structure minimale de pixels qui permet de l’identifier.\n\n\n\nSource: Sebastian Raschka\n\n\nEnsuite, pour générer une image à partir d’une description inédite intervient le modèle de diffusion qui reconstruit une image à partir du mélange de l’ensemble des pixels qui traduisent les concepts principaux d’une instruction. L’une des explications les plus pédagogiques pour comprendre le fonctionnement de ces modèles vient du Washington Post.\nSinon, on peut demander directement à ChatGPT de nous expliquer:\n\n\n\nL’actualité dans le monde du deep learning\nSi le succès d’estime de ces IA génératrices consacre les modèles de diffusion, l’année du deep learning ne se réduit pas à cette actualité.\nL’année a notamment été marquée par la compétition entre les librairies et écosystèmes TensorFlow, développé par Google, et PyTorch projet initié par Facebook/Meta. PyTorch, plus récent, bénéficie d’une dynamique plus ascendante que TensorFlow. Le succès d’HuggingFace, plateforme de mise à disposition de modèles, et où les implémentations PyTorch sont systématiques alors que celles en TensorFlow sont rares a participé à la diffusion de PyTorch.\nPreuve du succès de PyTorch, cet écosystème est dissocié de Meta depuis septembre afin de devenir un outil généraliste géré par la Linux Foundation. À l’inverse, Google semble se détacher graduellement de TensorFlow pour privilégier son nouvel écosystème JAX.\n\n\nDu changement côté RStudio\nDepuis quelques années, RStudio a fait le choix de devenir un écosystème de data science généraliste et non plus exclusivement attaché au langage R.\nCette année, cela s’est traduit par la publication, très commentée, de Quarto qui vise à proposer, dans de nombreux langages de programmation, des fonctionalités de publications reproductibles équivalentes à l’un des produits emblématiques de RStudio, à savoir R Markdown. Rien de mieux pour être convaincu de l’intérêt de cet outil que d’observer la galerie d’exemples, d’explorer la documentation très riche, ou de tester soi-même sur un exemple. Cet été, RStudio a également annoncé que Shiny, un autre produit emblématique, serait maintenant disponible sous Python, comme alternative à Dash ou Streamlit.\nL’année 2022 a été l’occasion, pour RStudio, d’un autre changement, symbolique celui-ci. Afin de détacher son image du langage R, l’entreprise a en effet changé de nom pour devenir posit. L’entreprise n’a néanmoins pas abandonné son activité foisonnante dans R puisque Hadley Wickham a commencé à publier de nouveaux chapitres pour une nouvelle édition augmentée de l’ouvrage de référence R For Data Science.\n\n\nObservable devient un incontournable dans le monde de la dataviz\nPour permettre des visualisations interactives, cela fait plusieurs années que JavaScript est un incontournable et que le web assembly retient de plus en plus d’attention.\nLes journaux traditionnels utilisent ainsi de plus en plus le data scrollytelling , cette technique de narration qui consiste à présenter des informations sous forme de récit interactif, en utilisant une combinaison de texte et de graphiques qui apparaissent et disparaissent en fonction des actions du lecteur. L’un des exemples les plus réussis des dernières années a sans doute été la visualisation du New York Times “How the virus got out”. Cette approche a également été adoptée par le Ministère de l’Agriculture pour diffuser les chiffrés clés du recensement agricole. Nos voisins anglais ne sont pas en reste puisque les derniers résultats du recensement sont proposés sur un site web remarquable de fluidité.\nAfin de permettre une diffusion accrue de visualisations en JavaScript, Mike Bostock, déjà créateur de la librairie de dataviz de référence D3.js, est à l’origine de la plateforme observable, sorte de Github de la dataviz permettant du partage et de la réutilisation de notebooks réactifs. En cette année 2022, la plateforme a connu un véritable boom et est devenu un incontournable dans le domaine. L’une des raisons est l’ajout de fonctionalités qui permettent d’étendre le public cible au delà des développeurs web, déjà accoutumés à Javascript. Parmi les fonctionalités les plus remarquables, la possibilité depuis Novembre d’utiliser des requêtes SQL grâce à DuckDB permet aux habitués de R ou de Python de retrouver des manipulations auxquels ils sont habitués. La librairie Plot offre une grammaire proche de ggplot2.\nLa communauté des cartographes a été particulièrement active sur Observable, notamment à l’occasion du #30daymapchallenge. Nicolas Bertin (neocarto), dont on ne peut que recommander l’introduction à Observable faite pour le réseau, ou Eric Mauvière font partie des comptes à suivre dans la communauté francophone.\nObservable, en tant que langage construit sur JavaScript, est également disponible pour les utilisateurs de Quarto, ce qui permet de mettre à disposition des visualisation réactives sans passer nécessairement par la plateforme observablehq.com pour mettre à disposition des visualisations réactives, ce qui constitue une alternative intéressante aux applications qui nécessitent un serveur en arrière plan, comme Shiny ou Dash.\n \n\n\nLes autres actualités en France\nLe rapport du conseil d’État pour la construction d’une IA de “confiance” a donc été publié en une année 2022 où les avancées techniques des dernières années commencent à être accessibles grâce à des outils plus grand public, ce qui va nécessairement soulever des enjeux éthiques et juridiques.\nLe projet Onyxia, qui vise à proposer une infrastructure de data science à l’état de l’art pour data scientists, a organisé son deuxième Openlab. L’occasion de revenir sur le projet, sa philosophie, ses dernières avancées mais aussi d’échanger sur les perspectives de réutilisation dans de multiples environnements et de nouer des partenariats qui permettront au projet de grandir encore en 2023."
  },
  {
    "objectID": "event/presentation-du-projet-meta-academy-carpentries/index.html",
    "href": "event/presentation-du-projet-meta-academy-carpentries/index.html",
    "title": "Présentation du projet Meta Academy - Carpentries",
    "section": "",
    "text": "Replay de l’événement:"
  },
  {
    "objectID": "event/presentation-des-packages-r-et-python-pour-acceder-a-lopen-data-de-linsee/index.html",
    "href": "event/presentation-des-packages-r-et-python-pour-acceder-a-lopen-data-de-linsee/index.html",
    "title": "Présentation des packages R et Python pour accéder à l’open data de l’Insee",
    "section": "",
    "text": "Présentation autour des packages développés par des data scientists pour faciliter la récupération des données officielles de l’Insee :"
  },
  {
    "objectID": "event/presentation-des-packages-r-et-python-pour-acceder-a-lopen-data-de-linsee/index.html#replay",
    "href": "event/presentation-des-packages-r-et-python-pour-acceder-a-lopen-data-de-linsee/index.html#replay",
    "title": "Présentation des packages R et Python pour accéder à l’open data de l’Insee",
    "section": "Replay",
    "text": "Replay"
  },
  {
    "objectID": "event/ocrisation-teklia/index.html",
    "href": "event/ocrisation-teklia/index.html",
    "title": "“OCRisation, état de l’art et projets auxquels participe Teklia” par Christopher Kermorvant",
    "section": "",
    "text": "Le replay de l’événement est disponible ci-dessous.\n\nLes slides présentées par Christopher Kermorvant sont aussi disponibles ici."
  },
  {
    "objectID": "event/2025-12-01-network-day/index.html#présentation-de-loffre-llm-du-ssp-cloud-insee---division-innovation-instruction-technique-diit",
    "href": "event/2025-12-01-network-day/index.html#présentation-de-loffre-llm-du-ssp-cloud-insee---division-innovation-instruction-technique-diit",
    "title": "Troisième journée du SSPHub",
    "section": "Présentation de l’offre LLM du SSP Cloud, Insee - Division Innovation instruction technique (DIIT)",
    "text": "Présentation de l’offre LLM du SSP Cloud, Insee - Division Innovation instruction technique (DIIT)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2025-12-network/1_SSPCloud_LLM.pdf\",\n  \"Télécharger les slides\"\n)"
  },
  {
    "objectID": "event/2025-12-01-network-day/index.html#extraire-et-analyser-les-compétences-numériques-dans-les-offres-demploi-grâce-au-nlp-dares---service-statistique-ministériel-du-ministère-du-travail-et-de-lemploi-insee---dee",
    "href": "event/2025-12-01-network-day/index.html#extraire-et-analyser-les-compétences-numériques-dans-les-offres-demploi-grâce-au-nlp-dares---service-statistique-ministériel-du-ministère-du-travail-et-de-lemploi-insee---dee",
    "title": "Troisième journée du SSPHub",
    "section": "Extraire et analyser les compétences numériques dans les offres d’emploi grâce au NLP (Dares - Service statistique ministériel du Ministère du Travail et de l’Emploi & Insee - DEE)",
    "text": "Extraire et analyser les compétences numériques dans les offres d’emploi grâce au NLP (Dares - Service statistique ministériel du Ministère du Travail et de l’Emploi & Insee - DEE)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2025-12-network/2_JOCAS_DARES.pdf\",\n  \"Télécharger les slides\"\n)"
  },
  {
    "objectID": "event/2025-12-01-network-day/index.html#quelles-évolutions-récentes-et-à-venir-sur-data.gouv-dinum",
    "href": "event/2025-12-01-network-day/index.html#quelles-évolutions-récentes-et-à-venir-sur-data.gouv-dinum",
    "title": "Troisième journée du SSPHub",
    "section": "Quelles évolutions récentes et à venir sur data.gouv (Dinum)",
    "text": "Quelles évolutions récentes et à venir sur data.gouv (Dinum)\n\nPrésentation disponible sur Figma"
  },
  {
    "objectID": "event/2025-12-01-network-day/index.html#automatisation-des-infos-rapides-justice-grâce-au-package-r-chartegraphique.sser-sser---ssm-justice",
    "href": "event/2025-12-01-network-day/index.html#automatisation-des-infos-rapides-justice-grâce-au-package-r-chartegraphique.sser-sser---ssm-justice",
    "title": "Troisième journée du SSPHub",
    "section": "Automatisation des Infos rapides Justice grâce au package R chartegraphique.sser (SSER - SSM Justice)",
    "text": "Automatisation des Infos rapides Justice grâce au package R chartegraphique.sser (SSER - SSM Justice)\n\nLe papier a été publié lors des journées de méthodologie statistique 2025.\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2025-12-network/4_SSER_autoIRJ.pdf\",\n  \"Télécharger les slides\"\n)"
  },
  {
    "objectID": "event/2025-12-01-network-day/index.html#le-package-torchtextclassifiers-un-cadre-unifié-pour-la-classification-de-texte-basé-sur-pytorch-et-pytorch-lightning-insee---ssplab",
    "href": "event/2025-12-01-network-day/index.html#le-package-torchtextclassifiers-un-cadre-unifié-pour-la-classification-de-texte-basé-sur-pytorch-et-pytorch-lightning-insee---ssplab",
    "title": "Troisième journée du SSPHub",
    "section": "Le package torchTextClassifiers, un cadre unifié pour la classification de texte basé sur PyTorch et PyTorch Lightning (Insee - SSPLab)",
    "text": "Le package torchTextClassifiers, un cadre unifié pour la classification de texte basé sur PyTorch et PyTorch Lightning (Insee - SSPLab)\n\n\n\n\n\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2025-12-network/5_TorchTextClassifier_Insee.pdf\",\n  \"Télécharger les slides\"\n)"
  },
  {
    "objectID": "event/2025-12-01-network-day/index.html#le-portail-httpsdata.ina.fr-produire-des-indicateurs-de-suivi-des-médias-grâce-à-lia",
    "href": "event/2025-12-01-network-day/index.html#le-portail-httpsdata.ina.fr-produire-des-indicateurs-de-suivi-des-médias-grâce-à-lia",
    "title": "Troisième journée du SSPHub",
    "section": "Le portail https://data.ina.fr/, produire des indicateurs de suivi des médias grâce à l’IA",
    "text": "Le portail https://data.ina.fr/, produire des indicateurs de suivi des médias grâce à l’IA\nCf. le portail https://data.ina.fr/\n\nfunction createButton(slides, message=\"Télécharger les slides\"){\n  const button = html`\n  &lt;p class=\"text-center\"&gt;\n    &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n      &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;${message}\n    &lt;/a&gt;\n  &lt;/p&gt;`\n  return button\n}"
  },
  {
    "objectID": "event/2025-04-09-API/index.html",
    "href": "event/2025-04-09-API/index.html",
    "title": "Atelier - Comment récupérer des données par API ?",
    "section": "",
    "text": "L’atelier a eu lieu le 9 avril 2025 (15h - 16h30), en présentiel à l’Insee et en distanciel pour les membres du réseau du SSP Hub. Environ 35 personnes ont participé de l’Insee (DG ou directions régionales), de différents services statistiques ministériels ou d’autres horizons. Merci à tous pour les échanges !\n\nSlides de la présentation\n\n\n\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"https://inseefrlab.github.io/ssphub-ateliers-slides/slides-data/api.html#/title-slide\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Voir les slides\n  &lt;/a&gt;\n&lt;/p&gt;`\n\n\n\n\n\n\n\n\nDocumentation de l’atelier & replay\nLe matériel lié à l’atelier, y compris le replay, est disponible ici. \n\n\nQuestions / contact\nSi vous avez la moindre question 🤨, n’hésitez pas à contacter 📧 contact-ssphub@insee.fr."
  },
  {
    "objectID": "event/2024-05-02-quarto/index.html",
    "href": "event/2024-05-02-quarto/index.html",
    "title": "Quarto : Une évolution de R Markdown pour des travaux statistiques reproductibles",
    "section": "",
    "text": "2 mai (15h - 16h30)\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://cderv.github.io/2024-quarto-evolution-rmd/#/title-slide\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Voir les slides\n  &lt;/a&gt;\n&lt;/p&gt;`"
  },
  {
    "objectID": "event/2024-05-02-quarto/index.html#christophe-dervieux-quarto-une-évolution-de-r-markdown-pour-des-travaux-statistiques-reproductibles",
    "href": "event/2024-05-02-quarto/index.html#christophe-dervieux-quarto-une-évolution-de-r-markdown-pour-des-travaux-statistiques-reproductibles",
    "title": "Quarto : Une évolution de R Markdown pour des travaux statistiques reproductibles",
    "section": "",
    "text": "2 mai (15h - 16h30)\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://cderv.github.io/2024-quarto-evolution-rmd/#/title-slide\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Voir les slides\n  &lt;/a&gt;\n&lt;/p&gt;`"
  },
  {
    "objectID": "event/2022-06-20-funathon/index.html",
    "href": "event/2022-06-20-funathon/index.html",
    "title": "Funathon de juin 2022",
    "section": "",
    "text": "Présentation\nEn 2022, une nouvelle édition a eu lieu les 20 et 21 juin, tournant cette fois autour de la thématique de l’environnement, et du changement climatique. Cette fois encore, 9 sujets ont été proposés, chaque fois accompagnés de notebooks d’explication (en R et en Python). Ceci a notamment permis de travailler sur :\n\nl’extraction de données de Twitter,\nl’analyse textuelle des données du grand Débat,\nla réalisation d’analyses graphiques autour de la montée des eaux,\nl’analyse des données issues de la base Ademe sur les logements,\nle machine learning à partir des données de la météo,\nles données satellites,\nl’utilisation de Fasttext,\nd’Elastic Search,\nenfin, 3 Master class ont également été proposées sur :\n\n\nla pertinence d’utiliser Python quand on connait R ;\n\n\nune initiation à Elastic ;\n\n\nL’utilisation du deep learning pour classifier des données issues d’images satellites.\n\n\n\nTous les niveaux d’expertise étaient les bievenus.\n# Détails L’ensemble du materiel associé, et les différents liens vers les master class, sont présents dans le repo du funathon."
  },
  {
    "objectID": "event.html",
    "href": "event.html",
    "title": "Evénements",
    "section": "",
    "text": "Françoise Bahoken et Nicolas Lambert, présentation de leur livre Cartographia\n\n\nLe 13 janvier (14h30 - 15h30), Françoise Bahoken et Nicolas Lambert nous ont présenté leur dernier livre…\n\n\n\n\n\n\n13 janv. 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nTroisième journée du SSPHub\n\n\nProgramme et modalités d’inscription à la 3e journée du réseau\n\n\n\n\n\n\n1 déc. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtelier - Comment récupérer des données sous format Parquet ?\n\n\nLe format Parquet est un format de données connaissant une popularité importante du fait de ses caractéristiques techniques (orientation colonne, compression…\n\n\n\n\n\n\n16 avr. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtelier - Comment récupérer des données par API ?\n\n\nLes API (Application Programming Interface) sont un mode d’accès aux données en expansion. Grâce aux API, l’automatisation de scripts est facilitée puisqu’il n’est plus…\n\n\n\n\n\n\n9 avr. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeuxième journée du SSPHub\n\n\nProgramme et modalités d’inscription à la 2e journée du réseau\n\n\n\n\n\n\n14 oct. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto : Une évolution de R Markdown pour des travaux statistiques reproductibles\n\n\nPour fiabiliser la production de documents construits en valorisant des données (tableaux, graphiques, etc.), RStudio (devenu Posit depuis) a construit il y a quelques…\n\n\n\n\n\n\n2 mai 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nEric Mauvière, “La dataviz pour donner du sens aux données et communiquer un message”\n\n\nLe 29 février (15h - 16h), Eric Mauvière nous fera une présentation, avec de nombreux exemples issus de la statistique publique, de la manière dont une visualisation de…\n\n\n\n\n\n\n29 févr. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremière journée du SSPHub\n\n\nReplay de la première journée de présentation du SSPHub\n\n\n\n\n\n\n29 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n“OCRisation, état de l’art et projets auxquels participe Teklia” par Christopher Kermorvant\n\n\nLe 29 mars de 15h à 16h nous recevons Christopher Kermorvant, chercheur spécialisé en OCRisation et fondateur de Teklia. Il nous fera un état de l’art de l’OCRisation puis…\n\n\n\n\n\n\n29 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation du projet Meta Academy - Carpentries\n\n\nPour favoriser l’adoption des langages R, Python et Git dans les administrations, le programme ModernStat piloté par l’OCDE et Statistics Canada, a lancé un projet…\n\n\n\n\n\n\n28 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation des packages R et Python pour accéder à l’open data de l’Insee\n\n\nL’Insee met à disposition ses données par le biais d’API ou par son site web. Pour faciliter la…\n\n\n\n\n\n\n13 févr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation de gridviz par Julien Gaffuri\n\n\nEvénement de présentation de gridviz par Julien Gaffuri (Eurostat)\n\n\n\n\n\n\n20 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvénement de clôture de la saison 1 du programme 10%\n\n\nLe 5 décembre, a lieu l’événement de clotûre de la saison 1 du programme 10%. Au programme, restitution des [projets portés cette…\n\n\n\n\n\n\n5 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation d’Observable par Nicolas Lambert\n\n\nobservable est la nouvelle plateforme de dataviz réactive. Initiée par Mike Bostock (créateur de D3.js), ce réseau social de la dataviz a pour…\n\n\n\n\n\n\n16 nov. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunathon de juin 2022\n\n\nPrésentation du deuxième Funathon du SSPLab organisé le 20 juin 2022 autour de 9 sujets, en R et en Python.\n\n\n\n\n\n\n19 juin 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunathon de juin 2021\n\n\nPrésentation du premier Funathon du SSPLab organisé le 21 juin 2021 autour de 8 sujets, en R et en Python, à partir de données Airbnb\n\n\n\n\n\n\n20 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéminaire - la méthodologie des appariements\n\n\nSéminaire de Méthodologie statistique et de sciences des données du 12 avril 2021 : revue des méthodes d’appariement et des principaux concepts et quelques exemples de…\n\n\n\n21 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéminaire - nouvelles approches pour coder dans une nomenclature : machine learning et autocomplétion\n\n\nSéminaire de Méthodologie statistique et de sciences des données du 14 janvier 2020 : innovation et expériences pratiques récentes pour réaliser des tâches de classification…\n\n\n\n14 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéminaire - Big Data et statistiques publiques : questions de méthodes et lancement du SSPLab\n\n\nSéminaire de Méthodologie statistique et de sciences des données du 30 novembre 2016 : enjeu technique et statistique de l’utilisation de la science des données pour la…\n\n\n\n30 nov. 2016\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "course/ssplab-geomatique/index.html",
    "href": "course/ssplab-geomatique/index.html",
    "title": "Géomatique appliquée à la statistique",
    "section": "",
    "text": "La cartographie thématique et les statistiques ont émergé progressivement au cours du 19e siècle du besoin des états naissants de se connaître, les deux disciplines ayant rapidement cultivé une forte interdépendance. La diffusion des statistiques territoriales a favorisé l’essor des représentations cartographiques modernes. En contrepartie, la cartographie a nourri les réflexions sur la prise en compte de l’espace dans l’étude des faits sociaux.\nLes progrés de l’informatique ont décuplé la capacité des acteurs publics et privés à diffuser des cartes statistiques. Il n’a jamais été aussi simple de produire et de communiquer une information statistique à l’aide d’une carte. Ce constat est néanmoins à tempérer, car la cartographie statistique reste toujours une pratique d’initiés. Une présentation simple des outils géomatiques récents qui permettent de stocker, traiter et diffuser l’information spatiale pour le statisticien est donc utile.\nCe document propose une présentation unifiée des concepts géomatiques, qui pourra être appliquée quelque soit l’outil choisi. La présentation est cependant restreinte aux données vectorielles et ne traite pas des images (données dites rasters). Leur traitement nécessite en effet des outils adaptés, tels que la reconnaissance d’image, qui dépasse le cadre de ce document"
  },
  {
    "objectID": "blog/retrospective2022/index.html",
    "href": "blog/retrospective2022/index.html",
    "title": "Rétrospective de l’année 2022",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit\nLa fin de l’année est généralement synonyme de bétisiers, best of ou rétrospectives personnalisées qui nous permettent de nous rappeler les événements marquants de l’année.\nPour célébrer la fin de l’année 2022, la newsletter de janvier adopte un format un peu spécial pour proposer, en deux temps, deux rétrospectives.\nCette première newsletter revient sur les principaux événements de l’année 2022 dans le monde de la data science. La seconde newsletter proposera une rétrospective quantitative sur le réseau des data scientists de la statistique publique, à la manière des rétrospectives personnalisées de nos applications préférées.\n\nLes IA créatrices de contenu à l’honneur\nSi l’année 2022 a été particulièrement riche dans le domaine de la data science, c’est principalement grâce à deux coups médiatiques d’OpenAI, à savoir Dall-E et ChatGPT.\nCes deux outils ont beaucoup fait parler d’eux, au-delà de la sphère traditionnelle de la data science. Le buzz a été intense sur Twitter ou sur Mastodon, le réseau social dont le nombre d’utilisateurs a nettement augmenté en réaction au rachat de Twitter par Elon Musk en fin d’année.\n\nCes innovations, parce qu’elles pourraient avoir des effets à long terme sur la manière dont le grand public appréhende l’intelligence artificielle, ont beaucoup intéressé les médias traditionnels, notamment Le Monde, The Economist et sa “Nouvelle Frontière” ou le Guardian qui s’interroge sur la nature des tâches que l’intelligence artificielle pourra remplacer à terme : procédurales et régies par des règles bien définies ou bien également des activités nécessitant de la créativité et des capacités d’analyse ?\nPour une fois, il ne s’agit donc pas de souligner exclusivement les limites de ces modèles voire leurs dérives (deep fake, biais racistes…) mais aussi de s’enthousiasmer sur leur potentiel créatif. Il est difficile de rester insensible à certaines des créations artistiques des modèles Dall-E, Stable Diffusion, Midjourney et consorts ou de résister à la tentation de tester la capacité de ChatGPT à répondre à des questions complexes Les chercheurs, et pas des moindres (notamment Andrew Ng ou Gaël Varoquaux) se sont également saisis de cette question et ont souligné les biais de raisonnement et excès de confiance de ces IA.\n\n\n\nhttps://github.com/Stability-AI/stablediffusion\n\n\nSi vous désirez utiliser Python de manière créative pour générer du contenu avec Stable Diffusion, vous pouvez consulter ce tutoriel qui fonctionne sur le SSPCloud ou sur Google Colab.\n\n\n\nLe succès des modèles de diffusion\nCes IA génératrices de contenu reposent toutes, à plusieurs niveaux, sur des réseaux de neurone.\nLe premier étage de la fusée est un modèle de langage (large language model) qui synthétise un langage en un ensemble complexe de paramètres. Les plus connus sont BERT et GPT-3. L’inflation dans le nombre de paramètres n’est pas prête de s’arrêter. Si les ressources nécessaires à entraîner en 2018 le modèle BERT (110 millions de paramètres) avaient déjà été critiquées en raison de leur coût financier et environnemental, cette complexité a encore augmenté depuis. Le modèle GPT-3, sorti en 2020, et qui sert de base à Dall-E et ChatGPT intègre 175 milliards de paramètres. Un chiffre qui apparaît minime par rapport aux 17O trillions de paramètres attendus pour le modèle GPT-4 en 2023.\nEn ce qui concerne les IA créatrices de contenu visuel, le deuxième étage de la fusée est un modèle d’analyse d’image qui apprend à associer des images à une description textuelle afin de détecter des structures communes entre des mots ou des séquences de mots et des formes sur des images. Il s’agit de déconstruire une forme en une structure minimale de pixels qui permet de l’identifier.\n\n\n\nSource: Sebastian Raschka\n\n\nEnsuite, pour générer une image à partir d’une description inédite intervient le modèle de diffusion qui reconstruit une image à partir du mélange de l’ensemble des pixels qui traduisent les concepts principaux d’une instruction. L’une des explications les plus pédagogiques pour comprendre le fonctionnement de ces modèles vient du Washington Post.\nSinon, on peut demander directement à ChatGPT de nous expliquer:\n\n\n\nL’actualité dans le monde du deep learning\nSi le succès d’estime de ces IA génératrices consacre les modèles de diffusion, l’année du deep learning ne se réduit pas à cette actualité.\nL’année a notamment été marquée par la compétition entre les librairies et écosystèmes TensorFlow, développé par Google, et PyTorch projet initié par Facebook/Meta. PyTorch, plus récent, bénéficie d’une dynamique plus ascendante que TensorFlow. Le succès d’HuggingFace, plateforme de mise à disposition de modèles, et où les implémentations PyTorch sont systématiques alors que celles en TensorFlow sont rares a participé à la diffusion de PyTorch.\nPreuve du succès de PyTorch, cet écosystème est dissocié de Meta depuis septembre afin de devenir un outil généraliste géré par la Linux Foundation. À l’inverse, Google semble se détacher graduellement de TensorFlow pour privilégier son nouvel écosystème JAX.\n\n\nDu changement côté RStudio\nDepuis quelques années, RStudio a fait le choix de devenir un écosystème de data science généraliste et non plus exclusivement attaché au langage R.\nCette année, cela s’est traduit par la publication, très commentée, de Quarto qui vise à proposer, dans de nombreux langages de programmation, des fonctionalités de publications reproductibles équivalentes à l’un des produits emblématiques de RStudio, à savoir R Markdown. Rien de mieux pour être convaincu de l’intérêt de cet outil que d’observer la galerie d’exemples, d’explorer la documentation très riche, ou de tester soi-même sur un exemple. Cet été, RStudio a également annoncé que Shiny, un autre produit emblématique, serait maintenant disponible sous Python, comme alternative à Dash ou Streamlit.\nL’année 2022 a été l’occasion, pour RStudio, d’un autre changement, symbolique celui-ci. Afin de détacher son image du langage R, l’entreprise a en effet changé de nom pour devenir posit. L’entreprise n’a néanmoins pas abandonné son activité foisonnante dans R puisque Hadley Wickham a commencé à publier de nouveaux chapitres pour une nouvelle édition augmentée de l’ouvrage de référence R For Data Science.\n\n\nObservable devient un incontournable dans le monde de la dataviz\nPour permettre des visualisations interactives, cela fait plusieurs années que JavaScript est un incontournable et que le web assembly retient de plus en plus d’attention.\nLes journaux traditionnels utilisent ainsi de plus en plus le data scrollytelling , cette technique de narration qui consiste à présenter des informations sous forme de récit interactif, en utilisant une combinaison de texte et de graphiques qui apparaissent et disparaissent en fonction des actions du lecteur. L’un des exemples les plus réussis des dernières années a sans doute été la visualisation du New York Times “How the virus got out”. Cette approche a également été adoptée par le Ministère de l’Agriculture pour diffuser les chiffrés clés du recensement agricole. Nos voisins anglais ne sont pas en reste puisque les derniers résultats du recensement sont proposés sur un site web remarquable de fluidité.\nAfin de permettre une diffusion accrue de visualisations en JavaScript, Mike Bostock, déjà créateur de la librairie de dataviz de référence D3.js, est à l’origine de la plateforme observable, sorte de Github de la dataviz permettant du partage et de la réutilisation de notebooks réactifs. En cette année 2022, la plateforme a connu un véritable boom et est devenu un incontournable dans le domaine. L’une des raisons est l’ajout de fonctionalités qui permettent d’étendre le public cible au delà des développeurs web, déjà accoutumés à Javascript. Parmi les fonctionalités les plus remarquables, la possibilité depuis Novembre d’utiliser des requêtes SQL grâce à DuckDB permet aux habitués de R ou de Python de retrouver des manipulations auxquels ils sont habitués. La librairie Plot offre une grammaire proche de ggplot2.\nLa communauté des cartographes a été particulièrement active sur Observable, notamment à l’occasion du #30daymapchallenge. Nicolas Bertin (neocarto), dont on ne peut que recommander l’introduction à Observable faite pour le réseau, ou Eric Mauvière font partie des comptes à suivre dans la communauté francophone.\nObservable, en tant que langage construit sur JavaScript, est également disponible pour les utilisateurs de Quarto, ce qui permet de mettre à disposition des visualisation réactives sans passer nécessairement par la plateforme observablehq.com pour mettre à disposition des visualisations réactives, ce qui constitue une alternative intéressante aux applications qui nécessitent un serveur en arrière plan, comme Shiny ou Dash.\n \n\n\nLes autres actualités en France\nLe rapport du conseil d’État pour la construction d’une IA de “confiance” a donc été publié en une année 2022 où les avancées techniques des dernières années commencent à être accessibles grâce à des outils plus grand public, ce qui va nécessairement soulever des enjeux éthiques et juridiques.\nLe projet Onyxia, qui vise à proposer une infrastructure de data science à l’état de l’art pour data scientists, a organisé son deuxième Openlab. L’occasion de revenir sur le projet, sa philosophie, ses dernières avancées mais aussi d’échanger sur les perspectives de réutilisation dans de multiples environnements et de nouer des partenariats qui permettront au projet de grandir encore en 2023."
  },
  {
    "objectID": "blog/polars/index.html",
    "href": "blog/polars/index.html",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "",
    "text": "Le concept de dataframe est central pour le data scientist qui manipule des données tabulaires. En Python, Pandas est la solution de loin la plus populaire. En moyenne, le package est téléchargé 4 millions de fois par semaine, depuis des années.\nUn petit nouveau apporte un vent de fraîcheur dans le domaine : Polars.\nSes atouts ? D’excellentes performances et une expressibilité qui le rapproche d’un dplyr.\nCe post de blog revient sur les principaux atouts de Polars, sans vouloir être exhaustif. Un notebook illustrant les principales fonctionnalités du package vise à le compléter :"
  },
  {
    "objectID": "blog/polars/index.html#lévaluation-lazy",
    "href": "blog/polars/index.html#lévaluation-lazy",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "L’évaluation lazy",
    "text": "L’évaluation lazy\nPlusieurs éléments expliquent cette rapidité.\nEn premier lieu, Polars est conçu pour optimiser les requêtes : grâce au mode lazy (“paresseux”), on laisse la possibilité au moteur d’analyser ce qu’on souhaite faire pour proposer une exécution optimale (pour la lecture comme pour la transformation des jeux de données). La lazy evaluation est une méthode assez commune pour améliorer la vitesse des traitements et est utilisée, entre autres, par Spark.\nDu fait de la lazy evaluation il est ainsi possible, par exemple, si un filtre sur les lignes arrive tardivement, de le remonter dans l’ordre des opérations effectuées par Python afin que les opérations ultérieures ne soient effectuées que sur l’ensemble optimal de données. Ces optimisations sont détaillées dans la documentation officielle."
  },
  {
    "objectID": "blog/polars/index.html#lecture-optimisée-des-fichiers",
    "href": "blog/polars/index.html#lecture-optimisée-des-fichiers",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "Lecture optimisée des fichiers",
    "text": "Lecture optimisée des fichiers\nL’utilisateur Pandas est habitué à lire du CSV avec pd.read_csv. Avec Polars, il existe deux manières, très ressemblantes de le faire.\nimport polars as pl\n\n# Création d'une requête\nq = (\n    pl.scan_csv(\"iris.csv\") # Lecture lazy\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .groupby(\"species\")\n    .agg(pl.all().sum())\n)\n\n# Exécution de la requête\ndf = q.collect()\nAvec cette syntaxe, les connaisseurs de Pyspark retrouveront facilement leurs petits (ours 🐻).\nOn peut toujours lire de manière plus directe (en mode eager, “impatient”) en utilisant la fonction read_csv, et ensuite appliquer des transformations optimisables en glissant habilement lazy :\ndf = pl.read_csv(\"iris.csv\")\n\ndf_res = df.lazy() # ←  ici :)\n  .filter(pl.col(\"sepal_length\") &gt; 5)\n  .groupby(\"species\")\n  .agg(pl.all().sum())\n  .collect()\nPolars fonctionne également très bien avec le format Parquet, comme illustré dans le notebook qui accompagne ce post."
  },
  {
    "objectID": "blog/polars/index.html#parallélisation",
    "href": "blog/polars/index.html#parallélisation",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "Parallélisation",
    "text": "Parallélisation\nPolars parallélise les traitements dès que cela est possible, notamment dans le cas d’agrégation. Chaque coeur se charge d’une partie de l’agrégation et envoie des données plus légères à Python qui va finaliser l’agrégation.\n\n\n\nParallélisation\n\n\nIllustration du principe de la parallélisation\nSur les systèmes proposant de nombreux coeurs, cela peut faire gagner beaucoup de temps."
  },
  {
    "objectID": "blog/polars/index.html#des-couches-basses-à-la-pointe",
    "href": "blog/polars/index.html#des-couches-basses-à-la-pointe",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "Des couches basses à la pointe",
    "text": "Des couches basses à la pointe\nEnfin, le choix d’utiliser à la fois le format de représentation en mémoire Arrow et le langage Rust pour le coeur de la bibliothèque n’est pas étranger à cette performance."
  },
  {
    "objectID": "blog/polars/index.html#calculs-out-of-memory",
    "href": "blog/polars/index.html#calculs-out-of-memory",
    "title": "Polars, une alternative fraîche à Pandas",
    "section": "Calculs out of memory",
    "text": "Calculs out of memory\nPolars travaille vite mais présente aussi l’avantage de lire naturellement des jeux de données hors des limites de la mémoire de l’ordinateur grâce à sa capacité de lire en flux (méthode qu’on appelle le streaming).\n# La même requête que tout à l'heure va lire le fichier \"en flux\"\ndf = q.collect(streaming=True)\nDe plus, Polars lit nativement les fichiers Parquet qui par ses propriétés permet d’aller beaucoup plus vite que le CSV !"
  },
  {
    "objectID": "blog/onyxia/index.html",
    "href": "blog/onyxia/index.html",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "",
    "text": "Onyxia est un logiciel open source développé par l’Insee (disponible sur Github ) permettant de fournir un environnement de traitement de données à l’état de l’art. Principalement conçu pour permettre le travail interactif des data scientists, l’expérience fournie avec Onyxia favorise également la reproductibilité des travaux et leur mise en production.\nLe logiciel Onyxia est installé par des organisations souhaitant créer un datalab, c’est-à-dire une plateforme interactive de traitement de données. Ces organisations ont toutes le point commun de vouloir construire une plateforme qui embrasse les technologies cloud que sont la conteneurisation et le stockage objet tout en mettant à disposition celles-ci dans un environnement user-friendly où l’interconnexion entre ces différentes briques est gérée de manière cohérente. Les technologies cloud native sont devenues indispensables dans l’écosystème de la donnée, du fait d’une meilleure gestion des ressources de traitement ou de la capacité à créer un environnement parfaitement reproductible pour une mise en production accélérée.\nCe post de blog a pour objectif de présenter la raison d’être d’Onyxia, sa génèse et les solutions qu’apporte cette infrastructure à des irritants classiques des projets novateurs de data science."
  },
  {
    "objectID": "blog/onyxia/index.html#contexte",
    "href": "blog/onyxia/index.html#contexte",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Contexte",
    "text": "Contexte\nL’écosystème de la data science est en mouvement accéléré depuis 10 ans et le rôle du data scientist dans les organisations valorisant de la donnée évolue continuellement (Davenport et Patil 2022). Les data scientists modernes sont amenés à utiliser de plus en plus de langages et doivent être capables de maîtriser plusieurs architectures informatiques. La frontière est ainsi moins nette que par le passé entre statisticiens et informaticiens. De plus, les innovations récentes dans le monde du développement logiciel, notamment l’adoption massive de l’approche DevOps - approche qui consiste à automatiser la production de livrables dès la conception du prototype - a également fait évoluer les pratiques des data scientists.\nCe besoin de ressources informatiques croissantes, de flexibilité dans le prototypage de solutions informatiques et l’évolution des pratiques consistant à mettre à disposition en continu des livrables ont eu des implications importantes sur les architectures informatiques dominantes dans l’écosystème de la donnée. Pour répondre au besoin croissant de puissance de traitement, les serveurs partagés, organisés sous forme de clusters, se sont développés dans de nombreuses organisations. Après avoir connue son heure de gloire au début des années 2010, l’infrastructure HDFS (Hadoop Distributed File System), qui reposait sur des clusters où les données et la puissance de traitement étaient distribuées et collocalisées, a laissé place à des infrastructures plus scalables, basées sur l’approche de la conteneurisation."
  },
  {
    "objectID": "blog/onyxia/index.html#de-hdfs-à-la-conteneurisation",
    "href": "blog/onyxia/index.html#de-hdfs-à-la-conteneurisation",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "De HDFS à la conteneurisation",
    "text": "De HDFS à la conteneurisation\n\n\n\n\n\n\nNote\n\n\n\nCette partie plus technique développe des éléments pour comprendre le succès récent des infrastructures conteneurisées.\nElle pourra intéresser le lecteur curieux sur les fondements des infrastructures cloud modernes mais n’est pas nécessaire à la compréhension générale de l’article.\n\n\nLa conteneurisation, qui repose sur l’idée que les serveurs de stockage de la donnée peuvent être dissociés de ceux effectuant les traitements, sert de fondement aux principales plateformes cloud actuelles fournissant des services à la demande.\nCe nouveau paradigme part de deux constats. Le premier est que les échanges de données entre les noeuds d’un serveur sont aujourd’hui peu coûteux. Avec des flux réseaux suffisants et une technologie performante, il est donc possible d’échanger à un coût modéré de gros volumes de données au sein d’une infrastructure. Le deuxième constat est que la maintenance d’une infrastructure conteneurisée, faite pour être très malléable, est plus légère que celle d’une infrastructure basée sur des machines virtuelles ou sur les infrastructrures calibrées pour l’analytique big data comme HDFS reposant sur la collocalisation des données et des traitements1.\nLes données étant stockées sur des serveurs différents de ceux exécutant les traitements, l’accès à celles-ci se fait à travers des API qui permettent de traiter le système de stockage distant comme un système de fichiers classique. Onyxia a adopté une implémentation open source du système de stockage S3 appelée MinIO.\nEn ce qui concerne le traitement des données, le fait d’utiliser un système de conteneurs, c’est-à-dire une configuration logicielle portable minimaliste prête à l’emploi (par opposition aux machines virtuelles qui impliquent un système d’exploitation complet), offre une grande liberté sur le choix des logiciels de traitement. De nombreuses technologies open source devenues standards dans le monde de la data science (Jupyter, RStudio, ElasticSearch…) existent déjà sous cette forme et peuvent ainsi être adoptées dans une telle infrastructure pour fournir des services prêts-à-l’emploi pour les data scientists. La mise en musique de toutes ces petites boites auto-suffisantes, notamment l’optimisation des ressources concurrentes sur un serveur, est permise par la technologie d’orchestration Kubernetes.\n\n\n\nCentralisation des ressources par Onyxia\n\n\n\n\n\n\n\n\n\n\nNotePlus de détails pour comprendre le changement de paradigme vers la conteuneurisation 👇\n\n\n\n\n\nLes infrastructures big data reposent sur le principe du cluster (grappe) informatique. Des serveurs sont connectés entre eux, ce qui forme de manière imagée une grappe. Cette interconnexion de plusieurs serveurs entre eux peut se faire au niveau :\n\ndu stockage : les données volumineuses ne sont pas stockées sur un seul serveur mais au contraire réparties ;\ndu traitement : les calculs sont effectués par blocs sur plusieurs serveurs et le résultat de ceux-ci est ensuite transmis à un serveur maître.\n\nLe système Hadoop Distributed File System a été pensé pour tirer parti de l’algorithme de traitement parallélisé MapReduce proposé en 2004 par Google. Les fichiers volumineux sont fractionnés et répartis sur plusieurs serveurs.\n\nFonctionnement d’une architecture MapReduce (source: Datascientest)\nLa spécificité de l’architecture HDFS est que non seulement le stockage est distribué mais également aussi la puissance de traitement associée. On parle à ce propos de collocalisation : les traitements ont lieu sur les mêmes serveurs que ceux où sont stockés les données. Cela permet de réduire les mouvements de données (shuffle dans l’image ci-dessus) qui sont coûteux du point de vue de la performance. Cette collocalisation a permis au système HDFS de devenir, au début de la décennie 2010, le paradigme dominant. En tirant parti de la parallélisation permise par des langages très efficaces comme Spark tout en limitant les échanges réseaux pouvant faire perdre en performance, cette architecture a attiré au-delà de l’écosystème du big data.\nLe système HDFS présente néanmoins certaines limites qui expliquent sa perte de succès avec l’émergence d’un nouveau paradigme plus flexible.\nEn premier lieu, ce système nécessite beaucoup de ressources du fait de son design. Comme les traitements sont lourds et partagés pour des usages concurrents, les noeuds constituant le cluster peuvent subir des arrêts à cause de surcharge des ressources. Pour tenir compte de la nature instable de cette infrastructure big data, les fichiers sont dupliqués. Ainsi, lors d’une erreur sur le serveur générant un arrêt du nœud (par exemple à cause de traitements trop gourmands), les traitements sur l’ensemble des données sont sécurisés évitant également la perte partielle ou totale de ces dernières.\nL’implication est que les données, déjà volumineuses, sont dupliquées plusieurs fois impliquant des architectures assez monumentales. Si la duplication de la donnée n’est pas en soi choquante afin d’éviter la perte de données, cela a un effet pervers dans un système de collocalisation. A chaque ajout de noeuds pour le stockage de données, il est également nécessaire d’ajouter des ressources pour les traiter. Il est donc compliqué de décorréler l’ajout de ressources de stockage et de traitement. Cette absence de flexibilité est pénalisante dans un monde où les données sont mises à jour fréquemment et où les technologies de traitement, donc les besoins associés, évoluent rapidement. Les infrastructures HDFS sont donc lourdes à changer, que ce soit pour ajouter des ressources ou faire évoluer les distributions logicielles présentes dessus.\nLe deuxième facteur qui a favorisé le changement de paradigme est l’amélioration des échanges réseaux. Il n’est plus aussi coûteux que par le passé de transférer des volumes importants de données au sein d’une infrastructure. Cela facilite la décorrélation entre environnement de stockage et de traitement.\nCette séparation des environnements de stockage et de traitement permet alors d’adopter pour chacun les technologies les plus performantes. Dans le domaine du stockage, celle qui a rencontré le plus de succès est le système de stockage S3 développé par Amazon. L’implémentation open source du système S3 est MinIO, utilisée par Onyxia.\nDans le domaine du traitement, la technologie la plus performante dépend de la nature de la tâche réalisée. Selon qu’on désire effectuer de la recherche textuelle, des visualisations de données ou de l’analyse d’image, on ne va pas vouloir utiliser la même technologie. Pour mettre à disposition des logiciels sur un serveur, il existe principalement deux approches concurrentes.\nLa première repose sur le principe des machines virtuelles. Cette approche n’est pas nouvelle et de nombreuses organisations ont proposé ou proposent encore ce type d’infrastructures pour des serveurs collectifs de traitement. Cette approche est néanmoins lourde : elle nécessite un système d’exploitation complet dont il faudra ensuite adapter la configuration lors de l’installation de chaque logiciel. Plusieurs logiciels coexistent donc dans ce système d’exploitation même si un seul, par exemple, Python, est utilisé. Les machines virtuelles sont des infrastructures assez polluantes puisque pour faire fonctionner un système d’exploitation dans son ensemble, il est nécessaire de mobiliser des ressources plus importantes que celles seulement nécessaires aux traitements. De plus, la configuration d’un système d’exploitation, et notamment, la gestion de la dépendance de multiples logiciels à des configurations systèmes qui peuvent ne pas correspondre, n’est pas triviale. Il est donc lourd de faire évoluer une infrastructure reposant sur des machines virtuelles. L’absence de flexibilité d’une infrastructure reposant sur le principe des machines virtuelles est pénalisante dans un écosystème mouvant comme celui de la data science, où une partie importante du travail de prototypage consiste à tester plusieurs technologies pour déterminer celle s’intégrant le mieux dans un processus de traitement de données.\nLe système de la conteneurisation a justement été pensé pour cela : plutôt qu’installer de nombreuses librairies au niveau du système, pour une fraction d’utilisateurs limitée à chacune, il est plus intéressant de créer des environnements complets qui vont exister de manière conjointe. Chaque framework va être construit comme un conteneur autosuffisant avec un système d’exploitation minime et un nombre minimal de couches de configurations supplémentaires. Un framework est livré sous la forme d’une image Docker, une technologie qui permet d’empaqueter un logiciel et ses dépendances sous la forme de boites minimalistes et les mettre à disposition facilement pour une réutilisation. Il existe par exemple des images Docker pour pouvoir utiliser RStudio, Jupyter, VSCode avec des configurations minimales afin d’exécuter du Python ou du R. A partir de celles-ci, l’utilisateur qui désire des configurations supplémentaires peut ajouter les couches qui lui sont utiles.\nMais les images Docker ne se réduisent pas à la mise à disposition d’environnements de développement. Une partie des technologies les plus appréciées de l’écosystème de la data science sont également livrées sous forme d’images Docker. Par exemple, le moteur de recherche ElasticSearch, très utilisé pour la recherche textuelle, peut être empaqueté dans une image Docker. Le logiciel Onyxia propose dès lors, dans un catalogue vivant, un certain nombre de logiciels très utiles pour les data scientists ayant fait l’objet d’un tel empaquetage. Les nombreuses images Docker servant à créer des services pour les data scientists sont disponibles en open source sur Github.\nPour organiser sur un serveur la coexistence de multiples utilisateurs de services gourmands en ressource, la solution Kubernetes fait aujourd’hui office de référence. Entre sa création en 2014 et aujourd’hui, cette solution d’orchestration, c’est-à-dire de gestion d’une infrastructure, est devenue incontournable. Outre son allocation dynamique des ressources, elle permet de transformer facilement le livrable d’une chaine de traitement en application disponible en continu. Ceci est particulièrement adapté dans un contexte de diversification des livrables fournis par les data scientists (API, application web, modèle…) et d’adoption d’une démarche DevOps voire MLOps."
  },
  {
    "objectID": "blog/onyxia/index.html#la-solution-onyxia",
    "href": "blog/onyxia/index.html#la-solution-onyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "La solution Onyxia",
    "text": "La solution Onyxia\n\nD’un cloud de l’administration à un logiciel ouvert\nPour permettre aux data scientists des administrations françaises de bénéficier de technologies cloud sans être dépendant d’un fournisseur de service privé, l’équipe innovation de l’Insee a eu l’idée de créer un datalab basé sur la philosophie de la conteneurisation en mobilisant exclusivement des composants open-source.\nCe datalab, né à l’Insee en 2018, a été ouvert à l’administration publique sous la forme d’une instance https://www.sspcloud.fr/ à condition d’utiliser des données ouvertes. En plus des agents déjà en poste dans l’administration, cette infrastructure sert depuis deux ans à former les élèves de l’ENSAE et de l’ENSAI dans le cadre de leur formation en data science.\nDébut 2023, ce sont plus de 3000 agents et étudiants qui sont inscrits sur cette infrastructure avec, en moyenne, 300 utilisateurs hebdomadaires. L’infrastructure de traitement propose 10 TB de RAM, 1100 CPU disponibles et 34 GPU. La capacité de stockage associée est de 150 TB.\nPour les utilisations internes de données plus sensibles, l’équipe innovation de l’Insee a rendu disponible le code source derrière le SSP Cloud dans le cadre d’un logiciel nommé Onyxia (https://www.onyxia.sh/). Ce logiciel est pensé comme un kit qui peut être installé sur un cluster Kubernetes, technologie détaillée précédemment."
  },
  {
    "objectID": "blog/onyxia/index.html#onyxia-en-bref",
    "href": "blog/onyxia/index.html#onyxia-en-bref",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Onyxia en bref",
    "text": "Onyxia en bref\n\nOnyxia propose principalement deux composants de valeur :\n\nune interface web qui agit comme la porte d’entrée du data scientist sur son datalab, lui facilitant l’accès aux technologies cloud et lui permettant de démarrer ses environnements de traitement de la donnée. L’interface ergonomique permet aux utilisateurs de données néophytes de démarrer des services standardisés sans se préoccuper de la configuration mais aussi aux data scientists plus aguerris de bénéficier de vastes possibilités de personnalisation du service.\ndes catalogues de logiciels : une petite vingtaine de services interactifs dont les plus utilisés sont RStudio, Jupyter, VScode, une quinzaine de services spécialisés dans les bases de données (Postgres, ElasticSearch…), 5 services d’automatisation (MLflow…) et 2 services de dataviz (Redash et Superset)\n\n\nLe catalogue des services disponibles dans Onyxia.\n\nCes deux composants peuvent être adaptés en fonction des besoins internes de chaque organisation. Tous les services interactifs sont automatiquement connectés à l’espace de stockage S3, et au coffre de secret Vault. La gestion des droits d’accès aux données stockées dans l’espace de stockage S3 ou dans des services de bases de données (ElasticSearch, PostGreSQL…) est automatisée afin que chaque service puisse accéder aux données sur lesquelles l’utilisateur détient des droits.\nOnyxia étant un ensemble malléable de logiciels conteneurisés, il est possible de ne pas adopter l’ensemble des services proposés par l’équipe de l’Insee qui maintient Onyxia. Il est également possible de changer certaines des briques de base pour l’adapter à des éléments d’infrastructure interne. Par exemple, il est possible d’adapter la destination du service de stockage ou les configurations des environnements data science pour l’adapter à des ressources."
  },
  {
    "objectID": "blog/onyxia/index.html#linterface-et-les-services-proposés-par-onyxia",
    "href": "blog/onyxia/index.html#linterface-et-les-services-proposés-par-onyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "L’interface et les services proposés par Onyxia",
    "text": "L’interface et les services proposés par Onyxia\n\n\nOnyxia offre des marges de manoeuvre sur l’interface\n\n\n\nL’une des principales forces d’Onyxia est d’offrir une multiplicité de services différents avec une interconnexion entre eux gérée de manière cohérente.\nLes conteneurs sont démarrés comme des services à la demande et la configuration automatique de ceux-ci permet d’assurer aux data scientists l’accès aux données disponibles dans des espaces de stockage ou des bases de données créées par l’utilisateur.\nLe catalogue de services se présente par le biais d’un formulaire ergonomique où l’utilisateur choisit la brique qu’il désire utiliser:\n\nLes data scientists et statisticiens n’ont donc pas besoin de connaître les détails du fonctionnement des briques techniques d’Onyxia pour utiliser la plateforme. Les éléments techniques comme la connexion au système de stockage sont, par défaut, déjà configurés :\n\nL’interface ergonomique permet de paramétrer certaines configurations si besoin, notamment les ressources à disposition du conteneur. Néanmoins l’allocation dynamique des ressources offre déjà de la flexibilité :\n\nL’utilisateur a accès à l’ensemble des services qu’il a ouvert depuis une page dédiée :\n\nLes services interactifs comme Jupyter, VSCode ou RStudio permettent alors à l’utilisateur d’accéder à une interface pour exécuter des traitements Python ou R.\n\nL’accès aux données peut se faire depuis la ligne de commande (via un utilitaire Minio Client) ou par un package Python ou R dédié qui permet de traiter le système de stockage distant comme un système local. Les traitements sont exécutés sur les serveurs de la plateforme qui héberge les notebooks, indépendamment de la machine par laquelle l’utilisateur accède au service. Par exemple, dans le cas du SSPCloud, les traitements sont exécutés depuis des serveurs hébergés à l’Insee."
  },
  {
    "objectID": "blog/onyxia/index.html#la-communauté-onyxia",
    "href": "blog/onyxia/index.html#la-communauté-onyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "La communauté Onyxia",
    "text": "La communauté Onyxia\nTous les composants sont proposés en open source par l’Insee ce qui permet de fédérer une communauté d’utilisateurs et de développeurs de ce produit. Il s’agit d’un bel exemple de mutualisation au sein de l’État et au delà. Les dépôts peuvent être retrouvés sur le Github de l’équipe innovation (celui de l’interface web, celui des images pour la data-science…). La communauté peut proposer de nouveaux services dans le catalogue.\nCette approche bottom up a déjà permis d’adapter des services aux besoins des utilisateurs ou d’améliorer la solution grâce à des retours des ré-utilisateurs d’Onyxia."
  },
  {
    "objectID": "blog/onyxia/index.html#les-plateformes-basées-sur-onyxia",
    "href": "blog/onyxia/index.html#les-plateformes-basées-sur-onyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Les plateformes basées sur Onyxia",
    "text": "Les plateformes basées sur Onyxia\nLa plateforme d’origine, le SSPCloud, est ouverte à tous les agents de l’État et à plusieurs écoles. Celle-ci est exclusivement limitée à l’exploitation de données open data. Cette stratégie d’offreur de services de traitement sur l’open data permet de montrer l’expertise de l’Insee sur les sujets data science.\nLes principaux usages de cette plateforme sont les suivants :\n\nla formation ;\nl’organisation de hackathons ;\nla mise à disposition de services innovants et visualisations utilisant de l’open data ;\n\nGrâce à la mise à disposition de la solution Onyxia sur Github, il est néanmoins possible d’adapter cette plateforme pour des datalab internes, sur données plus sensibles.\nL’Insee n’est donc désormais plus seul et fédère de nombreux acteurs autour de son projet. Fin 2021, Eurostat a été la première organisation en dehors de l’Insee à choisir Onyxia pour construire son Cloud Agnostic Data Lab. Expertise France pour le projet DATAFID a fait le choix d’Onyxia tout comme le CASD, le GENES ou encore le BercyHub avec le projet Nubonyxia.\nD’autres organisations sont plus dans une phase de POC ou d’étude : l’INS norvégien, Pole Emploi, Data4Good, le ministère de l’Intérieur, le ministère de la Justice, l’Inria…\nDans le cadre du TOSIT, association qui réunit de gros acteurs publics et privés autour de solutions open source, un certain nombre d’entreprises s’intéressent à Onyxia."
  },
  {
    "objectID": "blog/onyxia/index.html#vidéo-de-présentation-donyxia",
    "href": "blog/onyxia/index.html#vidéo-de-présentation-donyxia",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Vidéo de présentation d’Onyxia",
    "text": "Vidéo de présentation d’Onyxia"
  },
  {
    "objectID": "blog/onyxia/index.html#références",
    "href": "blog/onyxia/index.html#références",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Références",
    "text": "Références\n\n\nDavenport, Thomas H, et DJ Patil. 2022. « Is Data Scientist Still the Sexiest Job of the 21st Century? » Harvard business review. https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century."
  },
  {
    "objectID": "blog/onyxia/index.html#footnotes",
    "href": "blog/onyxia/index.html#footnotes",
    "title": "Onyxia: l’infrastructure cloud mère des dragons",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nOn peut ajouter que cette question n’est pas exclusivement technologique. Même s’il est volontairement polémique, l’article de Jordan Tigani “Big Data is Dead” illustre bien le changement de paradigme du monde de la tech.↩︎"
  },
  {
    "objectID": "blog/embedding/index.html",
    "href": "blog/embedding/index.html",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "",
    "text": "Avec le développement de la collecte automatisée d’information numérique, les données textuelles sont devenues omniprésentes, que ce soit sous la forme d’e-mails, de réponses à des enquêtes, d’articles de presse ou encore de commentaires sur les réseaux sociaux. Ces données peuvent être une source très riche d’informations mobilisable par les statisticiens, pour peu qu’ils parviennent à en faire un traitement statistique. Ainsi, une problématique récurrente dans la statistique publique consiste à classer des informations formulées en langage courant (professions, noms de produits, noms de communes, etc.) dans des nomenclatures standardisées (PCS1, NAF2, COG3…).\nOr, le traitement des données textuelles pose une difficulté particulière: le langage naturel n’a pas de sens pour un ordinateur ! Un ordinateur ne travaille qu’avec des nombres, et ne peut pas manipuler directement des mots, des expressions ou des phrases. C’est pourquoi de multiples méthodes ont été développées au cours des dernières décennies pour proposer des solutions génériques permettant de traiter des corpus de données textuelles à la fois peu structurés et hétérogènes. Cet ensemble de méthodes de traitement automatisé du langage, plus connues sous l’acronyme NLP (natural langage processing) constituent encore aujourd’hui un champ de recherche particulièrement actif.\nCe billet de blog n’a pas l’ambition de proposer un aperçu des méthodes de NLP, mais simplement de présenter deux méthodes fréquemment utilisées pour transformer l’information textuelle pour la rendre compréhensible et utilisable par une machine:"
  },
  {
    "objectID": "blog/embedding/index.html#traiter-un-texte-comme-une-information-numérique-les-approches-possibles",
    "href": "blog/embedding/index.html#traiter-un-texte-comme-une-information-numérique-les-approches-possibles",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "Traiter un texte comme une information numérique : les approches possibles",
    "text": "Traiter un texte comme une information numérique : les approches possibles\n\nL’approche bag of words\nLe principe du bag of words est qu’on peut décrire un document comme un dictionnaire de mots (un sac de mots) dans lequel on pioche plus ou moins fréquemment un terme en fonction de son nombre d’occurrences.\nLa manière la plus simple de transformer des phrases ou des libellés textuels en une information numérique est de passer par un objet que l’on appelle la matrice document-terme. L’idée est de compter le nombre de fois où les mots (les termes, en colonne) sont présents dans chaque phrase ou libellé (le document, en ligne). Cette matrice fournit alors une représentation numérique des données textuelles.\nConsidérons un corpus constitué des trois phrases suivantes :\n\n_“La pratique du tricot et du crochet_”\n“Transmettre la passion du timbre”\n“Vivre de sa passion”\n\nLa matrice document-terme associée à ce corpus est la suivante :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrochet\nde\ndu\net\nla\npassion\npratique\nsa\ntimbre\ntransmettre\ntricot\nvivre\n\n\n\n\nLa pratique du tricot et du crochet\n1\n0\n2\n1\n1\n0\n1\n0\n0\n0\n1\n0\n\n\nTransmettre sa passion du timbre\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n0\n0\n\n\nVivre de sa passion\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n\nMission accomplie ! 🎉 Chaque phrase du corpus est associée à un vecteur numérique.\nIl est maintenant possible de manipuler cette matrice comme des données tabulaires classiques. Par exemple, on pourrait appliquer l’un des algorithmes usuels de classification (régression logistique, forêt aléatoire, gradient boosting, etc.) pour classer ces phrases dans des catégories.\nL’approche bag-of-words répond donc au besoin initial de transformer les données pour les rendre manipulables par une machine, en représentant les données textuelles sous la forme d’une matrice document-terme. Cette approche présente néanmoins une limite: elle traite tous les termes de façon indépendante et ne restitue pas la proximité de certains termes. Par exemple, rien dans la matrice document-terme de l’exemple précédent n’indique que les termes ’tricot” et “crochet” relèvent du même champ lexical. Un autre type de représentation plus complexe et plus riche constitue souvent comme une meilleure option : le plongement lexical.\n\n\nLe plongement lexical\nLe plongement lexical (word embedding en anglais) consiste à projeter l’ensemble des termes qui apparaissent dans le corpus dans un espace numérique à \\(n\\) dimensions. Chaque mot est représenté par un vecteur de taille fixe (comprenant \\(n\\) nombres), de façon à ce que deux mots dont le sens est proche possèdent des représentations numériques proches. Ainsi les mots « chat » et « chaton » devraient avoir des vecteurs de plongement assez similaires, eux-mêmes également assez proches de celui du mot « chien » et plus éloignés de la représentation du mot « maison ».\n\n\n\nIllustration du word embedding\n\n\n\nIllustration du plongement lexical. Source : Post de blog Word Embedding : Basics\n\n \nChacune des \\(n\\) composantes va encoder des informations différentes, comme le fait d’être un être vivant ou un objet, le genre, l’âge, le niveau d’abstraction, etc. C’est pour cette raison que des termes appartenant au même champ lexical auront des représentations numériquement proches. En pratique, les vecteurs de plongement ont des dizaines voire des centaines de composantes et il est impossible d’associer à chacune une interprétation univoque : toutes les notions s’entremêlent, mais chaque composante a un rôle à jouer.\nLe plongement lexical possède deux avantages par rapport à l’approche bag of words. D’une part, il fournit une représentation dense des termes, qui est plus adaptée aux algorithmes d’apprentissage statistique que la représentation creuse (matrice contenant beaucoup de zéros) de l’approche bag of words. D’autre part, les opérations mathématiques ont un sens sur les vecteurs du plongement. C’est là la magie du plongement lexical: il devient possible de faire des mathématiques avec les mots. Ainsi par exemple, les vecteurs résultant de la différence entre les représentations des mots « femme » et « homme » d’une part, et des mots « reine » et « roi » d’autre part, devraient être proches, car conceptuellement ces couples de mots sont régis par la même relation : un changement de genre.\nCette formule, souvent résumée sous la forme,\n\\[\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\\]\na assuré le succès des embeddings, car elle permet à une machine d’appréhender les relations logiques entre les mots.\nJusqu’ici, nous avons parlé du plongement de mots, mais comment obtenir le plongement d’un libellé textuel ? Une possibilité est de considérer tous les mots qui composent le libellé et de calculer la moyenne de leurs vecteurs de plongement."
  },
  {
    "objectID": "blog/embedding/index.html#construction-dun-plongement-lexical",
    "href": "blog/embedding/index.html#construction-dun-plongement-lexical",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "Construction d’un plongement lexical",
    "text": "Construction d’un plongement lexical\nUn plongement lexical se construit en parcourant un grand corpus de textes et en repérant les mots qui apparaissent souvent dans le même contexte. L’ensemble des articles Wikipedia est un des corpus de prédilection des personnes ayant construit des plongements lexicaux. Il comporte en effet des phrases complètes, contrairement à des informations issues de commentaires de réseaux sociaux, et propose des rapprochements intéressants entre des personnes, des lieux, etc.\nLe contexte d’un mot est défini par une fenêtre de taille fixe autour de ce mot. La taille de la fenêtre est un paramètre de la construction de l’embedding. Le corpus fournit un grand ensemble d’exemples mots-contexte, qui peuvent servir à entraîner un réseau de neurones.\nPlus précisément, il existe deux approches :\n\nContinuous bag of words (CBOW), où le modèle est entraîné à prédire un mot à partir de son contexte ;\nSkip-gram, où le modèle tente de prédire le contexte à partir d’un seul mot.\n\n\n\n\nIllustration de la différence entre les approches CBOW et Skip-gram\n\n\n\nIllustration de la différence entre les approches CBOW et Skip-gram. Source : Anwarvic sur StackOverflow\n\n \n\nAlgorithmes célèbres\nLa méthode de construction d’un plongement lexical présentée ci-dessus est celle de l’algorithme Word2Vec. Il s’agit d’un modèle open-source développé par une équipe de Google en 2013. Word2Vec a été le pionnier en termes de modèles de plongement lexical.\nLe modèle GloVe constitue un autre exemple4. Développé en 2014 à Stanford, ce modèle ne repose pas sur des réseaux de neurones mais sur la construction d’une grande matrice de co-occurrences de mots. Pour chaque mot, il s’agit de calculer les fréquences d’apparition des autres mots dans une fenêtre de taille fixe autour de lui. La matrice de co-occurrences obtenue est ensuite factorisée par une décomposition en valeurs singulières. Il est également possible de produire des plongements de mots à partir du modèle de langage BERT, développé par Google en 2019, dont il existe des déclinaisons dans différentes langues, notamment en Français (les modèles CamemBERT ou FlauBERT)\nEnfin, le modèle FastText, développé en 2016 par une équipe de Facebook, fonctionne de façon similaire à Word2Vec mais se distingue particulièrement sur deux points :\n\nEn plus des mots eux-mêmes, le modèle apprend des représentations pour les n-grams de caractères (sous-séquences de caractères de taille \\(n\\), par exemple « tar », « art » et « rte » sont les trigrammes du mot « tarte »), ce qui le rend notamment robuste aux variations d’orthographe ;\nLe modèle a été optimisé pour que son entraînement soit particulièrement rapide.\n\nA l’Insee, plusieurs modèles de classification de libellés textuels dans des nomenclatures reposent sur l’algorithme de plongement lexical FastText.\n\n\n\nIllustration du modèle fastText\n\n\n\nIllustration du fonctionnement du modèle fastText sur un libellé de profession\n\n \n\n\nComment utiliser ces modèles en pratique ?\nCollecter à nouveau les données ayant servi à entrainer un modèle puis le ré-entraîner implique énormément de ressources, ce qui est coûteux en temps et peu écologique5.\nEn Python, plusieurs librairies proposent les modèles Word2Vec, GloVe, BERT ou FastText. Le package gensim les met toutes en œuvre à l’exception de BERT. Ce dernier est disponible sur HuggingFace, la principale plateforme de mise à disposition de modèles pré-entraînés. Il est ainsi possible d’utiliser BERT avec les librairies PyTorch ou Keras. Chacun des modèles présentés possède également son package dédié, généralement développé par l’équipe de recherche ayant entraîné le modèle.\nEn R, il faut utiliser les packages word2vec, text2vec (pour le modèle GloVe) et fastTextR."
  },
  {
    "objectID": "blog/embedding/index.html#bonus-le-plongement-lexical-en-version-ludique",
    "href": "blog/embedding/index.html#bonus-le-plongement-lexical-en-version-ludique",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "Bonus : le plongement lexical en version ludique",
    "text": "Bonus : le plongement lexical en version ludique\nLe résultat d’un plongement lexical peut avoir de nombreux usages. Il rend notamment possible le calcul de la proximité entre deux mots quelconques.\nUne manière de procéder est de calculer la similarité cosinus entre les vecteurs de plongement des deux mots. Plus précisément, la similarité entre deux mots de représentations vectorielles \\(u\\) et \\(v\\) est définie comme le cosinus de leur angle \\( \\) : \\[cos(\\theta) = \\frac{u \\cdot v}{\\lVert u\\rVert \\lVert v\\rVert}\\]\n\n\n\nIllustration de la similarité cosinus\n\n\n\nIllustration de la similarité cosinus en deux dimensions\n\n \nLe calcul de la proximité entre les mots est à la base du jeu cemantix. Le principe est proche du jeu Wordle mais s’en distingue sur un point : il y a certes un mot à trouver chaque jour et il s’agit de faire des propositions de mots mais le jeu répond en donnant la proximité entre les mots proposés et le mot du jour. Ainsi, au fil des propositions, on a une vision de plus en plus précise du champ lexical associé au mot mystère, jusqu’à finalement le trouver."
  },
  {
    "objectID": "blog/embedding/index.html#footnotes",
    "href": "blog/embedding/index.html#footnotes",
    "title": "Le plongement lexical ou comment apprendre à lire à un ordinateur",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nLa nomenclature PCS (professions et catégories socioprofessionnelles) sert à la codification des professions dans le recensement et les enquêtes auprès des ménages. Elle permet ainsi de classer un ensemble de professions dans une même catégorie. Par exemple, dans sa dernière version (PCS 2020), la catégorie des “Professions libérales de santé” (31A) regroupe diverses professions médicales: médecins libéraux, dentistes, psychologues, vétérinaires, pharmaciens libéraux… Une description plus complète de cette nomenclature et de son historique est disponible sur le site de l’Insee↩︎\nLa NAF (nomenclature d’activités française), est une nomenclature des activités économiques productives, principalement élaborée pour faciliter l’organisation de l’information économique et sociale. Il s’agit d’une typologie facilitant la représentation de l’économie sous forme de secteurs. Par exemple, au sein de l’industrie manufacturière (section C), la NAF distingue les industries alimentaires de l’industrie de l’habillement ou de l’industrie automobile. Une description plus complète de cette nomenclature et de son historique est disponible sur le site de l’Insee↩︎\nLe Code Officiel Géographique est le référentiel permettant de relier des adresses, des noms de communes ou encore des noms de collectivités locales à un identifiant unique. Pour plus d’informations, voir le site de l’Insee↩︎\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation↩︎\nStrubell, Ganesh, and McCallum (2019) estiment que l’entraînement d’un modèle à l’état de l’art dans le domaine du NLP nécessite autant d’énergie que ce que consommeraient cinq voitures, en moyenne, au cours de l’ensemble de leur cycle de vie.↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Vous avez une idée de sujet ? Toute personne souhaitant coontribuer est la bienvenue, quel que soit son statut, où elle travaille. Pour contribuer, proposez votre idée sur Github ou par mail. Le mode opératoire détaillé est rappelé sur Github.\nPour rappel, les informations qui sont diffusées sur ce blog n’engagent que les contributeurs et en aucun cas les institutions dont ils dépendent.\n\n\n\n\n\n\n\n\n\n\n\nGuide d’utilisation des données du recensement de la population au format Parquet\n\n\n\nPython\n\nR\n\nParquet\n\n\n\nUn post de blog pour accompagner la mise à disposition des données détaillées du recensement au format Parquet.\n\n\n\n\n\n\n23 oct. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnyxia: l’infrastructure cloud mère des dragons\n\n\n\nInsee\n\nsspcloud\n\n\n\nLes technologies cloud sont incontournables dans l’écosystème de la donnée. Pour ne pas se rendre dépendante de fournisseurs de services externes, l’Insee a développé un…\n\n\n\n\n\n\n10 mai 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolars, une alternative fraîche à Pandas\n\n\n\nPython\n\nPandas\n\nPolars\n\nData wrangling\n\n\n\nPolars, une alternative moderne et fluide à Pandas\n\n\n\n\n\n\n10 févr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfolettre n°9\n\n\n\nInsee\n\nRetrospective\n\nInfolettre\n\n\n\nAprès la rétrospective de l’année 2022 de la data science, il est temps de se pencher sur l’année du réseau avec des visualisations interactives produites grâce à…\n\n\n\n\n\n\n10 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRétrospective de l’année 2022\n\n\n\nNLP\n\nObservable\n\nQuarto\n\nDeep learning\n\nInsee\n\nRetrospective\n\nInfolettre\n\n\n\nLa data science a beaucoup fait parler d’elle en 2022, notamment du fait des deux coups médiatiques d’openAI, à savoir…\n\n\n\n\n\n\n31 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe plongement lexical ou comment apprendre à lire à un ordinateur\n\n\n\nInsee\n\nNLP\n\n\n\nIntroduction aux méthodes de traitement du langage naturel.\n\n\n\n\n\n\n3 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe machine learning aux Journées de la Méthodologie Statistique 2022 (JMS)\n\n\n\nInsee\n\nevenement\n\n\n\nRevue des présentations en lien avec les travaux en machine learning aux JMS de 2022\n\n\n\n\n\n\n6 avr. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallélisation des traitements : Hadoop MapReduce vs Spark\n\n\n\ndonnées volumineuses\n\n\n\n\n\n\n\n\n\n\n1 juin 2016\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Le SSPHub, le réseau des data scientists",
    "section": "",
    "text": "Le réseau des data scientists du Service Statistique Publique (SSP) est constitué principalement, mais non exclusivement, par les administrations en charge de la production de statistiques officielles (Insee et Services Statistiques Ministériels principalement).\nLe réseau répond à plusieurs objectifs, dont les principaux sont:\n\nLe partage et la diffusion de connaissances au sein de la communauté des data scientists de l’administration autour des pratiques et des innovations de la data-science ;\nLa valorisation de travaux novateurs dans le champ de la production statistique ;\nFaciliter les échanges entre pairs, qu’ils appartiennent au service statistique public ou non.\n\nAfin de mieux cerner les objectifs, le public cible, les thèmes abordés par le réseau, et les moyens associés, un manifeste 📜 a été rédigé de manière collective."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "Le SSPHub, le réseau des data scientists",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nPlus de détails sont disponibles sur le site de l’Insee ici↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SSPHub",
    "section": "",
    "text": "// echo: false\n// output: false\ninscrits = 730\n// echo: false\nbadge = html`&lt;a href=\"https://grist.numerique.gouv.fr/o/ssphub/forms/jSjAV3L2F8mmiRVuVEpfF7/103\"&gt;&lt;img alt=\"Static Badge\" src=\"https://img.shields.io/badge/${inscrits}_inscrits-blue?style=social&label=⭐️%20Rejoindre%20la%20liste%20de%20diffusion&color=8A2BE2&link=https%3A%2F%2Fgrist.numerique.gouv.fr%2Fo%2Fssphub%2Fforms%2FjSjAV3L2F8mmiRVuVEpfF7%2F103\"&gt;\n&lt;/a&gt;\n`"
  },
  {
    "objectID": "index.html#les-dernières-actualités-et-contenus-du-réseau",
    "href": "index.html#les-dernières-actualités-et-contenus-du-réseau",
    "title": "SSPHub",
    "section": "Les dernières actualités et contenus du réseau",
    "text": "Les dernières actualités et contenus du réseau\n\n\n\n\n\n\n\n\n\n\nLa première infographie\n\n\nInfolettre du mois de janvier 2026\n\n\n\n\n\n\n30 janv. 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrançoise Bahoken et Nicolas Lambert, présentation de leur livre Cartographia\n\n\nLe 13 janvier (14h30 - 15h30), Françoise Bahoken et Nicolas Lambert nous ont présenté leur dernier livre…\n\n\n\n\n\n\n13 janv. 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nQui pour financer l’open source?\n\n\nInfolettre du mois de décembre 2025\n\n\n\n\n\n\n10 déc. 2025\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#les-derniers-billets-de-blog-et-événements",
    "href": "index.html#les-derniers-billets-de-blog-et-événements",
    "title": "SSPHub",
    "section": "Les derniers billets de blog et événements",
    "text": "Les derniers billets de blog et événements\nL’ensemble des billets de blog peut être retrouvé sur la page dédiée, tout comme les événements.\n\n\n\n\n\n\n\n\n\n\nFrançoise Bahoken et Nicolas Lambert, présentation de leur livre Cartographia\n\n\nLe 13 janvier (14h30 - 15h30), Françoise Bahoken et Nicolas Lambert nous ont présenté leur dernier livre…\n\n\n\n\n\n\n13 janv. 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nTroisième journée du SSPHub\n\n\nProgramme et modalités d’inscription à la 3e journée du réseau\n\n\n\n\n\n\n1 déc. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtelier - Comment récupérer des données sous format Parquet ?\n\n\nLe format Parquet est un format de données connaissant une popularité importante du fait de ses caractéristiques techniques (orientation colonne, compression…\n\n\n\n\n\n\n16 avr. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtelier - Comment récupérer des données par API ?\n\n\nLes API (Application Programming Interface) sont un mode d’accès aux données en expansion. Grâce aux API, l’automatisation de scripts est facilitée puisqu’il n’est plus…\n\n\n\n\n\n\n9 avr. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeuxième journée du SSPHub\n\n\nProgramme et modalités d’inscription à la 2e journée du réseau\n\n\n\n\n\n\n14 oct. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto : Une évolution de R Markdown pour des travaux statistiques reproductibles\n\n\nPour fiabiliser la production de documents construits en valorisant des données (tableaux, graphiques, etc.), RStudio (devenu Posit depuis) a construit il y a quelques…\n\n\n\n\n\n\n2 mai 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nEric Mauvière, “La dataviz pour donner du sens aux données et communiquer un message”\n\n\nLe 29 février (15h - 16h), Eric Mauvière nous fera une présentation, avec de nombreux exemples issus de la statistique publique, de la manière dont une visualisation de…\n\n\n\n\n\n\n29 févr. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide d’utilisation des données du recensement de la population au format Parquet\n\n\nUn post de blog pour accompagner la mise à disposition des données détaillées du recensement au format Parquet.\n\n\n\n\n\n\n23 oct. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnyxia: l’infrastructure cloud mère des dragons\n\n\nLes technologies cloud sont incontournables dans l’écosystème de la donnée. Pour ne pas se rendre dépendante de fournisseurs de services externes, l’Insee a développé un…\n\n\n\n\n\n\n10 mai 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremière journée du SSPHub\n\n\nReplay de la première journée de présentation du SSPHub\n\n\n\n\n\n\n29 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n“OCRisation, état de l’art et projets auxquels participe Teklia” par Christopher Kermorvant\n\n\nLe 29 mars de 15h à 16h nous recevons Christopher Kermorvant, chercheur spécialisé en OCRisation et fondateur de Teklia. Il nous fera un état de l’art de l’OCRisation puis…\n\n\n\n\n\n\n29 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation du projet Meta Academy - Carpentries\n\n\nPour favoriser l’adoption des langages R, Python et Git dans les administrations, le programme ModernStat piloté par l’OCDE et Statistics Canada, a lancé un projet…\n\n\n\n\n\n\n28 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation des packages R et Python pour accéder à l’open data de l’Insee\n\n\nL’Insee met à disposition ses données par le biais d’API ou par son site web. Pour faciliter la…\n\n\n\n\n\n\n13 févr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolars, une alternative fraîche à Pandas\n\n\nPolars, une alternative moderne et fluide à Pandas\n\n\n\n\n\n\n10 févr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation de gridviz par Julien Gaffuri\n\n\nEvénement de présentation de gridviz par Julien Gaffuri (Eurostat)\n\n\n\n\n\n\n20 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfolettre n°9\n\n\nAprès la rétrospective de l’année 2022 de la data science, il est temps de se pencher sur l’année du réseau avec des visualisations interactives produites grâce à…\n\n\n\n\n\n\n10 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRétrospective de l’année 2022\n\n\nLa data science a beaucoup fait parler d’elle en 2022, notamment du fait des deux coups médiatiques d’openAI, à savoir…\n\n\n\n\n\n\n31 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvénement de clôture de la saison 1 du programme 10%\n\n\nLe 5 décembre, a lieu l’événement de clotûre de la saison 1 du programme 10%. Au programme, restitution des [projets portés cette…\n\n\n\n\n\n\n5 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation d’Observable par Nicolas Lambert\n\n\nobservable est la nouvelle plateforme de dataviz réactive. Initiée par Mike Bostock (créateur de D3.js), ce réseau social de la dataviz a pour…\n\n\n\n\n\n\n16 nov. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe plongement lexical ou comment apprendre à lire à un ordinateur\n\n\nIntroduction aux méthodes de traitement du langage naturel.\n\n\n\n\n\n\n3 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunathon de juin 2022\n\n\nPrésentation du deuxième Funathon du SSPLab organisé le 20 juin 2022 autour de 9 sujets, en R et en Python.\n\n\n\n\n\n\n19 juin 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe machine learning aux Journées de la Méthodologie Statistique 2022 (JMS)\n\n\nRevue des présentations en lien avec les travaux en machine learning aux JMS de 2022\n\n\n\n\n\n\n6 avr. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunathon de juin 2021\n\n\nPrésentation du premier Funathon du SSPLab organisé le 21 juin 2021 autour de 8 sujets, en R et en Python, à partir de données Airbnb\n\n\n\n\n\n\n20 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallélisation des traitements : Hadoop MapReduce vs Spark\n\n\n\n\n\n\n\n\n\n1 juin 2016\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#les-projets-innovants-du-ssphub",
    "href": "index.html#les-projets-innovants-du-ssphub",
    "title": "SSPHub",
    "section": "Les projets innovants du SSPHub",
    "text": "Les projets innovants du SSPHub\nL’ensemble des projets innovants peut être retrouvé sur la page dédiée.\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nscanR, une application pour observer le paysage de la recherche et de l’innovation en France\n\n\nAgrégation et mise à disposition de données massives sur la recherche et l’innovation en France par des visualisations, des moteurs de recherche ElasticSearch et des API\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisations des données liées aux tests du SARS-Cov2\n\n\nPublication hebdomadaire des données liées aux tests de détection du SARS-Cov2 par Shiny à partir du système d’information SI-DEP\n\n\n\n\n\n\n1 juin 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoremifasol\n\n\nLe package  R Doremifasol facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur…\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\npynsee, un package Python  pour récupérer les données de l’Insee\n\n\nLe package  pynsee facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur Github.\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\nUtiliser les images satellites pour améliorer le recensement de la population dans les territoire ultra-marins\n\n\n\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nDétecter la cybercriminalité dans les procédures\n\n\nDétection des infractions relevant de la cyberdélinquance à partir d’une analyse textuelle des manières d’opérer\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nModélisation de l’appartenance au parc des véhicules routiers et de son utilisation\n\n\nAppariement de bases administratives et modélisation pour estimer le nombre de véhicules routiers en France\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne\n\n\nExploration des apports du webscraping pour mesurer le prix des nuitées hôtelières dans l’IPC\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrévoir la croissance en lisant le journal\n\n\nUtiliser les articles de presse en continu pour construire un indicateur aidant à prévoir la croissance\n\n\n\n\n\n\n1 mars 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparaison des méthodes d’appariement et apport du machine learning\n\n\nTester et comparer différentes méthodes d’appariements afin de dégager des recommandations pour les travaux nécessaires à la construction des répertoires, notamment dans le…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtraction automatique du tableau des filiales et participations des comptes sociaux des entreprises\n\n\nExtraire les informations de tableaux de comptes sociaux, en particulier des tableaux des filiales et participations, contenus dans des images scannées mises à disposition…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation\n\n\nCollecter sur le web les caractéristiques des produits pour améliorer la prise en compte des effets qualité dans l’indice des prix à la consommation\n\n\n\n\n\n\n1 juin 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n« GDP Tracker » : un outil pour des prévisions économiques en continu\n\n\nModèles de machine learning pour effectuer des prévisions en temps réel (nowcasting) pour alimenter les analyses conjoncturelles de l’Insee\n\n\n\n\n\n\n1 déc. 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nDétecter et traiter les valeurs aberrantes ou manquantes, application à la Déclaration Sociale Nominative\n\n\nUtilisation des méthodes de machine learning pour la détection et le traitement des valeurs aberrantes ou manquantes, application à la Déclaration Sociale Nominative\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#les-dernières-newsletters",
    "href": "index.html#les-dernières-newsletters",
    "title": "SSPHub",
    "section": "Les dernières newsletters",
    "text": "Les dernières newsletters\nToutes les newsletters précédemment publiées sont disponibles sur la page dédiée.\n\n\n\n\n\n\n\n\n\n\nLa première infographie\n\n\nInfolettre du mois de janvier 2026\n\n\n\n\n\n\n30 janv. 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nQui pour financer l’open source?\n\n\nInfolettre du mois de décembre 2025\n\n\n\n\n\n\n10 déc. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA\n\n\nInfolettre du mois d’octobre 2025\n\n\n\n\n\n\n25 oct. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa rentrée 2025: actualités, nouveautés, interview de rentrée\n\n\nInfolettre du mois de Septembre 2025\n\n\n\n\n\n\n29 sept. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks\n\n\nInfolettre du mois de Mars 2024\n\n\n\n\n\n\n7 mars 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web\n\n\nInfolettre du mois de Février 2024\n\n\n\n\n\n\n20 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nRétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud\n\n\nInfolettre du mois de Décembre 2023\n\n\n\n\n\n\n21 déc. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM\n\n\nInfolettre de rentrée, Septembre 2023\n\n\n\n\n\n\n10 sept. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropositions de lecture estivale\n\n\nInfolettre estivale, Juillet 2023\n\n\n\n\n\n\n1 juil. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDes innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter\n\n\nInfolettre du mois d’Avril 2023\n\n\n\n\n\n\n1 avr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nTapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley\n\n\nInfolettre du mois de Mars 2023, deuxième quinzaine\n\n\n\n\n\n\n15 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web\n\n\nInfolettre du mois de Mars 2023\n\n\n\n\n\n\n1 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images\n\n\nInfolettre du mois de Février 2023\n\n\n\n\n\n\n30 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz\n\n\nInfolettre du mois de Janvier 2023\n\n\n\n\n\n\n10 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nL’année 2022 dans le monde de la data science : IA, transformation de RStudio, Observable\n\n\nInfolettre du mois de Décembre 2022\n\n\n\n\n\n\n31 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchive des infolettres et lettres Big Data\n\n\nLes infolettres et lettres Big Data antérieures 👵👴, avant la publication sous forme de blog\n\n\n\n\n\n\n31 août 2022\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#les-réseaux-partenaires",
    "href": "index.html#les-réseaux-partenaires",
    "title": "SSPHub",
    "section": "Les réseaux partenaires",
    "text": "Les réseaux partenaires\nQuelques communautés de la data-science avec lesquels nous collaborons\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            CoP OCDE\n            \n\n            \n              Le groupe Community of Practice de l'OCDE est un réseau informel organisé autour des sujets d'innovation statistique.\n\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Eurostat & les  trusted smart statistics (TSS) \n            \n\n            \n              Eurostat encadre les réseaux des instituts nationaux de statistiques et les travaux européens autour de l'exploration de nouvelles sources de données et des nouveaux outils de datascience pour la statistique officielle\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            L'ENSAE, l'ENSAI et le CREST\n            \n\n            \n              L'ENSAE, l'ENSAI sont des écoles d'ingénieur en statistiques, science des données et analyse économique. Le CREST est un centre rassemblant des enseignants-chercheurs d’économie de l’École polytechnique et du CNRS\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            La chaire Finance digitale\n            \n\n            \n              L’objectif de la chaire est de conduire des travaux de recherche sur l’ensemble des innovations, de services, de produits ou d’organisations en lien avec le numérique, susceptibles de modifier le métier de l’intermédiaire financier. Elle est le fruit d'un partenariat entre Télécom Paris, l’université Paris II Panthéon-Assas, l’Institut Louis Bachelier, le Groupement des Cartes Bancaires (CB), la Caisse des Dépôts et l'Insee.\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Lab IA (Etalab) & la DINUM\n            \n\n            \n              La communauté des data scientists et acteurs de l’IA pour l’administration française et plus généralement la DINUM\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Onyxia\n            \n\n            \n              La communauté Onyxia, à l'origine du SSPCloud,\na pour objectif de fournir une plateforme flexible pour expérimenter\nles outils modernes de la data-science.\n\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Spyrales\n            \n\n            \n              Une communauté d'agents de l'Etat pour s'entraider en R et Python\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            UNECE ML Group\n            \n\n            \n              Le travail de recherche du Groupe ML est divisé en 5 groupes de travail visant à traiter différentes problématiques liées à l'utilisation de l'apprentissage automatique pour les statistiques officielles.\n\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            grrr\n            \n\n            \n              Grrr (\"pour quand votre R fait Grrr\") est un\ngroupe Slack (plateforme de discussion instantanée)\nfrancophone dédié aux échanges et à l’entraide autour de R.\nIl s'agit du point central de la communauté R francophone.\nIl est ouvert à tou.te.s et se veut accessible aux débutants. Vous pouvez même utiliser un pseudonyme si vous préférez.\n\n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "manifeste.html",
    "href": "manifeste.html",
    "title": "Le manifeste du réseau des data scientists du service statistique public",
    "section": "",
    "text": "Note\n\n\n\nCe manifeste est une production collective. Pour proposer des modifications à celui-ci, qui seront discutées collégialement, vous pouvez suivre le lien vers  indiqué à droite."
  },
  {
    "objectID": "manifeste.html#footnotes",
    "href": "manifeste.html#footnotes",
    "title": "Le manifeste du réseau des data scientists du service statistique public",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nRapport de l’Inspection Générale de l’Insee N° 2020_48/DG75-B001 (non public)↩︎\nLe Service Statistique Public (SSP) regroupe les institutions en charge de la production de statistiques officielles. Il est principalement constitué de l’Insee et des services statistiques ministériels (SSM). Pour en savoir plus, le site de l’Insee propose des éléments supplémentaires.↩︎\nIl est possible de rejoindre ce canal Tchap en cliquant ici↩︎"
  },
  {
    "objectID": "blog/2022_jms/index.html",
    "href": "blog/2022_jms/index.html",
    "title": "Le machine learning aux Journées de la Méthodologie Statistique 2022 (JMS)",
    "section": "",
    "text": "Signes des temps, l’édition 2022 des Journées de la Méthodologie Statistique (JMS) a mis en lumière la part croissante du machine learning dans les travaux menés au sein de la statistique publique, et au delà. Ainsi, cette année, ce sont 12 papiers utilisant ce type de méthodes qui ont été présentés - contre, à titre de comparaison, 4 en 2018.\nEn particulier, quatre types d’utilisation de telles méthodes ont été mises en avant :\n\nLe redressement de données manquantes\nLa codification automatique\nLes appariements\nLes études\n\n\nLe redressement de données manquantes\nCette thématique est la plus représentée, grâce à une session dédiée au traitement de données manquantes. Au sein de cette session, 4 communications présentaient des tentatives d’utilisation de méthodes de machine Learning à cette fin. L’objectif de ces 4 publications est différent :\n\nÉvaluer de façon théorique la possibilité d’utiliser les forêts aléatoires pour redresser les enquêtes par sondage en grande dimension, dans quel cas les modèles paramétriques sont parfois instables et inefficaces (Forêts aléatoires : D’une approche par modélisation assistée au traitement de la non-réponse) ;\nÉvaluer l’intérêt des réseaux de neurones dans le cadre du redressement de la non réponse, de façon théorique et en appliquant au cas de l’enquête emploi (Imputation de valeurs manquantes avec des réseaux de neurones : Prédiction des salaires dans l’enquête emploi :) ;\nComparer les différentes méthodes de machine learning de façon empirique dans le cadre d’une imputation de non réponse (Traitement de la non-réponse au moyen de méthodes de machine learning) ;\nÉvaluer un processus complet de correction de non-réponse, dans lequel le machine Learning n’est qu’une facette (Estimation des montants manquants de versements de TVA : Exploitation des données du contrôle fiscal).\n\n\n\nLa codification automatique et l’extraction de données\nTrois communications sur ces thèmes ont été présentées :\n\nUne tentative de codification automatique de la PCS au moyen de méthodes de machine Learning (Application de techniques de machine learning pour coder les professions dans la nomenclature des professions et catégories socio-professionnelles 2020) - cf. la page dédiée au projet sur le site ;\nUne tentative d’extraction automatique d’informations des documents scannés issues des comptes sociaux des entreprises (Extraction automatique de données issues d’images scannées : Une illustration par les comptes sociaux d’entreprises) - cf. la page dédiée au projet sur le site ;\nUne classification automatique des infractions commises à partir d’une analyse textuelle et de l’utilisation de réseau de neurones (Détection des infractions relevant de la cyberdélinquance) - cf. la page dédiée au projet sur le site.\n\n\n\nMéthodes d’appariements\nLe machine learning peut également être utilisé pour faciliter les appariements. A cet égard, deux communications ont été présentées :\n\nUne communication discutant globalement des méthodes d’appariements, incluant - sans se restreindre - le machine learning (Probabilistes ou déterministes, des méthodes d’appariements au banc d’essai du programme RéSIL) - cf. la page dédiée au projet sur le site ;\nUne communication présentant une méthode innovante d’appariement flou à partir de l’utilisation d’Elastic Search et d’un réseau de neurone pré-entraîné (Enrichissement de données de caisses à partir d’informations nutritionnelles : Une approche par appariement flou sur données de grande dimension) - cf. la page dédiée au projet sur le site.\n\n\n\nEtudes\nLa sphère la plus large d’exploitation est bien sûr les études. Les sujets sont alors variés :\n\nL’estimation d’un parc de véhicules roulants, en modélisant à la fois la probabilité qu’un véhicule roule encore, et le nombre de kilomètres parcourus (Modélisation de l’appartenance au parc des véhicules routiers et de son utilisation) ;\nL’estimation de la valeur d’un patrimoine immobilier dans le cadre du projet FidelImmo (Estimation de la valeur du patrimoine immobilier des ménages à partir de données exhaustives) ;\nL’utilisation des données du site MeilleursAgents.com pour modéliser les loyers, et mieux comprendre les déterminants sous-jacents (Appréhender la rentabilité locative pour comprendre les mécaniques qui sous-tendent les loyers et les prix à l’aide de méthodes de machine learning) ;\nL’utilisation de méthodes de machine Learning pour améliorer la prévision du PIB (méthodes de nowcasting) (Nowcasting PIB : imputation de variables non encore publiées)."
  },
  {
    "objectID": "blog/hadoop_spark/index.html",
    "href": "blog/hadoop_spark/index.html",
    "title": "Parallélisation des traitements : Hadoop MapReduce vs Spark",
    "section": "",
    "text": "À la base des systèmes dits « big data », il y a un principe central : la distribution, à la fois des données et des traitements, sur un ensemble de machines/ordinateurs formant un cluster. Le stockage des données brutes s’appuie le plus souvent sur un système de fichiers distribués. MapReduce est la première implémentation pour les big data du principe de parallélisation des traitements appliquée aux fichiers distribués. Il repose sur deux fonctions principales, map et reduce, qui sont appliquées parfois à de multiples reprises.\nLa première décrit une transformation que l’on applique aux valeurs d’une collection de données au format clé / valeur ; la deuxième applique une opération à toutes les valeurs d’une même clé. Prenons l’exemple du dénombrement des nucléotides composant un brin d’ADN : « AGTCGGGGCT ». L’objet en entrée est donc une séquence, et on souhaite obtenir en sortie une table avec les différents symboles et le nombre d’occurrences de chacun d’entre eux dans la séquence initiale. La démarche « naturelle » consiste à prendre le premier symbole de la séquence, de noter dans la table de sortie son nom et d’initialiser un compteur à 1, puis de revenir à la séquence, d’identifier le second et s’il correspond au premier, d’incrémenter le compteur correspondant… Cette démarche est assez simple à décrire, mais demande en pratique de parcourir tout ou partie de la séquence à chaque itération. Cela n’est pas problématique si la séquence est aussi élémentaire que celle prise dans cet exemple… mais le devient si elle correspond au génome humain, constitué de quelques milliards de nucléotides (symboles). De la même manière et pour utiliser un exemple plus classique pour un statisticien, filtrer une table selon la valeur des attributs d’une variable est une opération a priori simple et classique, mais dont la mise en œuvre pour des très grands volumes peut s’avérer particulièrement inefficace voire impossible..\nLa même opération réalisée en MapReduce, correspond aux opérations suivantes : lors du chargement des données, la séquence a été coupée en plusieurs sous-séquences chargées sur différentes machines ou nœuds ; la première étape (map) consiste alors à appliquer au niveau de chaque noeud une fonction dont le résultat est une liste de couples constitués chacun d’une clé et d’une valeur. Ici, la clé correspond au symbole lu, et la valeur à l’occurrence 1 : on a donc une liste de couples (A,1), (G,1), (T,1), (A,1),.. Dans une deuxième étape de triage (« shuffling and sorting »), on rassemble sur un même nœud toutes les paires correspondant à une même clé. Finalement, on applique une fonction d’agrégation (reduce) qui combine (ici on somme tout simplement) les valeurs correspondantes à chaque clé. On aboutit dans cet exemple à quatre paires constituées d’un symbole et du nombre d’occurrences de ce symbole sur l’ensemble de la séquence.\n Source: Statistique et Big Data Analytics Volumétrie – L’Attaque des Clones\nAutre exemple plus traditionnel en économétrie, la multiplication d’une très grosse matrice M avec un vecteur x : on pourra répartir les colonnes de la matrice sur différents nœuds et procéder localement à la multiplication de ces colonnes avec les composantes correspondantes de x, avant d’agréger le résultat final.\n\nDes outils ont rapidement été construits pour rendre l’écriture des programmes MapReduce plus transparente pour l’utilisateur. Hive, une des solutions les plus populaires, propose d’écrire ces programmes en SQL, comme on le ferait pour manipuler les données d’une base relationnelle classique. Ce modèle de programmation permet également d’appliquer efficacement sur un très grand volume de données toute une gamme d’algorithmes. Il existe cependant un certain nombre de limites à ce modèle : la nature même des étapes map et reduce ne permet pas facilement la mise en œuvre d’algorithmes itératifs en particulier pour certains choix d’architecture (après chaque opération map ou reduce, Hadoop MapReduce écrit les résultats intermédiaires sur disque, ces résultats étant ensuite lus si nécessaires, ce qui entraîne d’importants temps d’exécution). La solution Spark propose des performances supérieures (jusqu’à cent fois plus rapide lorsqu’il travaille en mémoire vive) grâce à un certain nombre d’innovations par rapport au modèle historique. En effet, si Spark conserve un modèle d’exécution distribué sa capacité à opérer bien plus souvent en mémoire vive lui permet de gagner en rapidité d’exécution par opposition aux lectures écritures systématiques du modèle Hadoop. Plus précisément, Spark garde en mémoire le maximum de résultats intermédiaires, ainsi que l’historique des opérations. En cas de panne, les opérations seront effectuées à nouveau, ce qui reste moins coûteux que les écritures/lectures systématiques."
  },
  {
    "objectID": "blog/parquetRP/index.html",
    "href": "blog/parquetRP/index.html",
    "title": "Guide d’utilisation des données du recensement de la population au format Parquet",
    "section": "",
    "text": "Ce guide présente quelques exemples d’utilisation des données du recensement de la population diffusées au format Parquet. Il s’agit d’une version HTML enrichissant le guide publié sur le site insee.fr pour les langages Python  et  avec des exemples interactifs pouvant être construits par le biais de Quarto Markdown et Observable.\nL’ensemble des codes utilisés pour produire cette note est disponible sur le dépôt Github  InseeFrLab/exemples-recensement-parquet au format Quarto Markdown.\nPour plus d’informations sur le format Parquet, dans un contexte de statistique publique, se référer à Dondon et Lamarche (2023). Pour un exemple sur la différence entre format CSV et Parquet illustré sur les données du recensement de la population, voir Mauvière (2022).\nCe guide propose d’utiliser DuckDB à travers plusieurs langages pour effectuer des traitements sur les fichiers détails du recensement. Par rapport à d’autres approches, DuckDB a été choisi pour son efficacité ainsi que pour son universalité1."
  },
  {
    "objectID": "blog/parquetRP/index.html#requêtes-sur-les-colonnes-select",
    "href": "blog/parquetRP/index.html#requêtes-sur-les-colonnes-select",
    "title": "Guide d’utilisation des données du recensement de la population au format Parquet",
    "section": "Requêtes sur les colonnes (SELECT)",
    "text": "Requêtes sur les colonnes (SELECT)\nLa liste des colonnes à extraire du fichier peut être renseignée avec la clause SELECT. Celles-ci peuvent être renommées en appliquant au passage la clause AS.\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nInputs.table(\n    db.query(\"SELECT IPONDI AS poids, AGED, VOIT FROM table_individu LIMIT 10\")\n)\n```\n\n\nduckdb.sql(\"SELECT IPONDI AS poids, AGED, VOIT FROM table_individu LIMIT 10\")\n\n\ndbGetQuery(\n  con,\n  \"SELECT IPONDI AS poids, AGED, VOIT FROM table_individu LIMIT 10\"\n)\n\n\ntable_individu %&gt;%\n  select(poids = IPONDI, AGED, VOIT) %&gt;%\n  head(10)\n\n\n\n\nInputs.table(\n    db.query(\"SELECT IPONDI AS poids, AGED, VOIT FROM table_individu LIMIT 10\")\n)\n\n\n\n\n\n\nDuckDB propose également des fonctionnalités pour extraire des colonnes à travers des expressions régulières. De nombreux exemples peuvent être trouvés sur cette page.\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nInputs.table(\n    db.query(\"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') FROM table_individu LIMIT 10\")\n)\n```\n\n\nduckdb.sql(\"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') FROM table_individu LIMIT 10\")\n\n\ndbGetQuery(\n  con,\n  \"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') FROM table_individu LIMIT 10\"\n)\n\n\ntable_individu %&gt;%\n  select(poids = IPONDI, contains(\"AGE\")) %&gt;%\n  head(10)\n\n\n\n\nInputs.table(\n    db.query(\"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') FROM table_individu LIMIT 10\")\n)"
  },
  {
    "objectID": "blog/parquetRP/index.html#requêtes-sur-les-lignes-where",
    "href": "blog/parquetRP/index.html#requêtes-sur-les-lignes-where",
    "title": "Guide d’utilisation des données du recensement de la population au format Parquet",
    "section": "Requêtes sur les lignes (WHERE)",
    "text": "Requêtes sur les lignes (WHERE)\nPour extraire un sous-échantillon des données complètes, la clause WHERE permet d’appliquer des filtres à partir de conditions logiques. Par exemple, il est possible de ne conserver, du fichier national, que les données de l’Aude (11), de la Haute-Garonne (31) et de l’Hérault (34).\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nInputs.table(\n    db.query(\"SELECT * FROM table_individu WHERE DEPT IN ('11', '31', '34') LIMIT 10\")\n)\n```\n\n\nduckdb.sql(\"SELECT * FROM table_individu WHERE DEPT IN ('11', '31', '34')\")\n\n\ndbGetQuery(\n  con,\n  \"SELECT * FROM table_individu WHERE DEPT IN ('11', '31', '34')\"\n)\n\n\ntable_individu %&gt;%\n  filter(DEPT %in% c(\"11\", \"31\", \"34\")) %&gt;%\n  head(10)\n\n\n\n\nInputs.table(\n    db.query(\"SELECT * FROM table_individu WHERE DEPT IN ('11', '31', '34') LIMIT 10\")\n)\n\n\n\n\n\n\nIl est également possible de formater cette liste telle qu’attendue par SQL à partir d’une liste Python ou d’un vecteur R plus classique. Pour cela, le code suivant peut servir de modèle :\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nliste_regions = [\"11\", \"31\", \"34\"]\nliste_regions_sql = liste_regions.map(item =&gt; `'${item}'`).join(\",\")\nInputs.table(\n    db.query(`SELECT * FROM table_individu WHERE DEPT IN (${liste_regions_sql}) LIMIT 10`)\n)\n```\n\n\ncon = duckdb.connect()\n\ncon.execute('''\n  CREATE OR REPLACE VIEW table_individu\n  AS SELECT * FROM read_parquet(\"FD_INDCVI_2020.parquet\")\n'''\n)\n\nliste_regions = [\"11\", \"31\", \"34\"]\n\ndep_slots = \", \".join([\"?\" for _ in liste_regions])\nquery = \"SELECT * FROM table_individu WHERE DEPT IN ({})\".format(dep_slots)\nliste_regions_sql = \", \".join([f\"'{dep}'\" for dep in liste_regions])\ncon.execute(query, liste_regions).fetchdf()\n\n\nliste_regions &lt;- c(\"11\", \"31\", \"34\")\nliste_regions_sql &lt;- glue_sql_collapse(\n  lapply(\n    liste_regions, function(dep) glue_sql(\"'{`dep`}'\", .con=con)\n    ),\n  \", \"\n)\nquery &lt;- glue_sql(\n  \"SELECT * FROM table_individu WHERE DEPT IN ({liste_regions_sql})\",\n  .con=con\n)\ndbGetQuery(con, query)\n\n\nliste_regions &lt;- c(\"11\", \"31\", \"34\")\ntable_individu %&gt;%\n  filter(DEPT %in% liste_regions)\n\n\n\n\nliste_regions = [\"11\", \"31\", \"34\"]\nliste_regions_sql = liste_regions.map(item =&gt; `'${item}'`).join(\",\")\nInputs.table(\n    db.query(`SELECT * FROM table_individu WHERE DEPT IN (${liste_regions_sql}) LIMIT 10`)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour en savoir plus sur les prepared statements avec DuckDB en Python, et plus généralement pour avoir des exemples d’utilisations différentes, c’est ici que ça se passe.\n\nLes filtres sur les observations peuvent être faits à partir de critères sur plusieurs colonnes. Par exemple, pour ne conserver que les observations de la ville de Nice où la date d’emménagement est postérieure à 2020, la requête suivante peut être utilisée :\n\nObservable via QuartoPythonRR (dbplyr)\n\n\n```{ojs}\nInputs.table(\n    db.query(\n        \"SELECT * FROM table_logement WHERE COMMUNE = '06088' and AEMM &gt; 2020\"\n    )\n)\n```\n\n\nquery = \"SELECT * FROM table_logement WHERE COMMUNE = '06088' and AEMM &gt; 2020\"\nduckdb.sql(query)\n\n\ndbGetQuery(\n  con,\n  \"SELECT * FROM table_logement WHERE COMMUNE = '06088' and AEMM &gt; 2020\"\n)\n\n\ntable_logement %&gt;%\n  filter(COMMUNE == \"06088\") %&gt;%\n  filter(AEMM &gt; 2020)\n# Peut aussi s'écrire en une fois :\n# table_logement %&gt;% filter(COMMUNE == \"06088\", AEMM &gt; 2020)\n\n\n\n\nInputs.table(\n    db.query(\n        \"SELECT * FROM table_logement WHERE COMMUNE = '06088' and AEMM &gt; 2020\"\n    )\n)"
  },
  {
    "objectID": "blog/parquetRP/index.html#footnotes",
    "href": "blog/parquetRP/index.html#footnotes",
    "title": "Guide d’utilisation des données du recensement de la population au format Parquet",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nDes propositions d’enrichissements de cette documentation à partir d’implémentations alternatives, par exemple s’appuyant sur Arrow et dbplyr ou sur Polars sont bienvenues sur le Github InseeFrLab/exemples-recensement-parquet.↩︎"
  },
  {
    "objectID": "blog/recap2022/index.html",
    "href": "blog/recap2022/index.html",
    "title": "Infolettre n°9",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit\nLa rétrospective de l’année 2022 promettait une version plus personnalisée, inspirée des visualisations proposées par les réseaux sociaux pour synthétiser l’activité de leurs utilisateurs.\nCette newsletter un peu spéciale propose un retour sur la première année du réseau des data scientists de la statistique publique dont la préfiguration a commencé en mars 2022 et qui a été lancé officiellement en septembre. Vous pourrez retrouver à la fin de la newsletter des informations plus classiques: événements, retour sur les actions du réseau, formations, etc.\nElle permet aussi d’illustrer le potentiel d’outils qui ont été présentés dans la rétrospective de l’année 2022. Toutes les figures sont réactives, notamment quand vous passez votre souris dessus. Les principaux ingrédients qui ont été ici utilisés, et qui avaient été mentionnés dans la première partie de la rétrospective, sont Observable, Quarto et DuckDB. Les données sont stockées sur le système de stockage S3 du SSPCloud."
  },
  {
    "objectID": "blog/recap2022/index.html#lannée-du-réseau",
    "href": "blog/recap2022/index.html#lannée-du-réseau",
    "title": "Infolettre n°9",
    "section": "L’année du réseau",
    "text": "L’année du réseau\nLe réseau comporte deux canaux de communication: une liste de diffusion mail et un canal de discussions instantanées. Intéressons nous d’abord à la liste de diffusion mail !\n\n\n\nPendant l’année 2022, 7 newsletters ont été diffusées par mail. Chacune a permis d’augmenter sensiblement le nombre de personnes dans la liste de diffusion. A la fin de l’année, il y avait 312 inscrits1 dans la liste de diffusion.\nLe réseau a organisé trois événements pendant l’année 2022. D’abord, avant l’été, deux open hours ont eu lieu. Cet événement informel prenant la forme de retour d’expérience a été l’occasion de discussions stimulantes autour de d’usage de la data science pour l’administration. En novembre, l’événement autour d’Observable animé par Nicolas Lambert a réuni près de 50 personnes."
  },
  {
    "objectID": "blog/recap2022/index.html#répartition-des-modes-daccès-au-réseau",
    "href": "blog/recap2022/index.html#répartition-des-modes-daccès-au-réseau",
    "title": "Infolettre n°9",
    "section": "Répartition des modes d’accès au réseau",
    "text": "Répartition des modes d’accès au réseau\nLe réseau propose deux canaux de diffusion de l’information: une liste de diffusion par mail et un canal de discussion instantanée qui utilise la messagerie sécurisée de l’Etat Tchap. Environ 55% des membres de la liste de diffusion (soit plus de 180 personnes) sont également inscrits sur le canal de discussion instantanée."
  },
  {
    "objectID": "blog/recap2022/index.html#composition-du-réseau",
    "href": "blog/recap2022/index.html#composition-du-réseau",
    "title": "Infolettre n°9",
    "section": "Composition du réseau",
    "text": "Composition du réseau\nLa diffusion d’informations par le réseau a permis de réunir des data scientists de 27 organisations différentes. L’Insee, qui représente 47% de l’effectif du réseau, est majoritaire. Suivent dans le palmarès, les services statistiques du Ministère de la Santé (DREES) et du Ministère du Développement Durable (SDES)."
  },
  {
    "objectID": "blog/recap2022/index.html#évolution-de-la-composition-du-réseau",
    "href": "blog/recap2022/index.html#évolution-de-la-composition-du-réseau",
    "title": "Infolettre n°9",
    "section": "Évolution de la composition du réseau",
    "text": "Évolution de la composition du réseau\nLa diffusion progressive d’informations par le biais des newsletters a permis de diversifier progressivement la composition de la liste de diffusion. Alors que la première newsletter de l’année 2022 avait été diffusée auprès de 14 institutions, ce sont des agents de 27 organisations qui ont reçues la dernière.\nLes événements organisés par le réseau ou les présentations spéciales, comme celle pour les administrateurs de l’INSEE en poste à l’ENSAE, ont également pu motiver des personnes à intégrer le réseau."
  },
  {
    "objectID": "blog/recap2022/index.html#programme-10",
    "href": "blog/recap2022/index.html#programme-10",
    "title": "Infolettre n°9",
    "section": "Programme 10%",
    "text": "Programme 10%\nLes membres du réseau des data scientists ont été particulièrement actifs dans le cadre du programme interministériel 10%, issu des recommandations d’un rapport INSEE-DINUM “Évaluation des besoins de l’État en compétences et expertises en matière de donnée”.\nLa saison 1 a donné sa chance à quatre projets, portés par différentes administrations. Si l’un d’eux existait déjà depuis plus de deux ans (projet Gouvdown), trois sont nés pour l’occasion, avec la mise en ligne de code immédiate (Cartiflette) ou postérieure au bootcamp de lancement (Socratext et matchSIRET) .\nTous les projets sont ouverts et disponible sur Github. Une statistique qui permet de représenter leur succès est le nombre de ⭐: c’est un peu un mélange entre un site en favori sous Firefox puisque cela permet de facilement retrouver un projet dans Github et le nombre de followers d’une page sur Facebook ou sur Twitter puisque cela permet de suivre l’activité d’un dépôt Github.\n\n\n\n\n\n\nNote\n\n\n\nCette visualisation fait appel à l’API Github. Si les figures ne s’affichent pas, cela peut être dû à un dépassement du nombre de requêtes par heure autorisées par l’API Github sans jeton. A l’heure actuelle, il n’existe pas encore de fonctionalité gratuite sous Observable pour stocker de manière sécurisée un jeton pour l’API Github.\n\n\nDérouler pour afficher une version non réactive\n\n\n\n\nProjet cartiflette\n\n\n\n\n\nProjet Socratext\n\n\n\n\n\nProjet Gouvdown\n\n\n\n\n\nProjet matchSIRET"
  },
  {
    "objectID": "blog/recap2022/index.html#autres-actualités-du-réseau",
    "href": "blog/recap2022/index.html#autres-actualités-du-réseau",
    "title": "Infolettre n°9",
    "section": "Autres actualités du réseau",
    "text": "Autres actualités du réseau\n\nPrésentation de Gridviz par Julien Gaffuri\nPour rappel, le 20 Janvier 2023 de 11h à 12h30 Julien Gaffuri (Eurostat) viendra nous présenter la librairie open-source Gridviz. Réservez ce créneau pour découvrir cette librairie qui ouvre de nouvelles perspectives pour la mise à disposition de données géographiques !\nTélécharger l’invitation à l’événement sous format Outlook\n\n\n\nSource: Notebook Hello Gridviz par neocarto sur Observable\n\n\n\n\nPremière place européenne au hackathon Big Data de l’ONU\nLes résultats du hackathon big data de l’ONU, ayant eu lieu du 7 au Novembre 2022, ont été annoncés ! L’équipe Datadive - constituée de membres du réseau de l’INSEE, de la DGFIP et du CASD - est arrivée à la première place des équipes européennes 🎉.\n\n\nGit et bonnes pratiques: des formations de formateurs prévus pour les statisticiens publics\nLes nouvelles formations à Git et aux bonnes pratiques avec R, testées récemment à l’Insee et au service statistique du Ministère du Travail, la DARES, (voir newsletters de Novembre et Décembre), deviennent des formations nationales.\nPour pouvoir diffuser les bonnes pratiques favorisant le partage de codes et la qualité des projets statistiques, il est nécessaire d’avoir le plus d’enseignants possibles pour cette formation. Pour permettre cela, un appel à candidat pour une formation de formateurs a été diffusée à l’Insee et dans les services statistiques ministériels. Si vous êtes intéressés et ne l’avez pas reçu, n’hésitez pas à envoyer un mail à contact-ssphub@insee.fr.\nEn attendant, les supports de ces formations sont déjà disponibles sur inseefrlab.github.io/formation-bonnes-pratiques-git/ et sur inseefrlab.github.io/formation-bonnes-pratiques-R/. Les codes sources sont bien-sûr ouverts et disponibles sur Github, tant pour la première partie que pour la seconde. Ceux-ci sont construits collectivement, n’hésitez pas à suggérer des modifications depuis Github.\nUn site web plus complet devrait prochainement voir le jour pour accompagner cette formation. En complément de celui-ci, des éléments peuvent déjà être trouvés dans le cours de 3e année de l’ENSAE sur la mise en production de projets data science et dans la documentation collaborative utilitR."
  },
  {
    "objectID": "blog/recap2022/index.html#footnotes",
    "href": "blog/recap2022/index.html#footnotes",
    "title": "Infolettre n°9",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nLes actions de communication du mois de janvier ont permis d’augmenter sensiblement le nombre de personnes dans cette liste (340 début janvier). Un retour spécial sur le mois de janvier sera l’occasion idéale pour une autre rétrospective quantitative.↩︎"
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Ressources utiles",
    "section": "",
    "text": "Une sélection de ressources utiles pour se former ou se perfectionner à la data science.\nN’hésitez pas à soumettre les ressources que vous jugez utiles sur notre GitHub .\n\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Les formations R du MTES\n            \n\n            \n              Une collection de cours `R` faite par le Ministère de la Transition Ecologique\n\n            \n\n            \n            \n              \n                \n                1 janv. 2024\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Appariements de données individuelles : concepts, méthodes, conseils\n            \n\n            \n              Un appariement consiste à rapprocher deux bases de données d’origine distincte partageant des unités statistiques communes mais contenant des informations différentes. Ce document de travail porte sur les cas où un identifiant commun n'existe pas et introduit en pratique les appariements de données individuelles sur traits d'identité.\n            \n\n            \n            \n              \n                \n                3 juil. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Cours sur la plateforme de dataviz Observable\n            \n\n            \n              When do you use a bar chart over a line chart?\nWhat are area charts good for?\nWhat's wrong with pie charts?\nLearn about how these different types of data visualization work,\n and how they're used, in Observable's first data visualization course!\nAttend lectures (or watch them later), ask questions, and once you've\ncompleted a small assignment at the end, you'll earn a certificate.\n\n            \n\n            \n            \n              \n                \n                1 avr. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Les réseaux de neurones appliqués à la statistique publique : méthodes et cas d’usages\n            \n\n            \n              Ce document de travail propose une introduction rapide aux réseaux de neurones, de leurs fondements théoriques jusqu’à leur mise en oeuvre pratique en R et python sur des problématiques spécifiques de statistique publique. Il  illustre les possibilités et les limites à travers trois cas d’usage détaillés sur  l’imputation de valeurs manquantes dans une enquête, l’exploitation de fichiers d’images et la réduction de dimension.\n            \n\n            \n            \n              \n                \n                16 févr. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Parcours complet sur le MLOps\n            \n\n            \n              Un site web très complet qui fait un effort\nde synthèse sur l'écosystème foisonnant du MLOps\n\n            \n\n            \n            \n              \n                \n                1 févr. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Neural Network: from zero to hero\n            \n\n            \n              A course by Andrej Karpathy on building neural networks, from scratch, in code.\n\n\n  \"We start with the basics of backpropagation and build up to modern deep neural networks, like GPT. Language models are an excellent place to learn deep learning, even if your intention is to eventually go to other areas like computer vision because most of what you learn will be immediately transferable.\"\n\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Observable pour la cartographie\n            \n\n            \n              Nicolas Lambert (neocarto) propose\nbeaucoup de ressources pédagogiques sur la cartographie depuis Observable.\nBeaucoup de ressources s'appuient\nsur bertin.js,\nune librairie très puissante et flexible pour la représentation cartographique.\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Data visualisation with d3.js\n            \n\n            \n              d3.js est la librairie favorite des spécialistes de\ndataviz en Javascript. Dans cette série de notebooks,\nproposée par Arthur Katossky,\nvous découvrirez comment utiliser la librairie pour\nconstruire des visualisations de données réactives.\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Découvrir Observable avec des données françaises\n            \n\n            \n              Eric Mauvière propose\nbeaucoup de ressources pédagogiques sur la plateforme Observable.\nBeaucoup s'appuient\nsur des données de la statistique publique, comme le fichier des\nprénoms ou le recensement.\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Introduction à Observable Plot.js\n            \n\n            \n              La librairie Plot.js\nvise à faciliter l'utilisation des fonctionnalités graphiques de\nJavascript . Elle propose une syntaxe très proche de celle\ndes librairies ggplot2 () ou seaborn (Python ).\n\n            \n\n            \n            \n              \n                \n                16 janv. 2023\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Python pour la data science\n            \n\n            \n              Un site web complet pour découvrir la richesse\nde Python  pour la data science. Ce cours\nest enseigné par Lino Galiana en\ndeuxième année (Master 1) de l'ENSAE.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Mise en production de projets de data science\n            \n\n            \n              Un site web complet pour découvrir la manière dont\ndes projets data-science peuvent être valorisés\net maintenus dans le temps.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Portail de la formation du SSPCloud\n            \n\n            \n              Le SSPCloud, la plateforme cloud\ndéveloppée par l'Insee, propose\nun certain nombre de tutoriels\nen Python  ou R .\n\n            \n\n            \n            \n              \n                \n                27 avr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Géomatique appliquée à la statistique\n            \n\n            \n              Une présentation simple des outils géomatiques récents qui permettent de stocker, traiter et diffuser l'information spatiale\n\n            \n\n            \n            \n              \n                \n                17 févr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            L'économétrie en grande dimension\n            \n\n            \n              Ce document de travail est une courte introduction aux principaux problèmes que l'on rencontre lorsque l'on souhaite faire de l'économétrie en grande dimension, c'est-à-dire lorsque p &gt; n - pour chaque observation, on dispose d'un nombre de caractéristiques potentiellement proportionnel ou plus grand que la taille de l'échantillon.\n\n            \n\n            \n            \n              \n                \n                17 févr. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Utiliser Git dans Jupyter Notebook ?\n            \n\n            \n              Un extrait du cours\nde Python pour la data-science\nde l'ENSAE.\n\n            \n\n            \n            \n              \n                \n                1 janv. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            dataESR: portail de l'open-data du Ministère de l'Enseignement Supérieur\n            \n\n            \n              #dataESR est un portail développé\npar le service statistique du Ministère de l'Enseignement Supérieur et de la Recherche\npour vous aider à trouver les ressources en données sur l'enseignement supérieur, la recherche et l'innovation.\n\n            \n\n            \n            \n              \n                \n                1 janv. 2022\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            DevOps for Data Science\n            \n\n            \n              Un site web très complet sur la manière\ndont l'approche DevOps peut être\nimportée dans des projets data-science.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            R reproducibility toolkit for the practical researcher\n            \n\n            \n              Un site web très complet développé par des universitaires\nsud-américains pour présenter la manière dont les\nprojets en R peuvent être construits de manière\nreproductible.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            R for Data Science\n            \n\n            \n              Un incontournable écrit par Hadley Wickham\nafin de faire découvrir l'univers du\ntidyverse (ggplot, dplyr...) de manière\nthématique.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            R Packages\n            \n\n            \n              Un incontournable écrit par Hadley Wickham\nafin d'apprendre à développer\ndes packages R.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            RZine, une collection de ressources utiles en R\n            \n\n            \n              Un site web développé par l'université Paris Diderot\ncomportant de nombreuses ressources utiles en R.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Machine Learning in Python with scikit-learn\n            \n\n            \n              Un MOOC de l'INRIA sur scikit-learn, l'écosystème\ncentral du Machine Learning en Python.\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            utilitR\n            \n\n            \n              \n  Le projet utilitR est une documentation sur l’usage du logiciel\n  , née à l’Insee,\n  destinée à tout utilisateur intéressé par la manipulation de données.\n\n\n\n  La documentation utilitR ne fait aucun pré-requis de niveau:\n  à la fois le débutant\n  et l'utilisateur plus expert désirant découvrir un nouveau champ ou bénéficier\n  d'une aide-mémoire pourront trouver du contenu qui les intéresse.\n\n\n\n  Afin que les exemples soient concrets,\n  tous les jeux de données sont issus du\n  site de l'Insee.\n\n\n            \n\n            \n            \n              \n                \n                27 avr. 2021\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            The Hitchhiker's guide to Python\n            \n\n            \n              Un livre de référence\nsur les bonnes pratiques pour des\nprojets Python .\n\n            \n\n            \n            \n              \n                \n                27 avr. 2016\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Google’s R Style Guide\n            \n\n            \n              Un guide de référence sur les bonnes pratiques\ndans l'écriture de code R\n\n            \n\n            \n            \n              \n                \n                27 avr. 2016\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Formation Initiation au Deep Learning (FIDLE)\n            \n\n            \n              Une formation du CNRS sur les problématiques\nd'apprentissage automatique (machine learning)\net d'apprentissage profond (deep learning).\n\n            \n\n            \n            \n              \n                \n                27 avr. 2016\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Apprendre Pandas en 10 minutes !\n            \n\n            \n              Un tutoriel des créateurs de Pandas pour apprendre en\npeu de temps à manipuler et analyser\nles données sous Python .\n\n            \n\n            \n            \n              \n                \n                27 avr. 2016\n              \n            \n            \n            \n          \n        \n\n        \n\n      \n     \n  \n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "event/2021-06-22-funathon/index.html",
    "href": "event/2021-06-22-funathon/index.html",
    "title": "Funathon de juin 2021",
    "section": "",
    "text": "Présentation\nLa première édition du Funathon a eu lieu le 21 et 22 juin 2021 et a réuni environ 150 agents, souvent réunis en équipe. Cette première édition a tourné autour de l’utilisation des données d’Airbnb, 8 sujets plus ou moins guidés étant proposés, avec chaque fois des notebooks (en R et en Python) permettant d’entrer facilement dans les sujets. Tous les niveaux d’expertise étaient les bievenus. Merci à tous !\n\n\nDétails du programme des deux jours\nCher.e.s Funambules,\nVous allez pouvoir vous consacrer cœur et âme pendant deux jours aux joies de la Datascience. Cette expérience sera à la fois longue et courte. Longue car nos calendriers surchargés nous permettent rarement de travailler en collaboration sur un sujet statistique. Mais trop courte, car s’approprier les données, imaginer leurs usages, tester des méthodes de Machine Learning demandent du temps et de la patience. Avant de commencer votre exploration, nous vous recommandons de bien lire les documents et les aides mis à disposition (nous remercions l’équipe du SSPLab pour son coup de main). Nous vous suggérons également de ne pas négliger la partie statistique descriptive, sans quoi vous risquez de mal interpréter les résultats de vos analyses plus complexes.\nLes données proviennent du site Inside Airbnb qui est devenu un acteur de premier plan dans la connaissance de Airbnb. Soutenu par les grandes métropoles européennes et mondiales, le site propose des données scrappées du site Airbnb. Ces données sont utilisées par les agences d’urbanismes (Urban planning) et la recherche académique pour documenter les conséquences des locations Airbnb sur les systèmes urbains. Ces données contiennent par construction des informations personnelles éventuellement identifiantes. Il est interdit d’en faire un usage commercial, ce qui va de soi dans le cadre du Funathon qui est une opération de formation et d’appropriation des outils de Machine Learning. En revanche, en tant que données personnelles, elles doivent être traitées par nous les Funambules, acteurs de la connaissance statistique, avec la même exigence de rigueur et d’éthique que toutes autres sources de données statistiques ou administratives.\nPendant ces deux jours, certain.e.s Funambules vont découvrir un nouvel outil de travail, le Datalab. Cet outil que nous offre l’équipe de la DIIT (que nous remercions chaleureusement) est l’Armageddon de la Datascience. Sa puissance est redoutable et grisante. En contrepartie, en tant qu’outil en devenir, son appropriation demande plus de temps. N’hésitez pas à consulter le canal général de slack pour poser des questions et y trouver éventuellement des réponses à vos interrogations.\nLa Datascience n’est pas seulement l’apanage des plus aguerri.e.s ou des plus virtuoses en informatique. Les esprits curieux ont toute leur place pour penser les données et proposer des approches innovantes. La critique dans les équipes des résultats obtenus nous semble indispensable et ce d’autant plus que les données ne sont pas parfaites. Vos analyses seront forcément biaisées et il est important de connaître et discuter leurs limites.\nN’oubliez pas, nous sommes des nains sur des épaules de géants, alors n’hesitez pas à recopier du code.\nL’ensemble du materiel des deux jours est disponible ici. Premier élément à ouvrir : 0_Kit_De_Demarrage. Puis, laissez vous guider !"
  },
  {
    "objectID": "event/2022-12-10prcent/index.html",
    "href": "event/2022-12-10prcent/index.html",
    "title": "Evénement de clôture de la saison 1 du programme 10%",
    "section": "",
    "text": "La DINUM et l’INSEE ont lancé en 2022 une expérimentation visant à encourager les experts de la donnée à consacrer au moins 10 % de leur temps de travail à des projets d’intérêt partagé à une échelleinterministérielle.\nCette expérimentation a permis la mise en place d’une communauté d’une cinquantaine d’experts publics de la donnée qui s’est mobilisée sur 4 projets d’intérêt commun autour de la data et de l’IA.\nLa saison 1 se clôture après six mois d’intense activité. L’occasion de faire le bilan avec des restitutions des projets portés cette année. Ces restitutions seront suivies d’une table ronde sur le sujet “Comment attirer et fidéliser les talents de la data au sein de l’Etat ?” avec des échanges autour de pratiques inspirantes issues du public (INSEE, Pôle Emploi, DINUM), du privé (MAIF), du milieu associatif (Data for good) et de l’étranger (Canada).\nLes projets portés dans le cadre de la saison 1 sont disponibles ici.\nLien de l’événement: https://www.eventbrite.fr/e/billets-evenement-de-cloture-de-la-saison-1-du-programme-10-460655772817"
  },
  {
    "objectID": "event/2022-12-10prcent/index.html#h00-15h00-pitchs-des-projets-dintérêt-commun",
    "href": "event/2022-12-10prcent/index.html#h00-15h00-pitchs-des-projets-dintérêt-commun",
    "title": "Evénement de clôture de la saison 1 du programme 10%",
    "section": "14h00-15h00 : Pitchs des projets d’intérêt commun",
    "text": "14h00-15h00 : Pitchs des projets d’intérêt commun\n\nsocratext : Une solution pour extraire des informations contenues dans des documents non directement exploitables (PDF, images, photos …) afin d’accélérer certaines tâches très chronophages lorsqu’elles sont réalisées à la main par les agents publics (par exemple, extraire les informations des tickets de caisse pour l’enquête budget des familles de l’INSEE ou bien cibler des informations précises dans des rapports médicaux de plusieurs pages).\n\nMinistères participants : Direction interministérielle du numérique, INSEE, ministère des Armées, ministère de l’Intérieur, Haute Autorité de Santé.\n\ngouvdown et shinygouv : une solution pour créer facilement des applications et des documents conformes à la charte graphique de l’État et faire gagner du temps aux agents.\n\nMinistères participants : INSEE, direction interministérielle de la transformation publique et réseau des DREAL\n\nMatchSIRET : une solution pour faciliter et fiabiliser le quotidien des agents en leur donnant le moyen de remonter à l’identifiant exact d’une entreprise à partir d’informations issues du terrain (le nom commercial, la localisation ou le secteur d’activité).\n\nMinistères participants : Direction interministérielle du numérique, ministère du Travail, de l’Emploi et de l’Insertion\n\ncartiflette : une solution pour harmoniser et associer des données géographiques issues de différents acteurs publics (INSEE, IGN, collectivités locales) à des contours géographiques normalisés via une API flexible. Ce projet sera associé à un site web, permettant ainsi de faciliter le travail des chercheurs, enseignants, administrations et médias souhaitant réaliser une carte à partir de données géographiques.\n\nMinistères participants : INSEE, ministère de la Santé, ministère de l’Intérieur et ministère de la Transition écologique, enseignants-chercheurs"
  },
  {
    "objectID": "event/2022-12-10prcent/index.html#h00---16h00---table-ronde-comment-attirer-et-fidéliser-les-talents-de-la-data-au-sein-de-letat",
    "href": "event/2022-12-10prcent/index.html#h00---16h00---table-ronde-comment-attirer-et-fidéliser-les-talents-de-la-data-au-sein-de-letat",
    "title": "Evénement de clôture de la saison 1 du programme 10%",
    "section": "15h00 - 16h00 - Table ronde : Comment attirer et fidéliser les talents de la data au sein de l’Etat ?",
    "text": "15h00 - 16h00 - Table ronde : Comment attirer et fidéliser les talents de la data au sein de l’Etat ?\n\nPartage de bonnes pratiques issues de la sphère publique :\n\nRomain Lesur, chef de la division innovation à l’INSEE ;\nStéphane Campion, chargé des relations externes sur l’IA et la Data à Pôle Emploi ;\nSoraya SAA, cheffe de la mission Talents ;\nSophie RAVEL, chargée de projet attractivité à la mission Talents de la DINUM.\n\nRegards croisés : initiatives issues de l’étranger, du privé et du milieu associatif :\n\nRegard de l’étranger : Antoine Augusti, datascientist ayant travaillé au Service numérique canadien\nRegard du privé : Yann Golhen, datascientist à la MAIF\nRegard du milieu associatif : Théo Alves Da Costa, Data For Good\n\n\n16h00: Pot final"
  },
  {
    "objectID": "event/2024-02-29-mauviere/index.html",
    "href": "event/2024-02-29-mauviere/index.html",
    "title": "Eric Mauvière, “La dataviz pour donner du sens aux données et communiquer un message”",
    "section": "",
    "text": "29 février (15h - 16h)\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-02-09-mauviere/conf_ssphub_item7-1.pdf\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Télécharger les slides\n  &lt;/a&gt;\n&lt;/p&gt;`\n\n\n\n\n\n\nUn exemple issu de la présentation d’Eric :"
  },
  {
    "objectID": "event/2024-02-29-mauviere/index.html#eric-mauvière-la-dataviz-pour-donner-du-sens-aux-données-et-communiquer-un-message",
    "href": "event/2024-02-29-mauviere/index.html#eric-mauvière-la-dataviz-pour-donner-du-sens-aux-données-et-communiquer-un-message",
    "title": "Eric Mauvière, “La dataviz pour donner du sens aux données et communiquer un message”",
    "section": "",
    "text": "29 février (15h - 16h)\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\n\nslides = \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-02-09-mauviere/conf_ssphub_item7-1.pdf\"\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Télécharger les slides\n  &lt;/a&gt;\n&lt;/p&gt;`\n\n\n\n\n\n\nUn exemple issu de la présentation d’Eric :"
  },
  {
    "objectID": "event/2024-10-14-network-day/index.html",
    "href": "event/2024-10-14-network-day/index.html",
    "title": "Deuxième journée du SSPHub",
    "section": "",
    "text": "Les sessions plénières de la journée du réseau sont disponibles ci-dessous:\n\n\nSéquencement de la vidéo et slides\n\n\n00:00-03:30: Introduction de la journée par Romain Lesur\n03:30-46:30: “Le blé vu du ciel : images satellitaires et prédiction des rendements agricoles à l’échelle de la parcelle” (Service statistique ministériel du Ministère de l’Agriculture, de la souveraineté alimentaire et de la Forêt).\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/agriculture.pdf\",\n  \"Télécharger les slides du SSM Agriculture\"\n)\n\n\n\n\n\n\n\n46:30-83:00: “Identifier et classer les causes de décès en automatisant le traitement des certificats en langage naturel” (CépiDC, Inserm)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/cepidc.pdf\",\n  \"Télécharger les slides du CépiDC\"\n)\n\n\n\n\n\n\n\n86:00-123:00: Keynote de Pascal Rivière (chef de l’Inspection générale de l’Insee) “Data science et statistique publique : contexte institutionnel et évolutions”\n123:00-164:00: “Extraction automatisée de tableaux dans des PDF pour la construction de statistiques d’entreprises” (Insee)\n\n\ncreateButton(\n  \"https://ssplab.pages.lab.sspcloud.fr/table-extraction-evaluation/#/title-slide\",\n  \"Voir les slides de l'Insee\"\n)\n\n\n\n\n\n\n\n164:00-205:00: “Scraper et retravailler les offres d’emploi en ligne pour permettre des analyses fines du marché du travail, le projet JOCAS” (Dares: Service statistique ministériel du Ministère du Travail et de l’Emploi)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/jocas.pptx\",\n  \"Télécharger les slides de la DARES\"\n)\n\n\n\n\n\n\n\nA partir de 205:00: Keynote de Pierre Etienne Devineau (ex-DINUM) sur les enjeux rencontrés par le projet Albert autour de l’entraînement et de la mise en production de grands modèles de langage (LLM) francophones.\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/albert.pptx\",\n  \"Télécharger les slides de la keynote\"\n)\n\n\n\n\n\n\n\nQuelques photos de l’événement:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: \n\n\n\nLes slides des différentes présentations peuvent être téléchargées ⬆️\n\nfunction createButton(slides, message=\"Télécharger les slides\"){\n  const button = html`\n  &lt;p class=\"text-center\"&gt;\n    &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n      &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;${message}\n    &lt;/a&gt;\n  &lt;/p&gt;`\n  return button\n}\n\n\n\n\n\n\n\n\nRappel du programme de la journée\n\n\n9h30-10h: Accueil, moment de convivialité autour d’un café.\n10h-11h30: Retours d’expérience de projets innovants autour de la classification de textes ou d’images :\n\nLe blé vu du ciel : images satellitaires et prédiction des rendements agricoles à l’échelle de la parcelle (SSM Agriculture) ;\nIdentifier et classer les causes de décès en automatisant le traitement des certificats en langage naturel (CépiDC, Inserm).\n\n11h30-12h15: Atelier de réflexion autour des applications des méthodes de classification de textes ou d’images sur les données des * participants.\n12h15-14h: Pause déjeuner, moment de convivialité.\n14h-14h30: Pascal Rivière (Chef de l’inspection générale de l’Insee) interviendra sur le thème “Data science et statistique publique : contexte institutionnel et évolutions”.\n14h30-16h: Retours d’expérience de projets faisant intervenir un processus innovant d’extraction de données :\n\nExtraction automatisée de tableaux dans des PDF pour la construction de statistiques d’entreprises (Insee) ;\nScraper et retravailler les offres d’emploi en ligne pour permettre des analyses fines du marché du travail, le projet JOCAS (Dares, SSM Travail) ;\n\n16h-16h30: Pause, moment de convivialité.\n16h30-17h30: Keynote par Pierre Etienne Devineau (Ministères sociaux) et Léo Guillaume (Dinum). Les enjeux rencontrés par le projet Albert autour de l’entraînement et de la mise en production de grands modèles de langage (LLM) francophones.\n17h30-19h30: Pot, moment de convivialité.\n\n\nPour toute information : contact-ssphub@insee.fr\n📺️ La première journée du réseau ayant eu lieu en 2023 est également disponible en replay."
  },
  {
    "objectID": "event/2024-10-14-network-day/index.html#replay-de-la-deuxième-journée-du-ssphub-le-14-octobre-2024",
    "href": "event/2024-10-14-network-day/index.html#replay-de-la-deuxième-journée-du-ssphub-le-14-octobre-2024",
    "title": "Deuxième journée du SSPHub",
    "section": "",
    "text": "Les sessions plénières de la journée du réseau sont disponibles ci-dessous:\n\n\nSéquencement de la vidéo et slides\n\n\n00:00-03:30: Introduction de la journée par Romain Lesur\n03:30-46:30: “Le blé vu du ciel : images satellitaires et prédiction des rendements agricoles à l’échelle de la parcelle” (Service statistique ministériel du Ministère de l’Agriculture, de la souveraineté alimentaire et de la Forêt).\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/agriculture.pdf\",\n  \"Télécharger les slides du SSM Agriculture\"\n)\n\n\n\n\n\n\n\n46:30-83:00: “Identifier et classer les causes de décès en automatisant le traitement des certificats en langage naturel” (CépiDC, Inserm)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/cepidc.pdf\",\n  \"Télécharger les slides du CépiDC\"\n)\n\n\n\n\n\n\n\n86:00-123:00: Keynote de Pascal Rivière (chef de l’Inspection générale de l’Insee) “Data science et statistique publique : contexte institutionnel et évolutions”\n123:00-164:00: “Extraction automatisée de tableaux dans des PDF pour la construction de statistiques d’entreprises” (Insee)\n\n\ncreateButton(\n  \"https://ssplab.pages.lab.sspcloud.fr/table-extraction-evaluation/#/title-slide\",\n  \"Voir les slides de l'Insee\"\n)\n\n\n\n\n\n\n\n164:00-205:00: “Scraper et retravailler les offres d’emploi en ligne pour permettre des analyses fines du marché du travail, le projet JOCAS” (Dares: Service statistique ministériel du Ministère du Travail et de l’Emploi)\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/jocas.pptx\",\n  \"Télécharger les slides de la DARES\"\n)\n\n\n\n\n\n\n\nA partir de 205:00: Keynote de Pierre Etienne Devineau (ex-DINUM) sur les enjeux rencontrés par le projet Albert autour de l’entraînement et de la mise en production de grands modèles de langage (LLM) francophones.\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2024-10-14-network/albert.pptx\",\n  \"Télécharger les slides de la keynote\"\n)\n\n\n\n\n\n\n\nQuelques photos de l’événement:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: \n\n\n\nLes slides des différentes présentations peuvent être téléchargées ⬆️\n\nfunction createButton(slides, message=\"Télécharger les slides\"){\n  const button = html`\n  &lt;p class=\"text-center\"&gt;\n    &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n      &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;${message}\n    &lt;/a&gt;\n  &lt;/p&gt;`\n  return button\n}\n\n\n\n\n\n\n\n\nRappel du programme de la journée\n\n\n9h30-10h: Accueil, moment de convivialité autour d’un café.\n10h-11h30: Retours d’expérience de projets innovants autour de la classification de textes ou d’images :\n\nLe blé vu du ciel : images satellitaires et prédiction des rendements agricoles à l’échelle de la parcelle (SSM Agriculture) ;\nIdentifier et classer les causes de décès en automatisant le traitement des certificats en langage naturel (CépiDC, Inserm).\n\n11h30-12h15: Atelier de réflexion autour des applications des méthodes de classification de textes ou d’images sur les données des * participants.\n12h15-14h: Pause déjeuner, moment de convivialité.\n14h-14h30: Pascal Rivière (Chef de l’inspection générale de l’Insee) interviendra sur le thème “Data science et statistique publique : contexte institutionnel et évolutions”.\n14h30-16h: Retours d’expérience de projets faisant intervenir un processus innovant d’extraction de données :\n\nExtraction automatisée de tableaux dans des PDF pour la construction de statistiques d’entreprises (Insee) ;\nScraper et retravailler les offres d’emploi en ligne pour permettre des analyses fines du marché du travail, le projet JOCAS (Dares, SSM Travail) ;\n\n16h-16h30: Pause, moment de convivialité.\n16h30-17h30: Keynote par Pierre Etienne Devineau (Ministères sociaux) et Léo Guillaume (Dinum). Les enjeux rencontrés par le projet Albert autour de l’entraînement et de la mise en production de grands modèles de langage (LLM) francophones.\n17h30-19h30: Pot, moment de convivialité.\n\n\nPour toute information : contact-ssphub@insee.fr\n📺️ La première journée du réseau ayant eu lieu en 2023 est également disponible en replay."
  },
  {
    "objectID": "event/2025-04-16-Parquet/index.html",
    "href": "event/2025-04-16-Parquet/index.html",
    "title": "Atelier - Comment récupérer des données sous format Parquet ?",
    "section": "",
    "text": "L’atelier a eu lieu le 16 avril 2025 (15h - 16h30), en présentiel à l’Insee et en distanciel pour les membres du réseau du SSP Hub. Environ 35 personnes ont participé de l’Insee (DG ou directions régionales), de différents services statistiques ministériels ou d’autres horizons. Merci à tous pour les échanges !\n\nSlides de la présentation\n\n\n\n\nhtml`${slides_button}`\n\n\n\n\n\n\n\nslides_button = html`&lt;p class=\"text-center\"&gt;\n  &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"https://inseefrlab.github.io/ssphub-ateliers-slides/slides-data/parquet#/title-slide\" target=\"_blank\"&gt;\n    &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;Voir les slides\n  &lt;/a&gt;\n&lt;/p&gt;`\n\n\n\n\n\n\n\n\nDocumentation de l’atelier & replay\nLe matériel lié à l’atelier, y compris le replay, est disponible ici. \n\n\nQuestions / contact\nSi vous avez la moindre question 🤨, n’hésitez pas à contacter 📧 contact-ssphub@insee.fr."
  },
  {
    "objectID": "event/2026-01-cartographia/index.html",
    "href": "event/2026-01-cartographia/index.html",
    "title": "Françoise Bahoken et Nicolas Lambert, présentation de leur livre Cartographia",
    "section": "",
    "text": "Replay de la présentation :\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2026-01-cartographia/2026-01-13-Cartographia.pdf\"&gt;Download the PDF instead.&lt;/a&gt;\n&lt;/p&gt;\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2026-01-cartographia/2026-01-13-Cartographia.pdf\",\n  \"Télécharger les slides\"\n)\n\n\n\n\n\n\n\nfunction createButton(slides, message=\"Télécharger les slides\"){\n  const button = html`\n  &lt;p class=\"text-center\"&gt;\n    &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n      &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;${message}\n    &lt;/a&gt;\n  &lt;/p&gt;`\n  return button\n}"
  },
  {
    "objectID": "event/2026-01-cartographia/index.html#cartographia---comment-les-géographes-redessinent-le-monde---françoise-bahoken-et-nicolas-lambert",
    "href": "event/2026-01-cartographia/index.html#cartographia---comment-les-géographes-redessinent-le-monde---françoise-bahoken-et-nicolas-lambert",
    "title": "Françoise Bahoken et Nicolas Lambert, présentation de leur livre Cartographia",
    "section": "",
    "text": "Replay de la présentation :\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2026-01-cartographia/2026-01-13-Cartographia.pdf\"&gt;Download the PDF instead.&lt;/a&gt;\n&lt;/p&gt;\n\n\ncreateButton(\n  \"https://minio.lab.sspcloud.fr/ssphub/diffusion/website/2026-01-cartographia/2026-01-13-Cartographia.pdf\",\n  \"Télécharger les slides\"\n)\n\n\n\n\n\n\n\nfunction createButton(slides, message=\"Télécharger les slides\"){\n  const button = html`\n  &lt;p class=\"text-center\"&gt;\n    &lt;a class=\"btn btn-primary btn-lg cv-download\" href=\"${slides}\" target=\"_blank\"&gt;\n      &lt;i class=\"fa-solid fa-file-arrow-down\"&gt;&lt;/i&gt;&ensp;${message}\n    &lt;/a&gt;\n  &lt;/p&gt;`\n  return button\n}"
  },
  {
    "objectID": "event/presentation-de-gridviz-par-julien-gaffuri/index.html",
    "href": "event/presentation-de-gridviz-par-julien-gaffuri/index.html",
    "title": "Présentation de gridviz par Julien Gaffuri",
    "section": "",
    "text": "Présentation par Julien Gaffuri de la librairie Gridviz. Le replay est disponible plus bas 👇\nGridviz est une librairie open source (disponible sur Github) consacrée à la visualisation cartographique de données carroyées (ou données géolocalisées par des (x,y) à carroyer). Très efficace, elle permet de représenter de manière fluide des volumes importants de données."
  },
  {
    "objectID": "event/presentation-de-gridviz-par-julien-gaffuri/index.html#replay",
    "href": "event/presentation-de-gridviz-par-julien-gaffuri/index.html#replay",
    "title": "Présentation de gridviz par Julien Gaffuri",
    "section": "Support et replay",
    "text": "Support et replay\nLe support est disponible ici."
  },
  {
    "objectID": "event/presentation-dobservable-par-nicolas-lambert/index.html",
    "href": "event/presentation-dobservable-par-nicolas-lambert/index.html",
    "title": "Présentation d’Observable par Nicolas Lambert",
    "section": "",
    "text": "Présentation d’observable par Nicolas Lambert sous la forme d’un notebook interactif.\nLe replay est disponible plus bas 👇\nobservable est une plateforme de dataviz réactive qui propose des notebooks communautaires de visualisations de données:\nQuelques ressources supplémentaires utiles:"
  },
  {
    "objectID": "event/presentation-dobservable-par-nicolas-lambert/index.html#replay",
    "href": "event/presentation-dobservable-par-nicolas-lambert/index.html#replay",
    "title": "Présentation d’Observable par Nicolas Lambert",
    "section": "Replay",
    "text": "Replay"
  },
  {
    "objectID": "infolettre/infolettre.html",
    "href": "infolettre/infolettre.html",
    "title": "Infolettres",
    "section": "",
    "text": "La première infographie\n\n\nInfolettre du mois de janvier 2026\n\n\n\n\n\n\n30 janv. 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nQui pour financer l’open source?\n\n\nInfolettre du mois de décembre 2025\n\n\n\n\n\n\n10 déc. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe belles cartographies, des packages R et l’importance des données d’entraînement pour l’IA\n\n\nInfolettre du mois d’octobre 2025\n\n\n\n\n\n\n25 oct. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa rentrée 2025: actualités, nouveautés, interview de rentrée\n\n\nInfolettre du mois de Septembre 2025\n\n\n\n\n\n\n29 sept. 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSora, la nouvelle IA d’OpenIA pour générer des vidéos ; Le Chat, le nouveau modèle de Mistral ; Observable, pour s’abstraire des notebooks\n\n\nInfolettre du mois de Mars 2024\n\n\n\n\n\n\n7 mars 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web\n\n\nInfolettre du mois de Février 2024\n\n\n\n\n\n\n20 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nRétrospective du réseau en 2023 (cocorico, beaucoup de nouveaux inscrits !) ; des nouvelles règles européennes pour l’IA ; le recensement de la population au format parquet ; un explorateur de fichier sur le SSPCloud\n\n\nInfolettre du mois de Décembre 2023\n\n\n\n\n\n\n21 déc. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM\n\n\nInfolettre de rentrée, Septembre 2023\n\n\n\n\n\n\n10 sept. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropositions de lecture estivale\n\n\nInfolettre estivale, Juillet 2023\n\n\n\n\n\n\n1 juil. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDes innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter\n\n\nInfolettre du mois d’Avril 2023\n\n\n\n\n\n\n1 avr. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nTapis rouge et graph de l’Insee ; questionnement sur l’IA ; faillite dans la Silicon Valley\n\n\nInfolettre du mois de Mars 2023, deuxième quinzaine\n\n\n\n\n\n\n15 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web\n\n\nInfolettre du mois de Mars 2023\n\n\n\n\n\n\n1 mars 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoReMiFaSol pour récupérer des données de l’Insee ; une masterclass datascientest sur les NLP et l’analyse d’images\n\n\nInfolettre du mois de Février 2023\n\n\n\n\n\n\n30 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz\n\n\nInfolettre du mois de Janvier 2023\n\n\n\n\n\n\n10 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nL’année 2022 dans le monde de la data science : IA, transformation de RStudio, Observable\n\n\nInfolettre du mois de Décembre 2022\n\n\n\n\n\n\n31 déc. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchive des infolettres et lettres Big Data\n\n\nLes infolettres et lettres Big Data antérieures 👵👴, avant la publication sous forme de blog\n\n\n\n\n\n\n31 août 2022\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "infolettre/infolettre_09/index.html",
    "href": "infolettre/infolettre_09/index.html",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à ssphub-contact@insee.fr suffit\nLa rétrospective de l’année 2022 promettait une version plus personnalisée, inspirée des visualisations proposées par les réseaux sociaux pour synthétiser l’activité de leurs utilisateurs.\nCette newsletter un peu spéciale propose un retour sur la première année du réseau des data scientists de la statistique publique dont la préfiguration a commencé en mars 2022 et qui a été lancé officiellement en septembre. Vous pourrez retrouver à la fin de la newsletter des informations plus classiques: événements, retour sur les actions du réseau, formations, etc.\nElle permet aussi d’illustrer le potentiel d’outils qui ont été présentés dans la rétrospective de l’année 2022. Toutes les figures sont réactives, notamment quand vous passez votre souris dessus. Les principaux ingrédients qui ont été ici utilisés, et qui avaient été mentionnés dans la première partie de la rétrospective, sont Observable, Quarto et DuckDB. Les données sont stockées sur le système de stockage S3 du SSPCloud."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#lannée-du-réseau",
    "href": "infolettre/infolettre_09/index.html#lannée-du-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "L’année du réseau",
    "text": "L’année du réseau\nLe réseau comporte deux canaux de communication: une liste de diffusion mail et un canal de discussions instantanées. Intéressons nous d’abord à la liste de diffusion mail !\n\n\n\nPendant l’année 2022, 7 newsletters ont été diffusées par mail. Chacune a permis d’augmenter sensiblement le nombre de personnes dans la liste de diffusion. A la fin de l’année, il y avait 312 inscrits1 dans la liste de diffusion.\nLe réseau a organisé trois événements pendant l’année 2022. D’abord, avant l’été, deux open hours ont eu lieu. Cet événement informel prenant la forme de retour d’expérience a été l’occasion de discussions stimulantes autour de d’usage de la data science pour l’administration. En novembre, l’événement autour d’Observable animé par Nicolas Lambert a réuni près de 50 personnes."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#répartition-des-modes-daccès-au-réseau",
    "href": "infolettre/infolettre_09/index.html#répartition-des-modes-daccès-au-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Répartition des modes d’accès au réseau",
    "text": "Répartition des modes d’accès au réseau\nLe réseau propose deux canaux de diffusion de l’information: une liste de diffusion par mail et un canal de discussion instantanée qui utilise la messagerie sécurisée de l’Etat Tchap. Environ 55% des membres de la liste de diffusion (soit plus de 180 personnes) sont également inscrits sur le canal de discussion instantanée."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#composition-du-réseau",
    "href": "infolettre/infolettre_09/index.html#composition-du-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Composition du réseau",
    "text": "Composition du réseau\nLa diffusion d’informations par le réseau a permis de réunir des data scientists de 27 organisations différentes. L’Insee, qui représente 47% de l’effectif du réseau, est majoritaire. Suivent dans le palmarès, les services statistiques du Ministère de la Santé (DREES) et du Ministère du Développement Durable (SDES)."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#évolution-de-la-composition-du-réseau",
    "href": "infolettre/infolettre_09/index.html#évolution-de-la-composition-du-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Évolution de la composition du réseau",
    "text": "Évolution de la composition du réseau\nLa diffusion progressive d’informations par le biais des newsletters a permis de diversifier progressivement la composition de la liste de diffusion. Alors que la première newsletter de l’année 2022 avait été diffusée auprès de 14 institutions, ce sont des agents de 27 organisations qui ont reçues la dernière.\nLes événements organisés par le réseau ou les présentations spéciales, comme celle pour les administrateurs de l’INSEE en poste à l’ENSAE, ont également pu motiver des personnes à intégrer le réseau."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#programme-10",
    "href": "infolettre/infolettre_09/index.html#programme-10",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Programme 10%",
    "text": "Programme 10%\nLes membres du réseau des data scientists ont été particulièrement actifs dans le cadre du programme interministériel 10%, issu des recommandations d’un rapport INSEE-DINUM “Évaluation des besoins de l’État en compétences et expertises en matière de donnée”.\nLa saison 1 a donné sa chance à quatre projets, portés par différentes administrations. Si l’un d’eux existait déjà depuis plus de deux ans (projet Gouvdown), trois sont nés pour l’occasion, avec la mise en ligne de code immédiate (Cartiflette) ou postérieure au bootcamp de lancement (Socratext et matchSIRET) .\nTous les projets sont ouverts et disponible sur Github. Une statistique qui permet de représenter leur succès est le nombre de ⭐: c’est un peu un mélange entre un site en favori sous Firefox puisque cela permet de facilement retrouver un projet dans Github et le nombre de followers d’une page sur Facebook ou sur Twitter puisque cela permet de suivre l’activité d’un dépôt Github.\n\n\n\n\n\n\nNote\n\n\n\nCette visualisation fait appel à l’API Github. Si les figures ne s’affichent pas, cela peut être dû à un dépassement du nombre de requêtes par heure autorisées par l’API Github sans jeton. A l’heure actuelle, il n’existe pas encore de fonctionalité gratuite sous Observable pour stocker de manière sécurisée un jeton pour l’API Github.\n\n\nDérouler pour afficher une version non réactive\n\n\n\n\nProjet cartiflette\n\n\n\n\n\nProjet Socratext\n\n\n\n\n\nProjet Gouvdown\n\n\n\n\n\nProjet matchSIRET"
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#autres-actualités-du-réseau",
    "href": "infolettre/infolettre_09/index.html#autres-actualités-du-réseau",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Autres actualités du réseau",
    "text": "Autres actualités du réseau\n\nPrésentation de Gridviz par Julien Gaffuri\nPour rappel, le 20 Janvier 2023 de 11h à 12h30 Julien Gaffuri (Eurostat) viendra nous présenter la librairie open-source Gridviz. Réservez ce créneau pour découvrir cette librairie qui ouvre de nouvelles perspectives pour la mise à disposition de données géographiques !\nTélécharger l’invitation à l’événement sous format Outlook\n\n\n\nSource: Notebook Hello Gridviz par neocarto sur Observable\n\n\n\n\nPremière place européenne au hackathon Big Data de l’ONU\nLes résultats du hackathon big data de l’ONU, ayant eu lieu du 7 au Novembre 2022, ont été annoncés ! L’équipe Datadive - constituée de membres du réseau de l’INSEE, de la DGFIP et du CASD - est arrivée à la première place des équipes européennes 🎉.\n\n\nGit et bonnes pratiques: des formations de formateurs prévus pour les statisticiens publics\nLes nouvelles formations à Git et aux bonnes pratiques avec R, testées récemment à l’Insee et au service statistique du Ministère du Travail, la DARES, (voir newsletters de Novembre et Décembre), deviennent des formations nationales.\nPour pouvoir diffuser les bonnes pratiques favorisant le partage de codes et la qualité des projets statistiques, il est nécessaire d’avoir le plus d’enseignants possibles pour cette formation. Pour permettre cela, un appel à candidat pour une formation de formateurs a été diffusée à l’Insee et dans les services statistiques ministériels. Si vous êtes intéressés et ne l’avez pas reçu, n’hésitez pas à envoyer un mail à contact-ssphub@insee.fr.\nEn attendant, les supports de ces formations sont déjà disponibles sur inseefrlab.github.io/formation-bonnes-pratiques-git/ et sur inseefrlab.github.io/formation-bonnes-pratiques-R/. Les codes sources sont bien-sûr ouverts et disponibles sur Github, tant pour la première partie que pour la seconde. Ceux-ci sont construits collectivement, n’hésitez pas à suggérer des modifications depuis Github.\nUn site web plus complet devrait prochainement voir le jour pour accompagner cette formation. En complément de celui-ci, des éléments peuvent déjà être trouvés dans le cours de 3e année de l’ENSAE sur la mise en production de projets data science et dans la documentation collaborative utilitR."
  },
  {
    "objectID": "infolettre/infolettre_09/index.html#footnotes",
    "href": "infolettre/infolettre_09/index.html#footnotes",
    "title": "Retex sur 2022, première année du réseau des datascientists ; snapshot de l’état du réseau à date ; présentation de Gridviz",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nLes actions de communication du mois de janvier ont permis d’augmenter sensiblement le nombre de personnes dans cette liste (340 début janvier). Un retour spécial sur le mois de janvier sera l’occasion idéale pour une autre rétrospective quantitative.↩︎"
  },
  {
    "objectID": "infolettre/infolettre_11/index.html",
    "href": "infolettre/infolettre_11/index.html",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "",
    "text": "Note\n\n\n\nVous désirez intégrer la liste de diffusion ? L’inscription se fait ici.\nCe numéro reprend le format expérimenté à la fin de l’année 2022: un résumé des dernières actualités du monde de la data science précède la présentation plus classique des nouvelles du réseau."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#chatgpt-continue-de-faire-parler",
    "href": "infolettre/infolettre_11/index.html#chatgpt-continue-de-faire-parler",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "ChatGPT continue de faire parler",
    "text": "ChatGPT continue de faire parler\n\n\n\n\n\nL’actualité est encore largement dominée par les discussions autour de ChatGPT. Les débats continuent notamment sur la pertinence de ce type d’outil dans l’enseignement (cf. Télérama). L’article du New Yorker “ChatGPT is a blurry JPEG of the web” propose une analyse en profondeur de la manière dont les modèles de langage reconstruisent des réponses originales en mélangeant des corpus rencontrés lors de la phase d’entraînement.\nAlors qu’il est difficile d’avoir des informations sur le corpus de ChatGPT ou les méthodes d’apprentissage mises en oeuvre, des développeurs ont mis en place de nombreux chatbots thématiques sur https://beta.character.ai/ s’appuyant sur des modèles de langage ouverts.\nCe mois-ci, Facebook-Meta est également rentré dans la danse avec son modèle LLaMa, ouvert aux chercheurs et ayant vocation à fonctionner sur des installations moins gourmandes en ressources que les modèles GPT ou PaLM.\nA peine deux mois après le lancement tonitruant de ChatGPT, et la création d’un “code rouge” du côté de Google, le lancement difficile par Microsoft d’une version de test de son robot conversationnel dans son navigateur Bing montre que l’intégration à un moteur de recherche traditionnel n’est pas évidente, notamment pour éviter la diffusion de fausses informations. Alors que ChatGPT avait bénéficié d’un gros travail humain pour cadrer son comportement, il semblerait que l’IA de Bing soit moins consensuelle dans ses propos.\nPendant ce temps, la recherche sur les modèles de diffusion continue à avancer à grande vitesse. La dernière innovation est la capacité à reconstruire des images à partir d’IRM de l’activité du cerveau grâce au modèle Stable Diffusion."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#traitement-de-données-tabulaires-arrow-et-polars-au-centre-du-jeu",
    "href": "infolettre/infolettre_11/index.html#traitement-de-données-tabulaires-arrow-et-polars-au-centre-du-jeu",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Traitement de données tabulaires: Arrow et Polars au centre du jeu",
    "text": "Traitement de données tabulaires: Arrow et Polars au centre du jeu\n\n\n\n\n\nDu côté des données tabulaires plus traditionnelles, Apache Arrow continue de s’affirmer comme un incontournable.\nLa version 2.0 de Pandas qui vient de sortir permet de plus facilement s’appuyer, en arrière-plan, sur Arrow plutôt que Numpy qui offre des performances et des fonctionnalités limitées sur données non numériques (cf. https://datapythonista.me). Il s’agissait d’une des limites majeures de Pandas, identifiées dès 2017 par son créateur Wes McKinney (voir ici) qui est également très impliqué dans le développement d’Apache Arrow.\nLa librairie Polars connait une certaine hype et va sans doute devenir dans les mois à venir une librairie incontournable, en alternative à Pandas. La dernière version de DuckDB (sortie mi-février) renforce l’interconnexion entre ces deux écosystèmes (exemples). Si vous voulez en savoir plus sur Polars, il est recommandé de suivre l’évolution de la liste “Awesome Polars” faite par Damien Dotta (relayée par la très bonne newsletter du site Data Elixir) et de lire le post à venir prochainement sur le blog de notre réseau.\nPar ailleurs, l’article de Jordan Tigani “Big data is dead” vaut le détour."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#huggingface.js-lapi-dhuggingface-directement-accessible-depuis-un-navigateur-web",
    "href": "infolettre/infolettre_11/index.html#huggingface.js-lapi-dhuggingface-directement-accessible-depuis-un-navigateur-web",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Huggingface.js: l’API d’HuggingFace directement accessible depuis un navigateur web",
    "text": "Huggingface.js: l’API d’HuggingFace directement accessible depuis un navigateur web\n\n\n\n\n\nHuggingface et Observable sont chacun devenus des incontournables dans leur domaine (voir notre newsletter de décembre). Alors un rapprochement entre ces deux univers, permettant d’utiliser de nombreux modèles d’apprentissage via l’API d’HuggingFace directement dans le navigateur, ça donne envie de s’amuser.\nLe notebook, disponible sur Observable, illustre la richesse des fonctionnalités disponibles."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#première-journée-du-réseau-en-avril",
    "href": "infolettre/infolettre_11/index.html#première-journée-du-réseau-en-avril",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Première journée du réseau en avril",
    "text": "Première journée du réseau en avril\n\n\n\n\n\nNotre réseau organise des événements virtuels depuis un an. Pour renforcer l’esprit communautaire, nous proposons une journée du réseau le 17 avril, en présentiel 📅.\nCet événement aura lieu dans le tiers-lieu la Tréso à Malakoff. Pendant la journée se succèderont plusieurs séquences pour construire ensemble le réseau, partager autour du sujet de la data science et échanger ensemble.\nLe programme et les modalités pratiques d’inscription seront communiqués prochainement ! Vous pouvez néanmoins déjà marquer la date dans votre calendrier."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#présentation-de-la-documentation-collaborative-carpentries",
    "href": "infolettre/infolettre_11/index.html#présentation-de-la-documentation-collaborative-carpentries",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Présentation de la documentation collaborative Carpentries",
    "text": "Présentation de la documentation collaborative Carpentries\nPour favoriser l’adoption des langages R, Python et Git dans les administrations, le programme ModernStat piloté par l’OCDE et Statistics Canada, a lancé un projet nommé Meta Academy et s’est rapproché de l’organisation américaine Carpentries dont l’objectif est de proposer des parcours progressifs de formation dans les langages open source, associés à des documentations disponibles de manière ouverte.\nL’absence de contenu en Français et l’orientation principalement académique des contenus a amené le programme ModernStat à proposer aux Carpentries de créer de nouveaux parcours de formations en Français. Ces programmes seraient créés par des membres de la communauté francophone des utilisateurs des langages R, Python et Git.\nKate Burnett-Isaacs, de Statistics Canada, nous présentera l’initiative le mardi 28 mars à 15h 📅 (invitation Outlook). La présentation aura lieu en Anglais et sera suivie d’un échange (Français et Anglais possibles). Si vous êtes intéressés par la manière dont les nombreux contenus créés par les membres du réseau pourraient prendre place dans ce cadre, n’hésitez pas à venir pour en savoir plus ! Informations pratiques ici !"
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#masterclass-datascientest",
    "href": "infolettre/infolettre_11/index.html#masterclass-datascientest",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Masterclass datascientest",
    "text": "Masterclass datascientest\n\n\n\n\n\nLes inscriptions à nos cycles de masterclass datascientest sont toujours ouvertes ! Pendant le mois de mars, nous continuerons d’avancer dans les deux cursus parallèles.\nEn premier lieu, la masterclass d’introduction au deep learning (10 mars de 10h à 12h 📅) permettra d’initier notre parcours de computer vision avec la présentation de certains concepts centraux du deep learning (perceptron, convolution, transfer learning…).\nLa deuxième session mensuelle, ayant lieu le 24 mars de 10h à 12h 📅 sera elle l’occasion d’avancer dans notre parcours NLP grâce au sujet de la similarité textuelle et de la classification de textes avec des méthodes d’embeddings.\n\n\n\n\n\n\nNote\n\n\n\nPour vous inscrire, il suffit de remplir ce formulaire !"
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#onyxia",
    "href": "infolettre/infolettre_11/index.html#onyxia",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Onyxia",
    "text": "Onyxia\n\n\n\n\n\nLe dernier post sur le site web du réseau revient sur le projet Onyxia, le logiciel initié par l’équipe innovation de l’Insee et mis à disposition sur Github pour permettre à d’autres organisations de développer une infrastructure de data science à l’état de l’art.\nPour en savoir plus sur le contexte de naissance d’Onyxia, les choix techniques mis en oeuvre ou la communauté des réutilisateurs, c’est par ici."
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#replay-de-lévénement-autour-des-packages-facilitant-laccès-à-lopen-data-de-linsee",
    "href": "infolettre/infolettre_11/index.html#replay-de-lévénement-autour-des-packages-facilitant-laccès-à-lopen-data-de-linsee",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Replay de l’événement autour des packages facilitant l’accès à l’open data de l’Insee",
    "text": "Replay de l’événement autour des packages facilitant l’accès à l’open data de l’Insee\n\n\n\n\n\nLe replay des présentations des packages Doremifasol (R) et Pynsee (Python) lors de notre événement du 13 février autour des packages facilitant l’accès à l’open data de l’Insee est en ligne !\nLa vidéo et les supports présentés sont mis à disposition sur le site web du réseau"
  },
  {
    "objectID": "infolettre/infolettre_11/index.html#programme-10",
    "href": "infolettre/infolettre_11/index.html#programme-10",
    "title": "ChatGPT continue de faire parler ; Arrow et Polars pour le traitement de données tabulaires ; l’API Huggingface accessible depuis un navigateur web",
    "section": "Programme 10%",
    "text": "Programme 10%\nLa journée de lancement du programme 10% annonce une saison prometteuse ! Plus de 50 personnes, issues d’un large panel d’administrations, se sont rencontrées et ont échangées autour de projets mutualisables. Au final, une demi-douzaine de projets ont déjà été identifiés.\nSi vous n’étiez pas disponible lors de cette première journée, il est possible à tout moment de rejoindre cette communauté. Le prochain atelier a lieu le 14 mars au Lieu de la Transformation Publique 📅 !"
  },
  {
    "objectID": "infolettre/infolettre_13/index.html",
    "href": "infolettre/infolettre_13/index.html",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "",
    "text": "Note\n\n\n\nVous désirez intégrer la liste de diffusion ? L’inscription se fait ici."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#un-foisonnement-davancées",
    "href": "infolettre/infolettre_13/index.html#un-foisonnement-davancées",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Un foisonnement d’avancées…",
    "text": "Un foisonnement d’avancées…\nL’actualité des modèles d’intelligence artificielle est très chargée (résumé des annonces du mois de mars) et cette lettre se concentrera sur les actualités majeures.\nGithub Copilot, l’assistant de code de Microsoft, devrait prochainement utiliser ChatGPT. Après ChatGPT, le navigateur Bing s’enrichit également d’un créateur d’image basé sur Dall-E. Pendant ce temps, Midjourney (une des principales alternatives de création d’images) sort sa V5, améliorant encore le réalisme du rendu.\nFace à la recrudescence des montages utilisant Midjourney, qu’il s’agisse de deep fakes ou d’images à vocation humoristique, Le Monde a publié un guide pour reconnaître une image générée par une IA.\n\n\n\nExemple d’image générée par IA (source: Reddit)"
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#qui-alimentent-les-débats-autour-de-la-place-de-lia-dans-la-société-et-notre-économie-numérique",
    "href": "infolettre/infolettre_13/index.html#qui-alimentent-les-débats-autour-de-la-place-de-lia-dans-la-société-et-notre-économie-numérique",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "… qui alimentent les débats autour de la place de l’IA dans la société et notre économie numérique…",
    "text": "… qui alimentent les débats autour de la place de l’IA dans la société et notre économie numérique…\nAlors que ChatGPT est déjà utilisé par plus de 100 millions d’utilisateurs, seulement quatre mois après sa sortie, il est légitime de se poser la question, comme le fait Gaspard Koenig dans Les Echos, du rapport à la vérité des IA génératrices et des implications sociétales de la généralisation de ces assistants.\nLes prospectivistes, après s’être consacrés au bitcoin et à la blockchain, comme Jean Detrez - le héros de Jean Philippe Toussaint -, commencent à proposer des évaluations des conséquences économiques de cette (r)évolution. Dernier chiffre en date : d’après Goldman Sachs, 300 millions d’emplois au niveau mondial pourraient être supprimés ou amenés à évoluer.\nPar ailleurs, la question de l’ouverture des corpus ayant servi à entraîner ces modèles ou de la licence à réutiliser des modèles pré-entraînés est importante à plusieurs égards. D’abord, dans une perspective scientifique, il apparaît compliqué d’évaluer la qualité d’un modèle ou lui proposer des alternatives sans accès à des données scientifiques de base comme le nombre de paramètres (inconnu pour GPT-4 par exemple). Mais c’est aussi une question économique : si ces outils deviennent de plus en plus incontournables, quel sera le business model de ce secteur ? Ces services resteront-ils gratuits avec en contrepartie une réutilisation des données fournies, potentiellement opaque et difficilement contrôlable par l’utilisateur, ou seront-ils monétisés ?\nLa publication en open source de modèles de langage est donc un enjeu d’indépendance pour les organisations intéressées par l’utilisation de chatbots ou l’extraction d’information. Cette publication continue à suivre un rythme presque quotidien. Cet état de l’art des modèles publiés en open source publié en janvier est déjà largement dépassé. Et celui-ci ne sera probablement plus à jour tout aussi rapidement.\n\n\n\n\n\nDans la dernière quinzaine, l’une des principales annonces est la publication par LAION d’OpenFlamingo, une version open source de Flamingo, modèle développé par DeepMind (filiale de Google) pour décrire de manière automatique une scène présente sur une image et offrir des informations contextuelles.\nAfin de pouvoir intégrer à la fois des fonctionnalités de reconnaissance d’image et textuelle, celui-ci s’appuie sur des composantes open source pour les modèles de langage et de reconnaissance d’image et sur un jeu de données ouvertes. Des exemples de réutilisation en Python sont disponibles sur le dépôt Github.\nDans le registre IA ouverte, une équipe de chercheurs de plusieurs universités américaines a mis en oeuvre un chatbot ouvert, à partir des modèles LLaMA (Meta, voir Newsletter #11) ou Alpaca (Stanford) : Vicuna. Ce chatbot généraliste permet à un internaute de discuter sur une grande variété de sujets. En plus de s’appuyer sur des modèles ouverts, ce qui peut faciliter sa ré-utilisation dans un cadre interne, ce chat présente un avantage technique puisqu’il s’appuie sur des modèles de langage plus économes en ressources que les modèles type GPT-4.\nDans le même temps, Databricks s’est également appuyé sur LLaMA et Alpaca pour proposer un autre modèle de langage ouvert avec lequel il est possible d’échanger. Ce modèle s’appelle Dolly et est pensé comme premier clone du modèle Alpaca. Dolly peut être entrainé en 30 minutes sur un corpus massif et ne présente “que” 6 milliards de paramètres (qu’il hérite d’Alpaca) ce qui en fait, dans l’écosystème actuel des modèles de langage (LLM), un nain : à titre de comparaison GPT-3 comporte 175 milliards de paramètres et le nombre de paramètres de GPT-4 est inconnu mais pourrait être de l’ordre de la centaine de trillions.\nMozilla rejoint également le bal en investissant 30 millions de dollars pour lancer la startup Mozilla.ai. Pour Mozilla, cette startup sert à adapter la philosophie à l’origine du navigateur Firefox au développement d’intelligences artificielles : proposer des outils ouverts indépendants des principaux acteurs marchands du numérique, sur le modèle des communs plutôt que du bien privé.\nLa question de la sécurité et de la confidentialité des informations fournies à OpenAI a également été au centre de l’attention au cours de la dernière quinzaine. OpenAI a révélé une faille de sécurité à l’origine d’une fuite de données. Cette question de la confidentialité des informations fournies à ChatGPT a d’ailleurs amené la CNIL italienne à demander le blocage temporaire de l’outil d’OpenAI (voir ici).\nCette interdiction prend place dans un contexte de discussions intenses autour de la place à venir dans nos sociétés de ces robots conversationnels. Une lettre ouverte au fort écho médiatique publiée par des figures de la tech (dont Elon Musk) et des universitaires réclame un moratoire dans le développement de nouveaux modèles d’intelligence artificielle, dont les signataires soulignent à quel point il est difficile de les “comprendre, prédire ou contrôler de manière fiable”.\nCette lettre a été rapidement critiquée. En premier lieu par Andrew Ng qui souligne qu’en plus de l’impossibilité pratique de mettre en oeuvre un tel moratoire dans le cadre d’une recherche privée, ce type de sursis retarde la recherche sur des applications bénéfiques, notamment dans le domaine de la santé. Mais la critique est aussi venue de Timnit Gebru (DAIR), Emily Bender (University of Washington), Angelina McMillan-Major (University of Washington) et Margaret Mitchell (Hugging Face), autrices citées dans la lettre en référence au concept de “Stochastic Parrots” (les modèles de langage répètent des séquences de formes linguistiques comme des ensemble de mots observés dans les corpus d’apprentissage, en fonction de critères probabilistes sur la façon dont ces informations se combinent, mais sans aucune référence au sens)."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#mais-ne-nous-font-pas-oublier-certaines-autres-nouveautés-dignes-dintérêt",
    "href": "infolettre/infolettre_13/index.html#mais-ne-nous-font-pas-oublier-certaines-autres-nouveautés-dignes-dintérêt",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "… mais ne nous font pas oublier certaines autres nouveautés dignes d’intérêt",
    "text": "… mais ne nous font pas oublier certaines autres nouveautés dignes d’intérêt\nL’intensité de l’actualité autour de ChatGPT occulte beaucoup de faits qui auraient, dans un autre contexte, amené à des discussions passionnées.\nAinsi, la publication en open source de l’algorithme de recommandation de Twitter permet de mieux comprendre la manière dont fonctionne l’algorithme de recommandation de Twitter, notamment la manière dont des bulles de filtre peuvent advenir après avoir cliqué sur un post par curiosité.\nLa publication de ce code prend place quelques jours après la découverte qu’une partie importante du code de Twitter était déjà sur Github, sans doute suite à une fuite après l’un des licenciements massifs des derniers mois.\nPeut-être n’avez-vous pas remarqué mais Twitter n’a plus le même logo depuis lundi. L’oiseau bleu a été remplacé par un chien (un Shiba Inu), logo d’une cryptomonnaie, le dogecoin, dont Elon Musk avait fait une promotion controversée et dont les cours ont connu une envolée depuis.\nLe logiciel de création d’applications Docker (principal logiciel de conteneurisation) noue un partenariat avec HuggingFace, qui rassemble une immense bibliothèque de modèles, pour faciliter la réutilisation de modèles de deep learning. Avec cette approche, il est plus facile de proposer aux ré-utilisateurs de modèles des applications prêtes à l’emploi pour tester ou enrichir un modèle sur des infrastructures compatibles avec la technique de la conteneurisation, approche utilisée par les principales plateformes de data science modernes (notamment par celles s’appuyant sur le logiciel Onyxia).\nLa plateforme Observable propose un comparatif des principales syntaxes de manipulation de données (JavaScript, Python, R et SQL).\nEnfin, si vous appréciez les cartes, un hashtag à suivre est le #MapPromptMonday."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#première-journée-du-réseau-le-17-avril",
    "href": "infolettre/infolettre_13/index.html#première-journée-du-réseau-le-17-avril",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Première journée du réseau le 17 avril",
    "text": "Première journée du réseau le 17 avril\n\n\n\n\n\nNous rappelons la journée du réseau le 17 avril, en présentiel 📅 (Newsletter #12). Les places en présentiel sont presque épuisées, ne tardez pas à vous inscrire ! Si vous désirez tout de même suivre les échanges, inscrivez-vous à la retransmission par Zoom."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#les-masterclass-avec-datascientest-continuent",
    "href": "infolettre/infolettre_13/index.html#les-masterclass-avec-datascientest-continuent",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Les masterclass avec DataScientest continuent",
    "text": "Les masterclass avec DataScientest continuent\nNos cycles de masterclass datascientest continuent ! Les cycles parallèles NLP et analyse d’image continuent.\nAu programme:\n\nAnalyse d’image, niveau confirmé (📅 14 avril, 10h-12h )\nNLP, niveau avancé (📅 12 mai, 10h-12h )\nAnalyse d’image, niveau avancé (📅 9 juin, 10h-12h )\n\nInscription ici !"
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#replay-bonnes-pratiques-pour-la-mise-en-production-de-projets-data-science-30-mars",
    "href": "infolettre/infolettre_13/index.html#replay-bonnes-pratiques-pour-la-mise-en-production-de-projets-data-science-30-mars",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Replay “Bonnes pratiques pour la mise en production de projets data science” (30 mars)",
    "text": "Replay “Bonnes pratiques pour la mise en production de projets data science” (30 mars)\nLe replay de la présentation succincte du contenu du cours de l’ENSAE “Bonnes pratiques pour la mise en production de projets data science” ayant eu lieu dans le cadre du programme 10% (voir Newsletter #12) est disponible sur le site du programme 10%."
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#replay-de-lévénement-autour-de-locrisation-avec-christopher-kermorvant-29-mars",
    "href": "infolettre/infolettre_13/index.html#replay-de-lévénement-autour-de-locrisation-avec-christopher-kermorvant-29-mars",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Replay de l’événement autour de l’OCRisation avec Christopher Kermorvant (29 mars)",
    "text": "Replay de l’événement autour de l’OCRisation avec Christopher Kermorvant (29 mars)\nChristopher Kermorvant, chercheur spécialisé en OCRisation et fondateur de Teklia, a proposé au réseau une présentation très pédagogique sur l’extraction de texte avec des méthodes de deep learning.\nA partir de l’exemple de recensements de la fin du XIXe siècle, nous avons ainsi bénéficié d’une excellente introduction à l’histoire des techniques d’OCRisation et la manière dont aujourd’hui ces modèles fonctionnent en associant reconnaissance d’image et analyse textuelle.\nLe replay est ici !"
  },
  {
    "objectID": "infolettre/infolettre_13/index.html#replay-de-la-présentation-de-la-documentation-collaborative-carpentries-28-mars",
    "href": "infolettre/infolettre_13/index.html#replay-de-la-présentation-de-la-documentation-collaborative-carpentries-28-mars",
    "title": "Des innovations rapides sur l’IA qui lancent un débat sur sa place dans la société ; algorithme de recommandation de Twitter",
    "section": "Replay de la présentation de la documentation collaborative Carpentries (28 mars)",
    "text": "Replay de la présentation de la documentation collaborative Carpentries (28 mars)\nKate Burnett-Isaacs, de Statistics Canada, nous a présenté l’initiative Meta Academy / Carpentries permettant de construire une documentation francophone de référence sur R, Python et Git à destination des utilisateurs de données.\nLe replay est ici !"
  },
  {
    "objectID": "infolettre/infolettre_15/index.html",
    "href": "infolettre/infolettre_15/index.html",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "",
    "text": "C’est la rentrée ! Comme les élèves qui reviennent sur les bancs des écoles, les modèles de machine learning ont périodiquement besoin de mettre à jour leurs connaissances.\nCette newsletter sera consacrée aux enjeux du ré-entrainement et de la spécialisation de modèles, une question d’actualité suite à la publication estivale de plusieurs grands modèles de langage (LLM) open source."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#un-entraînement-ex-nihilo-hors-de-portée",
    "href": "infolettre/infolettre_15/index.html#un-entraînement-ex-nihilo-hors-de-portée",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Un entraînement ex nihilo hors de portée",
    "text": "Un entraînement ex nihilo hors de portée\nSi de nombreuses tâches de modélisation ne nécessitent pas des modèles très sophistiqués, deux domaines de recherche - à savoir le traitement naturel du langage (NLP) et l’analyse d’image - ont connu ces dernières années des innovations importantes grâce à des réseaux de neurones à l’architecture de plus en plus complexe.\nPour être en mesure d’entraîner un modèle complexe, du type grand modèle de langage (LLM), il faut, a minima, disposer des intrants suivants:\n\nUn immense volume de données déstructurées. La constitution de ces corpus implique le moissonnage en masse de ressources en ligne, ce qui n’est pas sans poser des enjeux juridiques de propriété intellectuelle qui ne sont pas encore résolus. La récupération de ces données nécessite une bonne connaissance de la structure et la nature des données nécessaires pour entrainer un modèle ;\nDes ressources informatiques hors du commun. Les investissements importants pour les cartes graphiques (GPU), une matière première en pénurie et les coûts courants associés (électricité, maintenance…) font qu’une poignée d’acteurs du numérique disposent des moyens financiers adéquats pour entraîner un modèle ex nihilo. Cet article de Forbes évoque des montants de l’ordre de plusieurs dizaines voire centaines de millions de dollars.\nDes experts aux compétences à l’intersection entre la recherche en mathématique et informatique ainsi que des spécialistes en data engineering actuellement en pénurie sur le marché du travail.\n\nLa combinaison de ces facteurs rend difficile, si ce n’est impossible, l’entrainement ex nihilo de tels modèles par la majorité des acteurs de la donnée. Seule une poignée de centres de recherche diposent des ressources permettant d’entraîner ex nihilo ce type de modèles."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#la-réutilisation-en-pratique",
    "href": "infolettre/infolettre_15/index.html#la-réutilisation-en-pratique",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "La réutilisation en pratique",
    "text": "La réutilisation en pratique\nNéanmoins, les besoins d’utilisation de ces modèles dépassent le cercle des acteurs en mesure de les entraîner. En effet, les grands modèles de langage sont généralement entraînés sur des corpus génériques de langage naturel principalement issus d’internet (Table 1). Cela les rend capables de comprendre les interactions avec des utilisateurs, notamment leurs instructions (prompt) et d’interagir avec eux de manière assez naturelle.\nNéanmoins, pour des tâches très spécialisées ou alors face à des corpus particuliers, ces modèles génériques peuvent nécessiter d’être spécialisés pour obtenir de meilleures performances. Plusieurs types de techniques, de complexité graduelle, ont ainsi émergé pour être en mesure de réutiliser et améliorer un modèle pré-entraîné.\nUn frein à la réutilisation massive de modèles pré-entraînés est la nature propriétaire de certains modèles, dont les conditions de réutilisation peuvent être limitantes. Pour cette raison, l’émergence de modèles open source, dont la structure est plus transparente et dont les conditions de réutilisation sur des infrastructures internes sont moins restrictives, est devenu ces derniers mois un enjeu important dans l’écosystème de la donnée.\nLes discussions sur l’ouverture des modèles s’inscrivent dans le contexte d’un affrontement important entre deux visions du modèle économique du secteur numérique : si le co-créateur d’OpenAI a pu affirmer, pour justifier l’absence de transparence scientifique sur les modèles d’OpenAI “[on openly sharing research,] we were wrong”, un mémo interne de Google défendait quant à lui l’idée que les modèles open source sont amenés à prendre le dessus, car ils peuvent bénéficier à plus grande échelle du travail d’experts et de retours d’utilisateurs.\nLa publication cet été de deux modèles open source (LLaMA-2 par Meta et Falcon par l’Institut de l’innovation technologique d’Abu Dhabi) ouvre de nouvelles perspectives pour une réutilisation de modèles dans une infrastructure interne, à condition de disposer des ressources computationnelles suffisantes et d’une stratégie adaptée de ré-apprentissage.\nPour aller plus loin sur ce sujet, la suite de cette newsletter évoque des détails plus techniques. La masterclass sur le sujet du fine-tuning que nous organisons avec datascientest (plus d’informations 👇️) permettra également d’approfondir cette question.\n\n\n\n\n\n\nNoteDérouler pour en savoir plus sur le corpus d’entraînement de Falcon\n\n\n\n\n\n\n\n\nTable 1: Corpus d’entraînement de Falcon 180B\n\n\n\n\n\n\n\n\n\nSource de données\nProportion dans le corpus\n\n\n\n\nRefinedWeb-English (webscraping)\n75%\n\n\nRefinedWeb-Europe (webscraping)\n7%\n\n\nLivres\n6%\n\n\nSites de conversations (Reddit, StackOverflow, HackerNews…)\n5%\n\n\nCode (Github…)\n5%\n\n\nDocuments techniques (arXiv, PubMed…)\n2%"
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#de-nouveaux-grands-modèles-de-langage-llm",
    "href": "infolettre/infolettre_15/index.html#de-nouveaux-grands-modèles-de-langage-llm",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "De nouveaux grands modèles de langage (LLM)",
    "text": "De nouveaux grands modèles de langage (LLM)\nPas de vacances pour les principaux acteurs de la data science ! Ce champ de recherche appliquée continue à connaître une actualité dense avec la publication, cet été, de deux modèles importants:\n\nLLaMA-2 par Meta, disponible en versions 7B, 13B et 70B c’est-à-dire, respectivement, 7, 13 et 70 milliards de paramètres. Dans le domaine des LLM actuels, c’est donc un modèle plutôt minimaliste (GPT-3 comportait 175 milliards de paramètres, GPT-4 en comporterait 1.7 trillions soit 1700 milliards). Le site web LeBonLLM propose, déjà, des exemples de fine tuning de LLaMa ;\nFalcon par l’Institut d’Innovation et de Technologie d’Abu Dhabi. Falcon-40B avait déjà connu, cet été, un engouement important en se plaçant en tête des réutilisations sur HuggingFace. Falcon 180B, sorti il y a quelques jours, rapproche les performances de celles des modèles propriétaires.\n\nCes deux modèles permettent d’envisager des réutilisations après un ré-entrainement sur des jeux de données ad hoc pour améliorer leurs performances (technique du fine tuning). Ceci est possible grâce à leurs licences permissives de réutilisation. Celle de Falcon est assez standard puisqu’il s’agit d’une Apache 2.0. Celle de LLaMA-2 est quant à elle moins traditionnelle. La réutilisation est libre, y compris à des fins commerciale, sauf pour les gros acteurs du numérique, globalement les concurrents de Meta :\n\n\nAdditional Commercial Terms. If, on the Llama 2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n\nLicence de LLaMa-2 sur Github"
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#le-réentrainement-des-modèles",
    "href": "infolettre/infolettre_15/index.html#le-réentrainement-des-modèles",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Le réentrainement des modèles",
    "text": "Le réentrainement des modèles\nL’ouverture de ces modèles laisse envisager des réutilisations sur de nouveaux jeux de données dans des infrastructures internes. Cet été, Andrew Ng, dans sa newsletter, est revenu sur les méthodes pour affiner les performances d’un modèle sur des données qu’il n’a pas rencontrées dans son corpus d’entraînement.\n\nLa technique la plus simple est d’affiner les instructions (prompt) fournies à un modèle. Pour faire l’analogie avec l’apprentissage humain, pour obtenir une réponse mieux ciblée à une question, il est souvent nécessaire de reformuler une question. Par exemple, lors d’une interaction avec une IA assistante de code, il peut être utile de guider un LLM avec une instruction “as a data scientist” ;\nFournir quelques exemples à un modèle (few shot learning). De même qu’avec les humains, fournir un petit nombre d’exemples peut suffire à un modèle, par un raisonnement inductif, à comprendre et répondre de manière juste à son instructeur. Selon la difficulté de la tâche à mettre en oeuvre, il peut suffire de très peu de cas pour spécialiser un modèle en modifiant les dernières couches du réseau de neurone.\nFournir de nouvelles sources à un modèle avant de l’interroger (retrieval augmented generation). Cette technique consiste à enrichir la base de connaissance d’un modèle pré-entraîné avec un nouveau corpus puis l’interroger sur la même thématique afin, par exemple, d’obtenir une synthèse ou alors une information contenue dans les documents mais nécessitant un temps d’extraction non négligeable à un humain. Dans cette approche, il ne s’agit pas de réentrainer le modèle pour mettre à jour ses paramètres mais de lui donner plus de contexte pour améliorer la pertinence des prédictions ou réponses du modèle. Pour continuer l’analogie avec l’apprentissage humain, cette technique se rapproche d’une situation où un humain assimile une bibliographie pour rentrer dans un nouveau sujet.\nRéentrainement par spécialisation (fine tuning) pour affiner le modèle sur une tâche donnée. Il s’agit d’une approche qui peut ressembler à l’apprentissage d’une nouvelle langue pour un humain: à partir d’un certain stock de connaissances antérieures (une langue natale), on accumule en série des exemples bien choisis pour améliorer la compréhension d’une autre langue. Cette approche permet une économie de ressources puisqu’elle consiste à spécialiser un modèle généraliste mais nécessite que la nature du problème pour lequel est ré-entrainé un modèle ressemble à celle pour lequel le modèle a été entraîné. De même qu’essayer de transposer des règles d’une langue latine aidera peu à apprendre le japonais, spécialiser un modèle d’analyse d’image pour une tâche de classification de données textuelles sera inefficace.\n\nLe fine tuning est ainsi une solution intéressante à condition d’avoir testé si des approches plus simples n’apportent pas déjà des solutions satisfaisantes.\nDe plus, pour être en mesure de fine tuner un modèle, outre l’accès à des ressources computationnelles conséquentes (mais tout de même moindres qu’un entraînement ex nihilo), beaucoup de méthodes nécessitent de disposer de données labellisées, c’est-à-dire impliquent de posséder une base de données permettant de juger de la qualité des prédictions du modèle. Pour pallier cette absence, il est possible de mettre en oeuvre un processus humain d’annotation et fournir au modèle ces évaluations pour l’amener à s’améliorer (technique nommée reinforcement learning from human feedback).\nComme l’évoque encore Andrew Ng, l’accès à des modèles pré-entraînés change le cycle de développement des projets utilisateurs d’IA. Ces projets ne consistent plus, comme les logiciels classiques, à développer en amont des spécifications puis déployer un modèle correspondant à celles-ci mais, au contraire, à commencer par mettre en oeuvre rapidement un premier modèle dont le comportement sera évalué et amélioré en continu par des méthodes comme l’apprentissage par renforcement."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#mc-datascientest",
    "href": "infolettre/infolettre_15/index.html#mc-datascientest",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Masterclass datascientest sur le fine-tuning",
    "text": "Masterclass datascientest sur le fine-tuning\n\n\n\n\n\nNotre cycle de masterclass organisées en lien avec datascientest continue !\nAprès avoir exploré en détail les thématiques du traitement automatique du langage et de l’analyse d’image, nous progressons dans notre parcours avec le sujet du fine tuning.\nAu programme:\n\nRéutilisation de transformers (BERT) et de LLM (LLaMA) avec les librairies d’HuggingFace ;\nFine tuning de ces modèles.\n\nRendez-vous le 6 octobre de 10h à 12h ! Inscription ici"
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#autres-événements",
    "href": "infolettre/infolettre_15/index.html#autres-événements",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Autres événements",
    "text": "Autres événements\nQuelques événements ou informations intéressantes :\n\nHackathon velib, fermeture des inscriptions le 29 septembre ;\nHackathon de l’ONU: fermeture des inscriptions à la fin du mois ;\nPrix du jeune statisticien de l’IAOS : article à envoyer avant le 10 février 2024.\n\nLes personnes intéressées par former une équipe pour les hackathons peuvent contacter contact-ssphub@insee.fr."
  },
  {
    "objectID": "infolettre/infolettre_15/index.html#rejoindre-le-salon-tchap-ssp-hub",
    "href": "infolettre/infolettre_15/index.html#rejoindre-le-salon-tchap-ssp-hub",
    "title": "Coûts d’entrée trop élevés pour l’entraînement des modèles de langage qui s’orientent vers l’opensource ; LlaMaA et Falcon les nouveaux LLM",
    "section": "Rejoindre le salon Tchap SSP Hub",
    "text": "Rejoindre le salon Tchap SSP Hub\nPour échanger autour des activités du réseau et, plus largement, discuter entre pairs des sujets data science, il existe un salon SSP Hub dans la messagerie sécurisée de l’État Tchap. Celui-ci réunit plus de 250 personnes et permet des échanges plus directs, plus fréquents et plus informels que la liste de diffusion mail.\nSi vous avez un compte sur Tchap, vous pouvez rejoindre ce salon en cherchant celui-ci par son nom « SSP Hub ». En cas de problème pour le rejoindre, n’hésitez pas à envoyer un mail à contact-ssphub@insee.fr."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html",
    "href": "infolettre/infolettre_17/index.html",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "",
    "text": "En ce début d’année 2024, nous prenons un peu de recul sur l’année écoulée et vous présentons les avancées marquantes de l’année 2023 dans le domaine de la data science. Ces avancées concernent l’intelligence artificielle générative, au cœur des débats médiatisés, mais aussi plusieurs développements technologiques très utiles pour l’analyse et la diffusion de bases de données. Des éléments plus techniques, qui ne sont pas nécessaires à la compréhension globale, sont présents dans des encadrés déroulables ou sont référencés dans les sections “Pour en savoir plus”.\nLa fin de la newsletter est consacrée à quelques annonces sur les prochains événements communautaires du réseau."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html#panorama-des-avancées-ayant-eu-lieu-en-2023-dans-le-domaine-de-la-data-science",
    "href": "infolettre/infolettre_17/index.html#panorama-des-avancées-ayant-eu-lieu-en-2023-dans-le-domaine-de-la-data-science",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "Panorama des avancées ayant eu lieu en 2023 dans le domaine de la data science",
    "text": "Panorama des avancées ayant eu lieu en 2023 dans le domaine de la data science\n\nLes IA génératives toujours au coeur des débats\nDans la continuité de la sortie de ChatGPT en décembre 2022, les IA génératives ont continué en 2023 à focaliser une part importante de l’attention portée à la data science. Outre la publication de GPT-4 en mars (modèle embarqué dans la version Pro de ChatGPT), de nombreux grands modèles de langage (LLM) généralistes ont été publiés cette année : Llama-2 (Meta), Mixtral 7B (Mistral), Falcon 180B (Technology Innovation Institute), PaLM 2 (Google)…\nCes publications ont mis en avant le caractère stratégique de la mise à disposition de modèles open source. La récupération et la structuration de corpus massifs, l’entraînement de modèles intégrant des milliards de paramètres et l’évaluation ex post de ceux-ci est à la portée d’un nombre restreint d’acteurs. La publication en open source de modèles et de codes sources est dès lors indispensable pour, entre autres, être en mesure d’évaluer la pertinence scientifique des modèles ou permettre aux acteurs n’ayant pas ces moyens techniques et humains de pouvoir tout de même réutiliser ces modèles sur leurs propres données.\nNéanmoins, malgré l’ouverture progressive de modèles, notamment par le biais d’une mise à disposition sur la plateforme Hugging Face, des contraintes limitent encore la réutilisation de ces modèles dans des infrastructures internes. Ces réseaux de neurones étant très gourmands en calculs du fait de leur architecture complexe (des milliards de paramètres pour les grands modèles de langage), afin d’obtenir une réponse du modèle il est généralement nécessaire d’effectuer les calculs par le biais de cartes graphiques (GPU), celles-ci permettant plus de parallélisme dans les calculs que les processeurs (CPU). Cependant, comme les GPU sont plus consommatrices d’énergie et plus coûteuses à l’achat du fait notamment d’une demande excédant l’offre, l’accès à cette ressource est limité pour de nombreux acteurs. Le retour à des modèles plus légers, pouvant être exploités depuis des architectures informatiques plus accessibles, constitue l’un des défis de l’année 2024.\nLes débats concernant les droits d’exploitation commerciale d’informations collectées sur internet ont été nombreux en 2023. Après les plaintes médiatisées de Getty Images (envers Stability AI), d’un collectif d’auteurs célèbres (envers OpenAI), la grève des acteurs à Hollywood contre l’exploitation de leur image par des IA et des scénaristes contre l’utilisation des générateurs de texte, c’est maintenant le New York Times qui a déposé en décembre 2023 une plainte envers OpenAI auprès de la Cour Fédérale de Manhattan. A partir d’exemples, le journal américain met en avant le degré de confiance élevé que ChatGPT attribue aux informations issues des articles du quotidien, sans pour autant en citer la provenance, ni compenser financièrement le journal. Cela entraînerait un préjudice commercial dû à la réduction du trafic sur le site du New York Times. A contrario, le journal met en avant l’effet négatif sur son image que peuvent avoir des hallucinations attribuées au quotidien. Cette plainte fait suite à l’échec des négociations entre les deux acteurs au cours de l’année 2023. Ces actualités relatives aux droits d’auteurs interviennent dans un contexte où il s’agit de l’un des principaux axes d’intervention de l’“Artificial Intelligence Act” européen (voir Infolettre #16).\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nUn article du Washington Post sur le corpus d’entraînement des LLM ;\nUn article du Financial Times qui présente de manière très pédagogique la manière dont fonctionnent les LLM ;\nUn tutoriel sur les LLM par Hugging Face ;\nUn article du New York Times sur la pénurie de GPU ;\nUn article du site spécialisé The Verge sur les dernières évolutions de la plainte de Getty Images ;\nUn article de Courrier International sur la plainte d’un collectif d’auteurs envers OpenAI ;\nNew York Times vs OpenAI par le Monde et le New York Times ;\nLes chartes relatives au contenu produit par des IA génératives du gouvernement britannique et d’une vingtaine de médias français recensés par l’INA.\n\n\n\n\n\nDes avancées scientifiques en arrière plan\nLa tendance des LLM à l’hallucination, c’est-à-dire à la production de contenu plausible par sa forme mais factuellement faux, a été l’objet d’inquiétudes quant à la véracité des informations pouvant être mises en avant par les IA amenées à occuper une place croissante dans la diffusion de contenu. Pour faire face à ce défi, au cours de l’année 2023, les équipes de conception des LLM ont utilisé de manière croissante la technique du RAG (Retrieval Augmentated Generation). Celle-ci consiste, pour les modélisateurs, à cadrer le comportement du LLM en faisant en sorte qu’il privilégie des informations issues d’un corpus adapté spécialement à une tâche. Pour faire l’analogie avec l’apprentissage humain, les modèles où le RAG intervient peuvent être comparés à des étudiants préparant une dissertation s’appuyant sur un dossier préparé par les évaluateurs. Ce contexte pourra être utilisé pour construire une réponse argumentée et des exemples plus pertinents.\nLe succès des IA de discussion comme ChatGPT est intrinsèquement lié au travail humain qui a été mis en œuvre pour évaluer la pertinence des réponses proposées par le modèle afin de régulièrement mettre à jour le comportement du modèle. Sur une question donnée, l’humain évalue la réponse la plus pertinente faite par l’IA. A partir d’un volume suffisant d’évaluations, l’algorithme pourra, dans les prochaines situations similaires, faire un choix plus pertinent s’il apprend de ses erreurs. La technique ayant gagné cette année en popularité pour cette tâche est la DPO (Direct Preference Optimization) qui vise à simplifier l’intégration des retours humains dans le cycle de réentraînement d’un modèle. Cette problématique de supervision et d’amélioration continue d’un modèle dépasse d’ailleurs le cadre des modèles de langage : afin de s’assurer que les algorithmes ne perdent pas en qualité, l’évaluation humaine par le biais, par exemple, de campagnes de labellisation ou de retours des utilisateurs, est un enjeu important dans le cycle de vie de tout modèle mis en production.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLa vidéo “State of GPT” par Andrew Karpathy ;\nUn tutoriel sur le RAG par Hugging Face ;\nLe blog présentant la technique du Reinforcement learning with human feedbacks par OpenAI ;\nL’article académique présentant la DPO et un tutoriel d’Hugging Face ;\nUn article de blog d’Andrew Ng sur la DPO.\n\n\n\n\n\nLes bases de données vectorielles gagnent en popularité\nLe langage de programmation Python est le point d’entrée de référence dans le domaine de la data science, mais pour des besoins plus spécialisés, des logiciels dédiés viennent s’y intégrer.\nC’est le cas notamment des bases de données vectorielles comme ChromaDB. Ces bases facilitent la recherche de similarité entre documents textuels en exploitant des transformations de ceux-ci en vecteurs numériques (technique des embeddings).\nPar exemple, dans l’image ci-dessous, une base de données vectorielle pourra évaluer la similarité entre les termes en utilisant des techniques d’algèbre linéaire de manière plus efficace que ne le permettrait Python. Ce dernier est en effet un langage informatique généraliste, moins performant que des logiciels spécialisés pour faire de la recherche de similarité dans des corpus massifs. Dans une chaîne de production exploitant ce type de technique, Python servira de point d’entrée et déléguera ensuite les calculs complexes à la base de données vectorielle.\n\n\n\nUn exemple d’embedding et de rapprochement de textes. Chaque bande représente une dimension latente de notre langage. Il est possible de rapprocher les termes à partir de celles-ci. Source: Financial Times.\n\n\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLe framework LangChain pour construire par le biais de Python des applications utilisant des LLM : création d’une interface pour poser des questions à un LLM, transformation de la question en vecteur numérique par le biais d’une base vectorielle comme ChromaDB, interrogation du LLM, renvoi à l’utilisateur d’une réponse… ;\nUn tutoriel de realpython.com sur ChromaDB."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html#diffuser-des-données-au-format-parquet-pour-simplifier-leur-usage",
    "href": "infolettre/infolettre_17/index.html#diffuser-des-données-au-format-parquet-pour-simplifier-leur-usage",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "Diffuser des données au format Parquet pour simplifier leur usage",
    "text": "Diffuser des données au format Parquet pour simplifier leur usage\nDans le domaine de la diffusion des données open data, l’Insee a expérimenté le format Parquet à deux reprises pendant l’année 2023. En premier lieu, pour la diffusion des données du Répertoire Electoral Unique. Plus récemment, ce sont les données détaillées du recensement de la population qui ont été diffusées dans ce format, accompagnées d’un guide d’utilisation mis en ligne sur le blog du SSP Hub (plus de détails dans l’infolettre #16).\n\n\nQuelques exemples de retours sur la publication des données détaillées du recensement au format Parquet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe format de données Parquet est très intéressant pour les data scientists intéressés par le traitement de données volumineuses. Il permet des gains de performance importants par rapport au CSV sans être dépendant d’un logiciel propriétaire (contrairement aux formats .sas7bdat, dbase, etc.). Par exemple, les données détaillées du recensement diffusées par l’Insee pèsent 450Mo au format Parquet contre 5Go au format CSV. Des outils de traitement optimisés existent pour faciliter l’utilisation de ce format. Parmi ceux-ci, cette année a été marquée par la montée en puissance de DuckDB. Il s’agit d’un logiciel qui est utilisable par le biais des principaux langages maîtrisés par les data scientists : Python, R, JavaScript ou directement en ligne de commande.\nSa capacité à gérer de grandes quantités de données en faisant des requêtes SQL optimisées sur un fichier au format Parquet rend DuckDB particulièrement approprié pour le traitement de données volumineuses (voir les éléments plus techniques, ci-dessous). Avec ce logiciel, les données du recensement peuvent être lues en quelques secondes alors qu’il fallait plusieurs dizaines de minutes dans les précédents formats. Pour des utilisateurs de l’écosystème de l’open data ou pour des organisations dont le patrimoine de données prend plus la forme de fichiers que de bases de données PostGreSQL, DuckDB est une opportunité technologique permettant de valoriser des données dont le traitement et la diffusion nécessitait jusqu’à présent des ressources computationnelles importantes.\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\nRessources techniques:\n\nUn notebook  sur DuckDB issu d’une formation de l’Insee donnée à la BCEAO ;\nUne fiche sur Arrow, l’écosystème sous-jacent à DuckDB, dans la documentation collaborative utilitR ;\nLe post de blog sur la librairie  Polars, une approche alternative à DuckDB ;\nL’explorateur de données du SSPCloud qui repose sur DuckDB.\n\nDonnées diffusées par la statistique publique au format Parquet :\n\nLes données du Répertoire Electoral Unique ;\nLe guide d’utilisation des données du recensement de la population au format Parquet sous forme de billet de blog. Voir aussi l’infolettre #16 ;\nLes données de la délinquance enregistrée par la police et gendarmerie nationales publiées par le service statistique ministériel de la sécurité intérieur publiées au format Parquet.\n\nSur le format Parquet :\n\nUn article sur le format Parquet dans le Courrier des stats n°9 écrit par Alexis Dondon et Pierre Lamarche ;\nLe blog d’Eric Mauvière qui présente une série d’articles sur le format Parquet ;\nLa présentation de Romain Lesur sur le sujet pour l’atelier Modernisation of Official Statistics de l’UNECE.\n\n\n\n\n\n\n\n\n\nAstuceDes éléments plus techniques sur la gestion de la volumétrie des données\n\n\n\n\n\nIl existe principalement deux approches pour stocker, organiser et mettre à disposition des jeux de données structurés sous forme tabulaire : les fichiers et les bases de données relationnelles.\nLes bases de données relèvent d’une approche systémique. Un système de gestion de base de données (SGBD) est un logiciel qui gère à la fois le stockage d’un ensemble de données reliées, permet de mettre à jour celles-ci (ajout ou suppression d’informations, modification des caractéristiques d’une table…) et qui gère également les modalités d’accès à la donnée (type de requête, utilisateurs ayant les droits en lecture ou en écriture…). L’un des logiciels les plus connus dans le domaine est PostgreSQL.\nD’un autre côté, le stockage de données tabulaires sous forme de fichiers offre une approche plus décentralisée et flexible. Par rapport à des bases de données, les fichiers sont plus faciles à créer, partager et stocker et ne nécessitent pas systématiquement des logiciels spécialisés pour leur manipulation. Le stockage sous la forme de fichier consiste à organiser l’information présente dans un jeu de données dans des fichiers, de manière brute. Ces données peuvent être analysées sans recourir à un logiciel spécialisé. Même dans le cadre de formats propriétaires, comme le .xlsx ou .sas7bdat, le fait d’avoir une certaine forme de standardisation rend possible, même si ce n’est jamais parfaitement fiable, de lire ces données avec un autre logiciel que celui prévu initialement.\nLa logique de la base de données est donc très différente de celle du fichier. Par rapport à une base de données, l’approche des fichiers présente plusieurs avantages, à condition de privilégier des formats libres.\nEn premier lieu, les fichiers sont moins adhérents à un logiciel gestionnaire. Une transition d’un logiciel de traitement vers un autre n’implique pas de changer la source brute. En outre, alors que le traitement des bases de données nécessite l’intermédiation du logiciel de gestion adapté, les utilisateurs de Python ou R peuvent utiliser des fichiers à partir d’une librairie, donc un système beaucoup plus léger, qui sait comment transformer la donnée pour la retravailler depuis Python ou R.\nPour ces raisons, entre autres, il est plus pratique pour des utilisateurs finaux de données d’avoir accès à des fichiers plutôt qu’à des bases de données, à condition d’avoir les ressources computationnelles suffisantes pour pouvoir traiter ces fichiers.\nNéanmoins, cette condition d’accès à des ressources computationnelles suffisantes peut représenter une contrainte limitante dans un environnement où les données sont de volume croissant. Dans les environnements où la volumétrie des données était importante, les bases de données ont connu une certaine popularité puisqu’elles permettaient de gérer efficacement de grandes quantités de données. Comme, de plus, les bases de données offraient une gestion plus fine et fiable des droits d’accès et d’écriture sur les bases que ne le permettaient des fichiers, cette approche a pu connaître une certaine popularité.\nLe développement conjoint de formats de stockages orientés objets (comme le protocole S3, utilisé par les systèmes cloud modernes à l’image du SSPCloud) et d’outils de traitement efficaces comme DuckDB permet d’associer les avantages de ce dernier à ceux d’un système cohérent de fichiers partagés (lecture/écriture optimisées, dissociation des utilisateurs pouvant lire et écrire un fichier…).\nTechniquement, DuckDB fonctionne de manière optimale avec des fichiers au format Parquet. Ce format de données, orienté colonne, permet en effet d’optimiser des traitements classiques des data scientists : sélectionner seulement certaines colonnes d’un jeu de données, regrouper des données pour faire des calculs d’agrégats, etc.\n\n\n\nUne illustration du principe du stockage orienté colonne (Source: Michael Berk)\n\n\nPar exemple, dans le schéma ci-dessus, si on ne s’intéresse qu’aux dates enregistrées, il suffit de ne prendre que le bloc de données ad hoc. Il n’est pas nécessaire de lire tout le fichier pour ne garder que les dates, comme ce serait le cas avec un format CSV."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html#de-la-data-science-depuis-un-navigateur",
    "href": "infolettre/infolettre_17/index.html#de-la-data-science-depuis-un-navigateur",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "De la data science depuis un navigateur",
    "text": "De la data science depuis un navigateur\nLe gain de popularité de DuckDB au cours de l’année 2023 s’explique en partie grâce à sa version web qui permet d’exécuter des traitements de données par le biais de navigateurs web, sans avoir à installer de logiciel spécialisé comme Python  ou . C’est une approche typique du web assembly qui consiste à mettre à disposition des logiciels de calculs scientifiques par le biais d’un simple navigateur grâce à Javascript , qui est disponible sur tout navigateur. Cette approche est intéressante pour les institutions proposant des data visualisations car elle peut permettre de mettre en oeuvre des manipulations de données complexes directement depuis la source brute par le biais du navigateur, donc sans recourir à des serveurs externes hébergés dans des cloud.\n\n\n\n\n\n\nNoteFaire du  depuis le navigateur avec webR\n\n\n\n\n\nIl est maintenant possible de faire du R directement depuis un navigateur web grâce à webR, une librairie développée par Posit en 2023 et qui porte la grammaire R dans le navigateur sans recourir à Shiny (exemple à tester 👇️).\nL’idée est que le code d’analyse de données est en R mais qu’en arrière plan c’est du Javascript qui servira à l’exécution. La librairie est encore jeune mais celle-ci est prometteuse pour faciliter la transition entre du code d’analyse de données en R et une application interactive.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUn exemple plus complexe, utilisant le package ggplot2:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNotePour en savoir plus\n\n\n\n\nLa documentation officielle de WebR ;\nUn post sur le web assembly par ThinkR ;\nL’extension quarto-webR qui permet d’encapsuler du code webR dans un site web statique construit avec Quarto ;\nUne démonstration de WebR sur Observable."
  },
  {
    "objectID": "infolettre/infolettre_17/index.html#prochains-événements-du-réseau",
    "href": "infolettre/infolettre_17/index.html#prochains-événements-du-réseau",
    "title": "Le RAG pour limiter l’hallucination par l’IA ; l’avancée des bases de données vectorielles ; le format Parquet pour simplifier leur usage ; DuckDB débarque en version web",
    "section": "Prochains événements du réseau",
    "text": "Prochains événements du réseau\n\n“La dataviz pour donner du sens aux données et communiquer un message” par Eric Mauvière (📅 29 février, 15h-16h)\nLe 29 février (15h - 16h), Eric Mauvière nous fera une présentation, avec de nombreux exemples issus de la statistique publique, de la manière dont une visualisation de données peut être construite pour transmettre un message clair aux lecteurs. Cette présentation permettra d’évoquer les bonnes pratiques et les outils simples pour construire des visualisations de données faciles à lire et à comprendre afin de rendre le message intelligible, efficace et utile à un large spectre de publics.\nCet événement aura lieu en visio sur Zoom, il est ouvert à tous les membres du réseau. Pour les agents de la DG de l’Insee, une retransmission en salle 4-C-458 est organisée pour assister à la présentation puis échanger à l’issue de celle-ci.\nÉric Mauvière est statisticien, passé par la diffusion et les études régionales de l’Insee. Il a ensuite créé le logiciel cartographique web Géoclip utilisé, par exemple, par le site https://statistiques-locales.insee.fr/. Plus récemment, il a participé à la conception du site https://vizagreste.agriculture.gouv.fr/, portail de visualisation de données du service statistique ministériel du ministère de l’Agriculture et de la Souveraineté alimentaire. Depuis 3 ans, au sein d’Icem7, il participe à la diffusion de connaissances sur les problématiques de dataviz et forme sur mesure en sémiologie graphique et analyse de données.\n\nInvitation outlook\nLien Zoom\n\n\n\nMasterclass datascientest\nDe nouvelles masterclass en partenariat avec datascientest seront organisées prochainement. Un questionnaire pour recenser les besoins, similaire à celui proposé l’an dernier, sera transmis prochainement pour évaluer le contenu à prioriser.\n\n\nProgramme 10%\nLa saison 3 du programme 10% démarre prochainement. Ce programme, issu des recommandations du rapport de l’Inspection Générale de l’Insee et de la DINUM, permet à des experts de la donnée de l’administration de monter en compétence en consacrant jusqu’à 10 % de leur temps de travail à des projets transversaux, à des formations ou encore à des moments informels qui favorisent l’échange entre pairs.\nLe premier événement de la saison aura lieu le 11 mars 2024 au Lieu de la Transformation Publique (Paris XV). L’objectif de cette journée est de définir collectivement les projets mutualisables pouvant entraîner des collaborations entre data scientists de différentes administrations pendant l’année 2024.\nPlus d’informations à venir sur le site du programme.\n\nInscription sur Eventbrite.\n\n\n\nAutoformation de découverte à Python\nDepuis quelques semaines, une nouvelle formation est apparue au catalogue de formation de l’Insee et sur le portail de formation du SSPCloud : une auto-formation de découverte de Python, construite et mentorée par les équipes innovation de l’Insee.\nChaque chapitre de formation, disponible sous la forme de notebooks Jupyter, peut être ouvert en un clic à partir du catalogue de formation du SSPCloud. Ces ressources sont disponibles en continu, au-delà des périodes délimitées pour la formation.\nDes cycles de 6 semaines pendant lesquels les participants peuvent bénéficier d’un mentorat ont vocation à être organisés régulièrement. Les premiers formés selon cette modalité ont commencé leur apprentissage en ce début d’année 2024.\nCe système de mentorat prend deux formes :\n\ntout au long de la formation : vous pouvez poser toutes vos questions sur le canal Tchap dédié ; les mentors de la formation répondent rapidement, afin que vous ne restiez jamais bloqué ;\nponctuellement : une visio est réalisée si vous le désirez avec 1-2 mentors afin de pouvoir discuter plus en détail des problèmes que vous pouvez rencontrer.\n\nCe système de mentorat a l’avantage à la fois de favoriser une pédagogie par la pratique continue qui a fait ses preuves pour l’apprentissage des langages de programmation, tout en laissant à chacun la possibilité d’avancer à son rythme.\nLes dates des prochains cycles de mentorat seront prochainement communiquées. Il est néanmoins possible, en attendant, d’explorer les ressources disponibles sur le SSPCloud ainsi que rejoindre le canal Tchap SSPy - Formation \"Initiation à Python\" afin d’échanger sur celle-ci (poser des questions de compréhension, comprendre un bug, assister les collègues bloqués, etc.)."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html",
    "href": "infolettre/infolettre_19/index.html",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "",
    "text": "À l’occasion de cette dix-neuvième infolettre, on a discuté, réfléchi et pensé au sens de la vie. On a décidé de revoir le schéma de l’infolettre selon plusieurs critères :\n\nune périodicité mensuelle “garantie”, quitte à avoir des infolettres moins denses que d’autres ;\nsimplifier la rédaction et le contenu ;\ncontinuer de parler à toutes les personnes intéressées par la science des données, quel que soit son niveau d’expertise ;\ncontribuer à mettre en avant plus facilement des personnes ou des projets au sein du réseau.\n\nNouvelle structure, moins longue, plus ramassée, vous aurez plus de travail pour creuser plus loin les sujets ! L’idée est ainsi d’insérer au moins dans chaque veille :\n\nune datavisualisation\nles actualités du réseau et une veille\nune interview de quelqu’un ou d’une équipe selon un canevas à peu près stable.\n\nBonne lecture 📔 !"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#la-troisième-journée-du-réseau-1-décembre---la-tréso-malakoff",
    "href": "infolettre/infolettre_19/index.html#la-troisième-journée-du-réseau-1-décembre---la-tréso-malakoff",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "La troisième journée du réseau 📅 1 décembre - La Tréso (Malakoff)",
    "text": "La troisième journée du réseau 📅 1 décembre - La Tréso (Malakoff)\nRéservez votre 1er décembre ! Pour la troisième année consécutive, le SSPLab organise la journée du réseau pour rassembler les data-scientists de la statistique publique. Au menu : présentation de projets innovants, retour d’expérience et moments d’échanges informels (autrement appelés “pots” 🎉).\nComme les années précédentes, l’événement sera en présentiel et à distance pour permettre à tous de participer. Les détails seront publiés sur le site du réseau et si jamais vous voulez déjà vous inscrire alors que l’agenda n’est pas finalisé, c’est possible ici.\n\n👉️ Ajouter cet événement à votre agenda Outlook"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#le-site-du-réseau-évolue",
    "href": "infolettre/infolettre_19/index.html#le-site-du-réseau-évolue",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Le site du réseau évolue",
    "text": "Le site du réseau évolue\nL’inscription à la liste de diffusion a été revue et utilise maintenant Grist. Pour s’inscrire à la liste de diffusion, c’est par ici. Une fois inscrit, vous pouvez créer un compte sur Grist et vous connecter directement sur l’annuaire pour mettre à jour vos données, demander votre désinscription en cochant la case “Supprimez mon compte”.\nPar ailleurs, le site du réseau devrait évoluer dans les prochaines semaines. Il va s’étoffer pour présenter plus de projets en cours et permettre ainsi à tout un chacun de savoir qu’un projet existe et pouvoir échanger entre pairs. Si vous souhaitez valoriser un projet, n’hésitez pas à nous le faire savoir !"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#ia",
    "href": "infolettre/infolettre_19/index.html#ia",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "IA",
    "text": "IA\nComme toujours, une flopée d’articles a été publiée sur l’IA : le nouveau modèle d’OpenAI (GPT-5) a été déployé cet été, l’usage de l’IA se développe, des craintes se font entendre sur l’existence d’une bulle financière et, avec l’augmentation de son utilisation, de plus en plus de failles de sécurité liées sont découvertes. Un petit florilège rapide, non exhaustif :\n\nEn France, l’IA est de plus en plus utilisée par les entreprises d’après une étude de l’Insee. En 2024 ainsi, une entreprise sur dix utilise l’IA, et ce phénomène concerne particulièrement 33% des grandes entreprises et 42% de celles de l’information. L’usage de l’IA augmente de 4 points par rapport à 2023. L’IA est par ailleurs légèrement moins utilisée par les entreprises en France que dans l’Europe, où 13% des entreprises disent utiliser l’IA en 2024.\nSur l’impact de l’IA, notamment sur le travail et la productivité, de nombreuses études continuent d’être publiées. Petit disclaimer, la technologie évolue encore très vite : depuis son arrivée il y a moins de trois ans, les bugs relevés au début ne sont plus du tout d’actualité aujourd’hui : les images sont de bien meilleure qualité, des RAG ont été mis en place … Face à un domaine aussi changeant, les résultats des études varient donc encore beaucoup.\n\nCeci étant dit, les études montrent globalement que l’IA permettrait d’améliorer l’efficacité des travailleurs, particulièrement des non-experts, et réduit les inégalités de performance, même si les résultats sont contrastés. Selon cette étude, l’IA commence à avoir un impact négatif sur l’emploi, quand celle-ci estime à l’inverse que les gains de productivité pour les développeurs sont sur-estimés.\nL’usage de l’IA serait particulièrement efficace pour effectuer des tâches moyennement rares, l’humain restant plus efficace sur les tâches courantes (cf. par exemple ce papier). Par ailleurs, sur les tâches complexes ou rares, l’IA serait largement moins efficace que l’humain et produirait des résultats de qualité moindre (cf. ce papier).\n\nConcernant la technologie en soit, des chercheurs ont réussi, à partir d’un petit modèle d’IA générative, à classifier du texte aussi efficacement qu’avec un gros modèle et nécessitant bien moins de données. Pour ce faire, ils ont utilisé un modèle de régression pénalisée (type Lasso/Ridge) sur la représentation numérique sous-jacente du texte. Plus de détails dans leur article.\nDe nombreux articles font craindre l’existence d’une bulle financière autour de l’IA.\n\nEdward Zitron, un publiciste britannique, auteur et podcasteur, rappelle sur son blog toutes les raisons pour laquelle une bulle existerait actuellement sur l’IA. Il rappelle notamment que les 560Md$ investis par les GAFAM dans l’IA n’ont généré que très peu de bénéfices et que le seul gagnant est Nvidia. Comme le dit le proverbe, “Pendant la ruée vers l’or, ce ne sont pas les chercheurs d’or qui se sont le plus enrichis, mais les vendeurs de pelles et de pioches”.\nL’adoption de l’IA par les entreprises prendrait par ailleurs plus de temps qu’anticipé et n’aurait pas des rendements aussi rapides qu’espéré.\nPlus généralement, des articles, comme cet article de Forbes, rappellent que l’IA reste très utilisée aujourd’hui et que, même si aujourd’hui des investissements sont fait vers des projets peu productifs, l’adoption de nouvelles technologies prend du temps. Les articles citent beaucoup l’exemple d’internet, et de la bulle du début des années 2000 : les attentes du marché étaient trop hautes par rapport à tout le travail qu’il restait à faire, et cela n’empêche pas que aujourd’hui, 25 ans après cette bulle, les investissements dans le réseau internet ont permis de changer la société.\n\nEnfin, avec l’augmentation de son utilisation, la sécurité de la technologie est un enjeu qui est de plus en plus discuté, au-delà du détournement à des fins illégales qui attend toute innovation numérique :\n\nDes données confidentielles de Microsoft ont fuité après le piratage d’agents Copilot. Les hackeurs ont ainsi reçu par mail des extraits des contacts et des ventes de Microsoft.\nSelon le rapport d’Anthropic sur les menaces liées à l’IA, cette technologie a notamment été détournée pour :\n\ns’assurer des postes bien payés pour des Nord-Coréens, qui leur ont permis de rapatrier les capitaux au pays ;\nmassifier les fraudes aux données personnelles ;\nautomatiser les attaques par ransomware."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#parquet",
    "href": "infolettre/infolettre_19/index.html#parquet",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Parquet",
    "text": "Parquet\n\nLe site Hyperparam permet d’afficher très rapidement des données Parquet volumineuses sur son explorateur web très rapidement (en moins de 500ms). Pour la tuyauterie, tout est expliqué sur ce blog."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#kubernetes",
    "href": "infolettre/infolettre_19/index.html#kubernetes",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Kubernetes",
    "text": "Kubernetes\n\nComment détecter facilement des pods Kubernetes peu actifs et les désactiver? Un début de processus a été publié sur Devops.dev."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#nouveautés",
    "href": "infolettre/infolettre_19/index.html#nouveautés",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Nouveautés",
    "text": "Nouveautés\n\nUne nouvelle version des notebooks Observable est disponible en pré-production, avec un kit open source pour générer des notebooks et des sites statiques et une application pour Mac pour éditer ses notebooks en local, intégrant de manière plus fluide les apports de l’IA. Plus de détails par ici et une galerie d’exemples de sites.\nL’université allemande de Hanovre a publié une base d’embedding des entités d’Openstreetmap directement utilisable pour entraîner des modèles de machine learning.\nSelon une étude de Posit, le meilleur modèle d’IA pour aider à coder en Python serait ceux d’OpenAI (o3-mini, o4-mini) ou d’Anthropic (Claude Sonnet 4)."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#fun",
    "href": "infolettre/infolettre_19/index.html#fun",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Fun",
    "text": "Fun\n\nVous vous êtes déjà demandé comment résoudre un SUTOM avec les dépendances de Python ? Non ? Et bien quelqu’un a trouvé le moyen de résoudre des Sudoku et des Motus grâce à cela ! Tout est expliqué ici\nAvez-vous déjà vu une intelligence artificielle jouer au Loup-Garou ? Des étudiants de l’ENSAE se sont amusés ont étudié quelles IA étaient meilleures au jeu du Loup-Garou. Ce jeu nécessite en effet de mentir, de convaincre, et d’adapter sa stratégie pour survivre (pour les villageois) ou tuer tous les villageois (pour les loup-garous). A la fin, GPT-5 gagne dans 97 % des 60 matchs joués, contre 15% pour GPT-OSS-120b.\n\n\n\n\n\n\n\nAstuce\n\n\n\nVous voyez d’autres sujets d’actualité intéressants ? N’hésitez pas à les partager sur le groupe Tchap 💬 directement !"
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#première-interview-avec-nicolas-qui-travaille-à-linsee-ssplab",
    "href": "infolettre/infolettre_19/index.html#première-interview-avec-nicolas-qui-travaille-à-linsee-ssplab",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "Première interview avec Nicolas, qui travaille à l’Insee (SSPLab)",
    "text": "Première interview avec Nicolas, qui travaille à l’Insee (SSPLab)\n\n\n\nPeux-tu te présenter?\nDe formation ingénieur, j’ai travaillé huit ans dans l’administration publique avec un parcours que certains ont dit plutôt atypique (ce n’est pas totalement mon avis 🙃). J’ai notamment travaillé au sein de la DG Trésor et à la Commission européenne, sur des sujets de prévision de finances publiques, de négociations européennes et de suivi de la Banque centrale européenne. Le traitement de la donnée n’a pas été jusque-là au centre de mes postes mais l’importance des outils et de traitement plus robuste aurait facilité la vie à certains moments. J’arrive donc à l’Insee pour la première fois mais je suis content de “découvrir la maison”.  A l’Insee, je travaille au sein du SSPLab, le laboratoire de l’innovation en data sciences de l’Insee. L’équipe est chargée de faire de la veille et d’épauler les équipes métiers dans leurs projets. J’ai tout particulièrement l’honneur de succéder à Lino au poste d’animateur du SSPHub, big up à lui pour tout ce qu’il a fait ces trois dernières années !\n\n\nPeux-tu donner un conseil que tu aurais aimé recevoir en lien avec la data ?\nQuestion difficile, étant donné que je n’ai pas mené beaucoup de projets data jusqu’ici. Mais j’ai remarqué que l’ordinateur avait rarement tort, ce qui est assez frustrant, et qu’être persévérant était nécessaire. Avoir un code propre aussi, quand on n’est même pas capable de comprendre ce qu’on a codé soi-même en rentrant de vacances, cela donne une leçon pour la suite. J’aurai sûrement plus d’exemples d’ici un an ou deux 😉\n\n\nAs-tu un projet qui a particulièrement marché, et pourquoi a-t-il marché ? A l’inverse, as-tu un projet qui n’a pas marché et pourquoi ?\nJ’avais codé des petits programmes pour m’aider dans mon travail quotidien, sans rapport direct avec la donnée. Ce qui a aidé dans les deux cas c’est que le besoin métier était bien défini et bien compris, puisque j’étais à la fois le métier et le développeur. Cependant, ces programmes faisaient partie d’un shadow IT qui n’a pas dû me survivre bien longtemps. J’ai quand même réussi à pousser un programme qui faisait du publipostage jusqu’à sa mise en production. À ma surprise, c’est la phase de déploiement qui a été bien plus longue que la phase de développement : cela a dû me prendre quelques jours de code contre plusieurs semaines avant le déploiement.\n\n\nDans quel domaine le service public pourrait être aidé par une utilisation plus importante des données ?\nUn usage plus important de la donnée serait fort bénéfique pour la simplification des démarches pour les citoyens, permettre de diminuer le non-recours aux subventions et différents dispositifs publics tout en renforçant la qualité des données échangées entre administrations.\n\n\nLa dernière découverte technologique qui t’a marqué\nJ’avais un peu laissé de côté R Markdown depuis huit ans. En revenant dans le domaine de la data, je découvre à quel point ce milieu a évolué grâce à Posit, Quarto et l’apport de tous ces projets open source, auxquels des gens contribuent souvent naturellement. L’open source, c’est un peu comme les pompiers ou le secours en mer : ce sont des bénévoles qui font tenir et maintiennent des outils extraordinaires. Et plus récemment encore avec ma prise de poste à l’Insee, la découverte d’Onyxia, de duckDB et de Parquet m’ont aussi marqués."
  },
  {
    "objectID": "infolettre/infolettre_19/index.html#linterview-à-laquelle-vous-avez-échappé",
    "href": "infolettre/infolettre_19/index.html#linterview-à-laquelle-vous-avez-échappé",
    "title": "La rentrée 2025: actualités, nouveautés, interview de rentrée",
    "section": "L’interview à laquelle vous avez échappé",
    "text": "L’interview à laquelle vous avez échappé\nQuelle est la plus-value de l’IA par rapport à un travail humain ? J’ai comparé les questions que j’avais trouvées avec des idées produites par une IA. Voici les questions qu’il m’a proposées, que j’ai proposé de ne pas suivre. Remarquez, ces questions sont le résultat de deux ou trois échanges avec l’IA, je n’y ai pas passé beaucoup plus de temps.\n\nVoici une proposition de questions adaptées à des data scientists ou des managers de data scientists travaillant dans l’administration publique :\n\nFormations et influences “Quelle formation ou influence majeure a façonné votre approche des données et de la gestion dans l’administration ?”\nExpériences professionnelles “Quelles ont été les étapes clés de votre carrière dans l’administration qui ont influencé votre façon de gérer ou d’analyser les données ?”\nMotivations et passions “Qu’est-ce qui vous motive le plus dans l’analyse ou la gestion des données au sein de l’administration publique ?”\nRéalisations majeures “Quels sont vos principaux accomplissements en matière de gestion ou d’analyse de données dans votre rôle actuel ?”\nDéveloppement personnel “Quels défis spécifiques aux données avez-vous rencontrés dans le secteur public et comment les avez-vous surmontés ?”\nProjets actuels “Sur quels projets liés aux données travaillez-vous actuellement, et qu’est-ce qui les rend particulièrement intéressants ou innovants ?”\nRegrets ou aspirations “Y a-t-il des projets ou réalisations en lien avec les données que vous auriez aimé accomplir dans votre carrière mais qui ne se sont pas concrétisés ?”\nVision future “Comment voyez-vous l’évolution de l’importance des données dans l’administration publique dans les années à venir, et quel rôle aimeriez-vous y jouer ?”\nConseils aux jeunes générations “Quel conseil donneriez-vous à un jeune professionnel qui souhaite devenir data scientist ou manager de data scientists dans le secteur public ?”\nPhilosophie personnelle “Si vous deviez partager une philosophie ou un credo concernant la gestion ou l’analyse des données dans l’administration, quelle serait-elle ?”\n\nCes questions devraient aider à explorer les nuances spécifiques du travail des data scientists et de leurs managers dans le secteur public. Si vous souhaitez ajuster davantage ou ajouter des thèmes spécifiques, n’hésitez pas à me le faire savoir !"
  },
  {
    "objectID": "infolettre/infolettre_21/index.html",
    "href": "infolettre/infolettre_21/index.html",
    "title": "Qui pour financer l’open source?",
    "section": "",
    "text": "Début décembre, c’est le deuxième moment dur de l’année après juin. En septembre, on se dit qu’on fera tout cela d’ici Noël, large. Et puis on se réveille, c’est déjà début décembre, la to-do-list est loin d’être finie et la préparation des fêtes de fin d’année arrive à grand pas.\nAllez, courage, dernier sprint de 2025 !"
  },
  {
    "objectID": "infolettre/infolettre_21/index.html#la-troisième-journée-du-réseau-a-eu-lieu-le-1er-décembre",
    "href": "infolettre/infolettre_21/index.html#la-troisième-journée-du-réseau-a-eu-lieu-le-1er-décembre",
    "title": "Qui pour financer l’open source?",
    "section": "La troisième journée du réseau a eu lieu le 1er décembre",
    "text": "La troisième journée du réseau a eu lieu le 1er décembre\nLe 1er décembre 2025, le réseau a organisé sa troisième journée annuelle. Avec quatre présentations, deux interventions extérieures et un atelier de partage, cette édition a réuni une soixantaine de participants en présentiel et distanciel pour des échanges riches et constructifs. Merci à tous les participants pour leur participation active !\n\nLes présentations\n\nOffre LLM du SSPCloud : L’Insee (DIIT) a présenté les nouvelles fonctionnalités basées sur les modèles de langage (LLM) disponibles sur le SSPCloud, intégrant de manière plus poussée des fonctionnalités de complétion de code et d’analyses de données.\nExtraction des compétences dans JOCAS : La Dares et l’Insee (DEE) ont partagé une version test de leur projet d’extraction des compétences numériques dans les offres d’emploi, combinant reconnaissance d’entités nommées et classification par LLM. Ce projet vise à améliorer l’analyse des métiers et des parcours professionnels.\nAutomatisation des infos rapides justice : Le SSER (SSM Justice) a présenté son package R chartegraphique.sser, conçu pour automatiser la production des infos rapides justice. Les détails techniques sont disponibles sur le site des Journées de Méthodologie Statistique (JMS).\nPackage de classification textuelle : L’Insee (SSPLab) a présenté torchTextClassifiers, un package Python de classification textuelle, étendant fastText et reposant sur PyTorch. Ce package permet d’entraîner des modèles maisons à taille réduite en gardant le contrôle de leur architecture.\n\n\n\nAtelier collaboratif\nUn atelier d’échange entre les participants a permis de partager nos pratiques quotidiennes d’utilisation des outils d’IA pour les data scientists et statisticiens :\n\nQuels sont nos cas d’usage?\nQuels outils privilégier, et quels sont leurs avantages et limites ?\n\nNos échanges, riches et nombreux, ont permis de partager des retours d’expérience concrets et nos bonnes (et moins bonnes) pratiques.\n\n\nInvités\n\nLa Dinum a présenté les dernières évolutions de data.gouv.fr, dont data.pass.\nL’INA a présenté data.ina, un portail pour construire des indicateurs de suivi des médias.\n\nLes présentations et le replay de la journée sont disponibles sur la page de l’événement."
  },
  {
    "objectID": "infolettre/infolettre_21/index.html#prochain-événement-présentation-de-cartographia---13-janvier-2026---format-mixte-montrouge-et-en-ligne",
    "href": "infolettre/infolettre_21/index.html#prochain-événement-présentation-de-cartographia---13-janvier-2026---format-mixte-montrouge-et-en-ligne",
    "title": "Qui pour financer l’open source?",
    "section": "Prochain événement : présentation de Cartographia - 📅 13 janvier 2026 - format mixte (Montrouge et en ligne)",
    "text": "Prochain événement : présentation de Cartographia - 📅 13 janvier 2026 - format mixte (Montrouge et en ligne)\nLe prochain événement du réseau sera le 13 janvier 2026. Françoise Bahoken et Nicolas Lambert viendront nous parler de leur livre Cartographia et des questions de cartographie passionnantes qu’ils y abordent.\nNicolas Lambert était déjà intervenu pour présenter Observable, une librairie JavaScript très pratique pour faire des dataviz."
  },
  {
    "objectID": "infolettre/infolettre_21/index.html#résilience-et-open-source",
    "href": "infolettre/infolettre_21/index.html#résilience-et-open-source",
    "title": "Qui pour financer l’open source?",
    "section": "Résilience et open-source",
    "text": "Résilience et open-source\n\nLe monde numérique est très interdépendant\n\nDe récents incidents ont rappelé que notre monde numérique est très interdépendant de solutions parfois lointaines. Un bug dans un logiciel ou service critique, open-source ou payant, se répercute ainsi rapidement à échelle mondiale. Cloudflare a par exemple connu une panne le 18 novembre 20251, mettant KO de nombreux sites, y compris downdetector qui signale les pannes. La panne était due à une mise en production (ratée du coup). De la même manière, une panne de DNS chez Amazon Web Services le 20 octobre 2025 a perturbé de nombreuses applications dans le monde.\n\n\n\n\n\n\n\nEn 2020, par XKCD\n\n\n\n\n\n\n\nEn 2025, par Timothy A.\n\n\n\n\n\n\nLa dépendance numérique en images\n\n\n\n\n\nL’open source dépend du travail gratuit d’inconnus\n\nAu-delà de la simple interdépendance à des logiciels payants, le code open-source est souvent maintenu bénévolement par des inconnus, comme les secours en mer ou les pompiers volontaires.\n\nUn débat est ainsi apparu après que FFmpeg, un framework open-source vidéo largement utilisé (notamment par Chrome, Firefox ou YouTube), s’est retrouvé submergé de demande de correction de bugs, trouvés par l’IA de Google. Or dans l’open source, les bugs sont réparés par des mainteneurs, le plus souvent bénévoles, et qui ne peuvent plus suivre le rythme. Certaines personnes appellent ainsi Google, et plus largement les entreprises qui bénéficient de l’open-source et génèrent des revenus supérieurs aux PIB de certains pays du monde, à financer directement la maintenance des logiciels open-source qu’ils utilisent même si ce n’est pas qu’une question de financement.\n\nDes sous, des sous, des sous, oui mais combien ? On parle étonnamment de sommes plutôt faibles : à titre de comparaison, la fondation qui gère Python a un budget annuel de 5 millions de dollars. On l’apprend notamment dans ce billet de blog où la fondation explique pourquoi elle a refusé un financement de 1,5 million de dollars du gouvernement américain après l’avoir demandé (si vous n’avez pas le temps: c’est parce que le financement venait avec l’engagement de ne pas faire de promotion sur les thèmes de la diversité, de l’équité et de l’inclusion).\n\n\n\nDes alternatives existent\n\nBlois : La ville a choisi de prendre la fin des mises à jour de Windows 10 comme une opportunité et de basculer vers PrimTux, une distribution Linux éducative.\nCour internationale de justice (ICC) : En 2025, la Cour internationale de justice (qui dépend de l’ONU) et 9 de ses magistrats ont été ciblés par des sanctions américaines. Cela serait en soit une histoire en termes de souveraineté, mais vous avez déjà plus d’info en bas de page 2. Le président de la Cour a ensuite perdu l’accès à ses mails. Les versions divergent ensuite : Microsoft a-t-il volontairement coupé l’accès du président à ses mails avant de le rétablir ou cela était-il juste un incident? Toujours est-il que la Cour internationale de justice a annoncé en octobre 2025 son intention de basculer vers des solutions européennes3, comme rapporté par le Handelsblatt (auf Deutsch 🇩🇪)."
  },
  {
    "objectID": "infolettre/infolettre_21/index.html#ia-ia-ia",
    "href": "infolettre/infolettre_21/index.html#ia-ia-ia",
    "title": "Qui pour financer l’open source?",
    "section": "IA, IA, IA",
    "text": "IA, IA, IA\n\nLes modèles de langage seraient inversibles\nUne étude récente (Nikolaou et al., 2025) montre que les modèles de language sont injectifs4 : chaque entrée est mappée à une représentation interne unique. Le papier propose par ailleurs un algorithme, SipIt, capable de reconstruire le prompt original avec 100% de réussite et rapidement."
  },
  {
    "objectID": "infolettre/infolettre_21/index.html#very-big-data-isnt-dead",
    "href": "infolettre/infolettre_21/index.html#very-big-data-isnt-dead",
    "title": "Qui pour financer l’open source?",
    "section": "Very big data isn’t dead",
    "text": "Very big data isn’t dead\n\nDu mal à faire tourner des tables de 2 Go ? Imaginez le CERN, où le laboratoire du LHCb génère 25 millions de collisions de protons par seconde, soit la paille de 4 To de données par seconde. Comment faire ? Comme expliqué dans ce post, ils ont mis en place l’infrastructure pour filtrer les données et ne garder que 10 Go de données générées par seconde. C’est un peu la citation de Einstein :\n\n\nDo not worry about your difficulties in mathematics; I can assure you that mine are still greater.\n\n\n\n\nFiltering data in real time, LHCb"
  },
  {
    "objectID": "infolettre/infolettre_21/index.html#ressources-et-fun",
    "href": "infolettre/infolettre_21/index.html#ressources-et-fun",
    "title": "Qui pour financer l’open source?",
    "section": "Ressources et fun",
    "text": "Ressources et fun\n\nNouveaux outils\n\nR : Jarl, un nouveau linter pour R, conçu pour être simple et efficace.\nMurmure : Un outil d’IA open-source pour générer des résumés et des analyses de texte hors ligne et en local, développé par Al1X-AI.\n\n\n\nRessources et formation\n\nIA : Gender Bias in Large Language Models explique de manière très pédagogique le fonctionnement des LLM\nPython : Python is Not a Great Language for Data Science : comment démarrer en Python, et une comparaison (subjective) entre Python et R\nDocker : Voici un tuto pour (un peu) démystifier les conteneurs et en construire un avec Python.\n\n\n\nFun\n\nAdvent of Code : Un calendrier de l’avent pour développeurs (au niveau certain).\nGenZ vs Boomers : Si vous ne savez pas si vous êtes côté boomer ou genZ, allez faire un tour sur genzplyr et boomerplyr : vous comprendrez vite qu’il y a l’un des deux packages que vous ne comprenez pas. Je sais de quel côté je suis 👴."
  },
  {
    "objectID": "infolettre/infolettre_21/index.html#footnotes",
    "href": "infolettre/infolettre_21/index.html#footnotes",
    "title": "Qui pour financer l’open source?",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nIls ont même eu la bonne idée d’avoir une deuxième panne, plus rapidement réglée, le jour de la rédaction de cette infolettre 🙃.↩︎\nNicolas Guillou, juge français de la CPI qui faisait partie des juges ayant validé les mandats d’arrêt contre le premier ministre israélien et son ministre de la défense, explique dans cet article du Monde et dans son discours au congrès annuel de l’Union Syndicale des Magistrats (USM), ici, ce que signifie concrètement vivre sous sanction américaine. Plus de carte Visa ou Mastercard, vos comptes clients dans des entreprises comme Amazon ou Airbnb sont fermés, des banques, même non implantées aux États-Unis, ferment votre compte bancaire par sur-application des règles (over-compliance). Vos proches sont aussi touchés : interdiction de séjourner aux États-Unis, expulsion s’ils y sont et, légalement parlant, vos proches de nationalité américaine ne peuvent plus vous fournir de service sinon ils pourraient être poursuivis pénalement aux États-Unis. Or, beaucoup de fonctionnaires français ont des enfants américains : il suffit que leurs enfants soient nés aux États-Unis quand les parents travaillaient à l’ambassade de France ou dans les institutions internationales et, grâce au droit du sol, les enfants sont aussi de nationalité américaine.↩︎\nL’indépendance complète de ces solutions est par ailleurs sujette à débat.↩︎\nvieux rappel de maths : injectif veut dire à peu près que si deux objects transformés sont les mêmes, c’est que les objets avant transformation sont les mêmes.↩︎"
  },
  {
    "objectID": "infolettre/infolettre_old/index.html",
    "href": "infolettre/infolettre_old/index.html",
    "title": "Archive des infolettres et lettres Big Data",
    "section": "",
    "text": "Vous désirez intégrer la liste de diffusion ? Un mail à contact-ssphub@insee.fr suffit"
  },
  {
    "objectID": "infolettre/infolettre_old/index.html#les-anciennes-infolettres",
    "href": "infolettre/infolettre_old/index.html#les-anciennes-infolettres",
    "title": "Archive des infolettres et lettres Big Data",
    "section": "Les anciennes infolettres",
    "text": "Les anciennes infolettres\nLes infolettres avant 2022, dont le format était différent, sont archivées ici.\n\nInfolettre n°7, décembre 2022\nInfolettre n°6, novembre 2022\nInfolettre n°5, juin 2022\nInfolettre n°4, mai 2022\nInfolettre n°3, avril 2022\nInfolettre n°2, avril 2022\nInfolettre n°1, avril 2022"
  },
  {
    "objectID": "infolettre/infolettre_old/index.html#lettres-big-data",
    "href": "infolettre/infolettre_old/index.html#lettres-big-data",
    "title": "Archive des infolettres et lettres Big Data",
    "section": "Lettres big data",
    "text": "Lettres big data\nEt même encore avant avant les infolettres 👵👴, les lettres Big Data sont archivées ici.\n\nLettre Big Data n°12, mai 2022\nLettre Big Data n°11, octobre 2021\nLettre Big Data n°10, décembre 2020\nLettre Big Data n°9, décembre 2019\nLettre Big Data n°8, mai 2019\nLettre Big Data n°7, avril 2018\nLettre Big Data n°6, novembre 2017\nLettre Big Data n°5, mars 2017\nLettre Big Data n°4, septembre 2016\nLettre Big Data n°3, juin 2016\nLettre Big Data n°2, mars 2016\nLettre Big Data n°1, décembre 2015"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projets innovants du SSP",
    "section": "",
    "text": "Vous pouvez trouver ici des exemples de projets innovants portés par des membres du SSP.\n\nQu’est-ce qu’un projet innovant ?\nIl est toujours délicat de définir ex-ante ce qu’est un projet innovant. L’innovation technologique est par définition mouvante et évolue très vite. Cependant, comme rappelé dans le manifeste, les innovations technologiques récentes visent à simplifier et accélérer certains processus de production, à faciliter l’exploitation de sources de données non traditionnelles ou volumineuses, à automatiser certaines tâches, à communiquer auprès de publics plus larges avec des visualisations réactives ou encore, entre autres, à réduire le hiatus entre statisticiens et informaticiens.\nPar exemple, la modernisation d’une chaîne de traitement par l’utilisation de nouveaux packages ou de nouvelles méthodes de traitement d’information est une innovation. Elle peut ne pas être assez ambitieuse à elle-seule cependant pour définir un véritable projet innovant, et donc ne figurera pas forcément dans les projets présentés ici. L’usage de webscraping pour constituer une base de données, alimentée de manière automatique, sera considéré comme un projet innovant.\nA l’inverse, la simple mise à jour de code, ou l’utilisation de nouvelles bases administratives, si elle ne contient pas d’obstacles technologiques particuliers, n’est pas considéré en soit comme de l’innovation technologique. Cela ne veut pas dire que ce projet n’est pas nécessaire et bienvenu 😉.\nPar ailleurs, un projet innovant à la date et peut ne plus l’être quelques mois ou années plus tard, quand l’innovation s’est assez répandue pour être considéré comme un savoir classique.\nL’innovation se produit par ailleurs partout et n’est pas réservé aux divers laboratoires en innovation en data-science.\n\n\nQuel est le champ des projets présentés ici ?\nLa liste des projets présentés ici ne vise pas à être exhaustive. Elle est basée sur le volontariat. L’objectif est d’offrir un point central de partage entre data-scientist du SSP, comme rappelé dans le manifeste. Toute proposition d’ajout par fusion sur notre est la bienvenue !\n\n\nListe des projets\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées CB\n\nInsee\n\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nscanR, une application pour observer le paysage de la recherche et de l’innovation en France\n\n\n\nAPI\n\nen production\n\nopen-data\n\nSIES\n\ndatavisualisation\n\nElasticSearch\n\n\n\nAgrégation et mise à disposition de données massives sur la recherche et l’innovation en France par des visualisations, des moteurs de recherche ElasticSearch et des API\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisations des données liées aux tests du SARS-Cov2\n\n\n\nR\n\ndatavisualisation\n\nexpérimentation arrêtée\n\nDREES\n\nopen-data\n\n\n\nPublication hebdomadaire des données liées aux tests de détection du SARS-Cov2 par Shiny à partir du système d’information SI-DEP\n\n\n\n\n\n\n1 juin 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoremifasol\n\n\n\npackage\n\nR\n\nen production\n\nInsee\n\n\n\nLe package  R Doremifasol facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur…\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\npynsee, un package Python  pour récupérer les données de l’Insee\n\n\n\nen production\n\nInsee\n\npackage\n\nPython\n\n\n\nLe package  pynsee facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur Github.\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\n\nPython\n\npackage\n\ndeep learning\n\nimages satellites\n\nen production\n\n\n\nUtiliser les images satellites pour améliorer le recensement de la population dans les territoire ultra-marins\n\n\n\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\n\nwebscraping\n\nen production\n\ndatavisualisation\n\nSIES\n\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\ncodification automatique\n\nextraction de données\n\ndonnées de caisse\n\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\n\nwebscraping\n\nen production\n\ncodification automatique\n\nDARES\n\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\n\nmachine learning\n\nInsee\n\nCOICOP\n\ndonnées de caisse\n\nappariement\n\nexpérimentation arrêtée\n\ncodification automatique\n\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\n\nwebscraping\n\ndatavisualisation\n\nopen-data\n\nen production\n\nSIES\n\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\n\nPython\n\ncodification automatique\n\npackage\n\nen production\n\nMLFlow\n\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nDétecter la cybercriminalité dans les procédures\n\n\n\nen production ??\n\nSSMSI\n\ndeep learning\n\n\n\nDétection des infractions relevant de la cyberdélinquance à partir d’une analyse textuelle des manières d’opérer\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nModélisation de l’appartenance au parc des véhicules routiers et de son utilisation\n\n\n\nen production\n\nappariement\n\nSDES\n\ndonnées administratives\n\n\n\nAppariement de bases administratives et modélisation pour estimer le nombre de véhicules routiers en France\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne\n\n\n\nen production\n\nwebscraping\n\nIPC\n\nInsee\n\n\n\nExploration des apports du webscraping pour mesurer le prix des nuitées hôtelières dans l’IPC\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\n\nInsee\n\nprévisions\n\ndonnées comptes bancaires\n\nexpérimentation\n\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrévoir la croissance en lisant le journal\n\n\n\nInsee\n\nmachine learning\n\nexpérimentation\n\nprévisions\n\nwebscraping\n\n\n\nUtiliser les articles de presse en continu pour construire un indicateur aidant à prévoir la croissance\n\n\n\n\n\n\n1 mars 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparaison des méthodes d’appariement et apport du machine learning\n\n\n\nappariement\n\ndonnées administratives\n\nen production\n\n\n\nTester et comparer différentes méthodes d’appariements afin de dégager des recommandations pour les travaux nécessaires à la construction des répertoires, notamment dans le…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtraction automatique du tableau des filiales et participations des comptes sociaux des entreprises\n\n\n\nPython\n\nextraction de données\n\nAPI\n\nmachine learning\n\nen production\n\n\n\nExtraire les informations de tableaux de comptes sociaux, en particulier des tableaux des filiales et participations, contenus dans des images scannées mises à disposition…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\n\nPython\n\ncodification automatique\n\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\n\nprévisions\n\nexpérimentation\n\nInsee\n\ndonnées CB\n\ndonnées de téléphonie mobile\n\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\n\nInsee\n\nprévisions\n\ndonnées privées\n\nexpérimentation\n\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\n\ndatavisualisation\n\nmachine learning\n\nInsee\n\nexpérimentation\n\nopen-data\n\ndonnées de téléphonie mobile\n\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation\n\n\n\nen production ??\n\nInsee\n\nIPC\n\nwebscraping\n\nforêt aléatoire\n\n\n\nCollecter sur le web les caractéristiques des produits pour améliorer la prise en compte des effets qualité dans l’indice des prix à la consommation\n\n\n\n\n\n\n1 juin 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\nPython\n\ncodification automatique\n\ndonnées de caisse\n\nCOICOP\n\nIPC\n\nen production\n\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n« GDP Tracker » : un outil pour des prévisions économiques en continu\n\n\n\nmachine learning\n\nexpérimentation\n\nprévisions\n\nInsee\n\n\n\nModèles de machine learning pour effectuer des prévisions en temps réel (nowcasting) pour alimenter les analyses conjoncturelles de l’Insee\n\n\n\n\n\n\n1 déc. 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\n\nen production\n\nInsee\n\ncodification automatique\n\nmachine learning\n\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nDétecter et traiter les valeurs aberrantes ou manquantes, application à la Déclaration Sociale Nominative\n\n\n\ndonnées administratives\n\nInsee\n\nmachine learning\n\nen production ??\n\ndata editing\n\n\n\nUtilisation des méthodes de machine learning pour la détection et le traitement des valeurs aberrantes ou manquantes, application à la Déclaration Sociale Nominative\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\nInsee\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées administratives\n\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2018_segregation/index.html",
    "href": "project/2018_segregation/index.html",
    "title": "Ségrégation urbaine : un éclairage par les données de téléphonie mobile",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\n\nDétail du projet\nEn couplant des données de téléphonie mobile et des données administratives fiscales, cette étude analyse la ségrégation dans les principaux pôles urbains français. Des indices de ségrégation sociale et spatiale sont construits en utilisant la fréquence des interactions entre chaque clients de l’opérateur. Ces indicateurs synthétiques permettent de dresser un portrait de la ségrégation sociale, de son niveau comme de son évolution dans le temps – à l’horizon de la semaine comme de l’année civile. Les indices permettent également de mesurer la propension à communiquer plus fortement au sein d’un même groupe social, défini par un critère de revenu, ou au contraire une interaction forte entre plusieurs groupes sociaux. En particulier, cela permet de mesurer le contraste dans le comportement des groupes sociaux extrêmes. Le profil de ségrégation que les données de téléphonie mobile permettent de dégager est comparé à la littérature existante sur le sujet. Une attention particulière est portée aux quartiers « politique de la ville » où la question de la ségrégation a été particulièrement documentée.\n\n\nActeurs\nInsee\n\n\nProduits et documentation du projet\n- Ségrégation urbaine : un éclairage par les données de téléphonie mobile, Journées de méthodologie statistique 2018\n\n\n\n\n\nProjets similaires utilisant des données privées\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2019_gdp_tracker/index.html",
    "href": "project/2019_gdp_tracker/index.html",
    "title": "« GDP Tracker » : un outil pour des prévisions économiques en continu",
    "section": "",
    "text": "Prévisions du PIB en continu\n\n\n\n\nDétail du projet\nLe projet GDP Tracker consiste à construire un outil de prévision économique en continu, alimenté par l’ensemble des indicateurs conjoncturels récents disponibles à l’instant t. L’objectif de ce projet est d’exploiter, au mieux, les sources les plus récentes pour estimer la dynamique des agrégats macroéconomiques majeurs au cours des dernières semaines. Ces sources sont de nature diverse: enquêtes de conjoncture, comptes nationaux trimestriels, remontées statistiques en dur mais aussi de données issues de sources moins traditionnelles (recherches internet, etc.)   À partir de cet ensemble hétérogènes d’informations, l’outil effectue une prévision de croissance des agrégats macroéconomiques les plus scrutés pour connaître la conjoncture française (PIB, consommation des ménages, investissement…). Cette prévision est faite pour le trimestre contemporain ou les trimestres suivants, à partir de modèles de machine learning (LASSO, forêts aléatoires…).   L’outil a été initialement développé pour une note de conjoncture en 2019. Celui-ci a été repris suite à la crise sanitaire pour une utilisation plus systématique. D’abord centré sur la France et sur le seul PIB du trimestre contemporain, il a été décliné à d’autres pays (Allemagne, Italie, Espagne) et est en cours d’élargissement à des agrégats autres que le PIB et à des horizons de prévision plus lointains.\n\n\nActeurs\nInsee\n\n\nProduits et documentation du projet\n- Dossier Prévoir en continu la croissance française1, Note de conjoncture de l’Insee - décembre 2019"
  },
  {
    "objectID": "project/2019_gdp_tracker/index.html#footnotes",
    "href": "project/2019_gdp_tracker/index.html#footnotes",
    "title": "« GDP Tracker » : un outil pour des prévisions économiques en continu",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nSur le nowcasting, une expérimentation a par ailleurs eu lieu en 2022 pour l’imputation de variables non publiées et la prévision de croissance, cf présentation Nowcasting PIB : imputation de variables non encore publiées aux journées de méthodologie statistique de 2022.↩︎"
  },
  {
    "objectID": "project/2020_donnees_caisse/index.html",
    "href": "project/2020_donnees_caisse/index.html",
    "title": "Classification des données de caisse à partir de machine learning",
    "section": "",
    "text": "Classification des données de caisse à partir de machine learning\n\n\n\n\nDétail du projet\nLes données de caisse sont utilisées à l’Insee pour le calcul de l’IPC depuis 2010. Les données de caisses donnent en effet pour chaque code-barres, chaque jour et chaque point de ventes les quantités vendues ainsi que le chiffre d’affaires et/ou le prix auquel le produit est vendu. Pour exploiter ces données, il est toutefois nécessaire de savoir quel produit se trouve derrière un code-barres. Actuellement, l’IPC se fonde sur un référentiel de codes-barres, acheté à un prestataire et qui donne une information très détaillée et structurée des caractéristiques de ces produits. Cette information est payante et ne couvre pas l’ensemble des produits. L’expérimentation vise à identifier les étapes de traitement textuel des libellés, ainsi que les méthodes de classification ou autres, permettant de coder automatiquement les libellés, sans passer par le référentiel, dans la nomenclature Coicop pour l’IPC et sur les regroupements utilisés pour Emagsa dans le cadre du projet Nosica qui vise à intégrer notamment les données de caisse dans la production des indicateurs d’activité de court-terme. Elle teste aussi leur performance sur des jeux de données tests.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLes données de caisse sont aujourd’hui utilisées en production pour le calcul de l’inflation et pour le calcul d’indicateurs d’activité conjoncturelle.\n\n\nProduits et documentation du projet\n- Utiliser les données de caisses pour le calcul de l’indice des prix à la consommation, courrier des statistiques n°3 de l’Insee, décembre 2019  - Données de caisses et ajustements qualité, documents de travail n°F1704 de l’Insee, août 2017\n\n\nCode du projet\n- https://github.com/InseeFrLab/predicat : API pour classification des libellés de caisse  - https://github.com/InseeFrLab/product-labelling : Application de labellisation des données de caisse"
  },
  {
    "objectID": "project/2020_donnees_caisse/index.html#en-lien-avec-le-webscraping-et-lipc",
    "href": "project/2020_donnees_caisse/index.html#en-lien-avec-le-webscraping-et-lipc",
    "title": "Classification des données de caisse à partir de machine learning",
    "section": "En lien avec le webscraping et l’IPC",
    "text": "En lien avec le webscraping et l’IPC\nLa récolte de données en ligne (webscraping) n’est pas utilisée que dans le cadre de la production de l’inflation. Elle est aussi utilisée dans d’autres domaines et par d’autres entités que l’Insee au sein du service statistique public. L’Insee utilise par ailleurs depuis 2020 les données de caisse dans la définition de l’IPC, comme rappelé dans l’article Utiliser les données de caisses pour le calcul de l’indice des prix à la consommation du courrier des statistiques de 2019.\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne\n\n\nExploration des apports du webscraping pour mesurer le prix des nuitées hôtelières dans l’IPC\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrévoir la croissance en lisant le journal\n\n\nUtiliser les articles de presse en continu pour construire un indicateur aidant à prévoir la croissance\n\n\n\n\n\n\n1 mars 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation\n\n\nCollecter sur le web les caractéristiques des produits pour améliorer la prise en compte des effets qualité dans l’indice des prix à la consommation\n\n\n\n\n\n\n1 juin 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2020_donnees_caisse/index.html#en-lien-avec-lusage-de-nouvelles-sources-de-données",
    "href": "project/2020_donnees_caisse/index.html#en-lien-avec-lusage-de-nouvelles-sources-de-données",
    "title": "Classification des données de caisse à partir de machine learning",
    "section": "En lien avec l’usage de nouvelles sources de données",
    "text": "En lien avec l’usage de nouvelles sources de données\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2020_donnees_caisse/index.html#en-lien-avec-les-problématiques-de-classification-automatique",
    "href": "project/2020_donnees_caisse/index.html#en-lien-avec-les-problématiques-de-classification-automatique",
    "title": "Classification des données de caisse à partir de machine learning",
    "section": "En lien avec les problématiques de classification automatique",
    "text": "En lien avec les problématiques de classification automatique\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2020_mvtpop/index.html",
    "href": "project/2020_mvtpop/index.html",
    "title": "Mouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\n\n\nDétail du projet\nDans le cadre de ses missions d’analyse et de production de statistiques, l’Insee a estimé, à partir de nouvelles données privées d’opérateurs de téléphonie mobile, les mouvements de population internes en France pendant le premier confinement de mars 2020. Les données expérimentales ont été publiées en 2021.  Profitant des quelques jours entre l’annonce du confinement et son entrée en vigueur, la population a parfois pu choisir où passer cette période. En se basant sur les données privées de téléphonie mobile, sur lesquelles l’Insee travaillait déjà, l’Insee a ainsi estimé qu’environ 1,5 million de métropolitains avait rejoint leur département de résidence et que Paris avait perdu environ 450 000 habitants métropolitains par rapport aux premiers mois de 2020, dont la moitié étaient des résidents habituels de Paris. Ces excédents de population étaient largement répartis sur l’ensemble du territoire français. De manière générale (hormis Paris), la population était plus proche de son lieu de résidence habituel pendant le confinement : les motifs de nuitées dans d’autres régions, tels que le tourisme, les visites familiales ou les voyages d’affaires, étaient fortement restreints.  Un an plus tard, un ensemble de données plus riche a été publié par l’Insee avec la même méthode afin de visualiser les nuitées agrégées par lieu de résidence, en comparant la période avant et après le confinement. Une datavisualisation a par ailleurs été partagée.\n\n\nActeurs\nInsee\n\n\nProduits et documentation du projet\n- Population présente sur le territoire avant et après le début du confinement – Premiers résultats, avril 2020  - Population présente sur le territoire avant et après le début du confinement : résultats consolidés, mai 2020  - Retour partiel des mouvements de population avec le déconfinement, Insee Analyses n°54, juillet 2020  - La mixité sociale est plus forte en journée sur les lieux d’activité que pendant la nuit dans les quartiers de résidence, Insee Analyses n°59, novembre 2020  - Déplacements de population lors du confinement au printemps 2020, données expérimentales publiées, avril 2021  - Datavisualisation publiée sur les mouvements de population\n\n\nCode du projet\n- Le code est disponible sur GitHub  InseeFrLab/lockdown-maps-R\n\n\n\n\n\nProjets similaires liés aux nouvelles sources de données\n\n\n\n\n\n\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\nExploitation d’une expérience naturelle, la fermeture des frontières en 2020, pour mesurer la part d’achats transfrontaliers de tabac\n\n\n\n\n\n\n1 janv. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2021_Appariement/index.html",
    "href": "project/2021_Appariement/index.html",
    "title": "Comparaison des méthodes d’appariement et apport du machine learning",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nComparaison des méthodes d’appariement et apport du machine learning\n\n\n\n\nDétail du projet\nLe programme Resil vise à construire un système de répertoires d’individus, de ménages et de locaux d’habitation, durable et évolutif, mis à jour à partir de sources administratives diverses. Il nécessite l’agrégation de plusieurs sources de données sans identifiant direct commun.  Le but de l’expérimentation est de tester et de comparer différentes méthodes d’appariements afin de dégager des recommandations pour les travaux nécessaires à la construction des répertoires. Celles-ci seront fondées sur des critères de performance (qualité de l’appariement) mais aussi sur des considérations opérationnelles (facilité de déploiement, temps de calcul, etc.). L’objectif est notamment d’évaluer l’apport et les contraintes des méthodes probabilistes ainsi que du machine learning dans les tâches d’appariement. Ce travail s’accompagnera d’une réflexion sur la normalisation préalable des données et l’évaluation des résultats d’un appariement.\n\n\nActeurs\nInsee\n\n\nProduits et documentation du projet\n- Méthodologie d’appariement de données individuelles, Journées de méthodologie statistique 2022 ;  - Probabilistes ou déterministes, des méthodes d’appariements au banc d’essai du programme RéSIL, Journées de méthodologie statistique 2022 ;  - Impact du nettoyage des données sur la qualité d’un appariement, Journées de méthodologie statistique 2022  - Les appariements : finalités, pratiques et enjeux de qualité, document de travail de l’Insee, juillet 2024"
  },
  {
    "objectID": "project/2021_codif_PCS/index.html",
    "href": "project/2021_codif_PCS/index.html",
    "title": "Codification automatique des professions dans la nomenclature PCS 2020",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\n\n\nDétail du projet\nLa rénovation de la nomenclature des PCS en 2020 s’accompagne de la promotion d’un outil d’autocomplétion des libellés de profession dans une liste de libellés enrichis permettant le codage direct dans la case de la nomenclature ou des regroupements ad hoc complémentaires de la nomenclature. Or, l’outil d’autocomplétion ne sera disponible que pour des collectes informatisées et par ailleurs il comprend la possibilité de répondre “hors-liste”. Pour pouvoir intégrer la nouvelle PCS 2020 dans le recensement de population à la collecte 2024 et être en mesure de coder aussi les bulletins papier tout comme les réponses informatisées “hors-liste”, un algorithme de codification automatique en PCS 2020 de ces bulletins doit être créé. Suite au report de l’enquête de recensement 2021, des gestionnaires ont annoté en PCS 2020, avec double codage et arbitrage, 119 000 bulletins issus de l’EAR 2020. Le but de l’expérimentation est de tester et comparer différents modèles d’apprentissage statistique et méthodes de prétraitement pour le codage des professions dans la nomenclature PCS 2020 à partir du libellé de profession ainsi que des variables annexes utilisées lors de la phase d’annotation (statut de l’employeur, etc.) et d’en retenir le plus performant. L’objectif est de maintenir le taux de codifications correctes ainsi que le taux d’envoi en reprise manuelle à des niveaux similaires à l’existant.\n\n\nActeurs\nInsee\n\n\nProduits et documentation du projet\n- Application de techniques de machine learning pour coder les professions dans la nomenclature des professions et catégories socio-professionnelles 2020, Journées de méthodologie statistique 2022\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2021_detection_cyber/index.html",
    "href": "project/2021_detection_cyber/index.html",
    "title": "Détecter la cybercriminalité dans les procédures",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nDétecter la cybercriminalité dans les procédures\n\n\n\n\nDétail du projet\nLa cybercriminalité est une nouvelle forme de criminalité en plein essor, et la lutte contre celle-ci est un enjeu important pour les services de police et de gendarmerie. Elle s’avère toutefois particulièrement difficile à quantifier. Elle regroupe toutes les infractions pénales tentées ou commises à l’encontre ou principalement au moyen des systèmes d’information et de communication. La nomenclature des infractions ne permet de repérer qu’une partie de ce type de délinquance. Pour obtenir une meilleure mesure statistique, il est nécessaire d’utiliser des informations fournies dans des zones textuelles des procédures. L’enjeu de l’expérimentation est d’explorer le potentiel des méthodes d’analyse textuelle sur ces données pour mesurer l’ampleur de ce type de délinquance.\n\n\nActeurs\nInsee et service statistique ministériel de la sécurité intérieure (SSMSI)\n\n\nRésultats du projet\nLes travaux ont été repris par le SSMSI pour en déduire notamment une nomenclature à deux dimensions des délits relevant du cyber. L’algorithme prédit à plus de 95% les infractions comme relevant de la cybercriminalité au sens de la définition. De même, il prédit moins de 1% d’infractions comme ne relevant pas de la cybercriminalité au sens de notre définition.\n\n\nProduits et documentation du projet\n- Détection des infractions relevant de la cyberdélinquance à partir d’une analyse textuelle des manières d’opérer, Journée de méthodologie statistique, 2022"
  },
  {
    "objectID": "project/2021_vehicules/index.html",
    "href": "project/2021_vehicules/index.html",
    "title": "Modélisation de l’appartenance au parc des véhicules routiers et de son utilisation",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nModélisation de l’appartenance au parc des véhicules routiers et de son utilisation\n\n\n\n\nDétail du projet\nLe SSM du ministère de la Transition écologique (SDES) a entrepris en 2019 la refonte du répertoire statistique des véhicules routiers (RSVero). Principale innovation de ce projet, l’utilisation des données de contrôles techniques permet de s’assurer que les véhicules immatriculés sont toujours en circulation et de déterminer leur utilisation annuelle, grâce au relevé du compteur kilométrique effectué à chaque visite. Cependant, l’intégration de ces données n’est pas sans poser de questions méthodologiques : les visites interviennent à des dates variables, parfois avec du retard, et on ne peut connaître avec certitude le statut des véhicules dont la dernière visite précède la date à laquelle on souhaite déterminer le parc. Le projet permet d’estimer, pour toutes les voitures du répertoire, la probabilité qu’elles soient toujours en circulation à une date donnée et, le cas échéant, la distance annuelle parcourue. Plus largement, ce projet s’inscrit dans le mouvement de développement de méthodologies de traitement des sources administratives.\n\n\nActeurs\nInsee, SSM Transition écologique (SDES)\n\n\nRésultats du projet\nLa méthodologie est aujourd’hui en production et utilisée pour déterminer chaque année les véhicules en circulation, leurs caractéristiques et leur utilisation.\n\n\nProduits et documentation du projet\n- Méthodologie pour l’estimation des parcs de véhicules et des distances parcourues, document de travail, mars 2024  - Modélisation de l’appartenance au parc des véhicules routiers et de son utilisation, Journées de méthodologie statistique 2022\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\n\nmachine learning\n\nInsee\n\nCOICOP\n\ndonnées de caisse\n\nappariement\n\nexpérimentation arrêtée\n\ncodification automatique\n\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparaison des méthodes d’appariement et apport du machine learning\n\n\n\nappariement\n\ndonnées administratives\n\nen production\n\n\n\nTester et comparer différentes méthodes d’appariements afin de dégager des recommandations pour les travaux nécessaires à la construction des répertoires, notamment dans le…\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nDétecter et traiter les valeurs aberrantes ou manquantes, application à la Déclaration Sociale Nominative\n\n\n\ndonnées administratives\n\nInsee\n\nmachine learning\n\nen production ??\n\ndata editing\n\n\n\nUtilisation des méthodes de machine learning pour la détection et le traitement des valeurs aberrantes ou manquantes, application à la Déclaration Sociale Nominative\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\nInsee\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées administratives\n\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_Curiexplore/index.html",
    "href": "project/2022_Curiexplore/index.html",
    "title": "Curiexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\n\n\nDétail du projet\nLe projet CurieXplore, développé par le SIES (service statistique du Ministère de l’Enseignement Supérieur et de la Recherche) propose une visualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.  La plateforme combine du contenu textuel collecté par le SIES auprès du réseau diplomatique français et des données issues de sources statistiques internationales (Unesco, banque mondiale, OCDE, Eurostat, …) ou privées (classements d’Etablissements d’ES comme Shanghai, UMULTIRANK ou THE) moissonnées automatiquement régulièrement (webscraping).\n\n\nActeurs\nService statistique du Ministère de l’Enseignement Supérieur et de la Recherche (SIES)\n\n\nRésultats du projet\nLe projet est en production, avec un site dédié disponible sur curiexplore.enseignementsup-recherche.gouv.fr.\n\n\nCode du projet\n- Le code est disponible sur GitHub  https://github.com/dataesr/curiexplore-ui.\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne\n\n\nExploration des apports du webscraping pour mesurer le prix des nuitées hôtelières dans l’IPC\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrévoir la croissance en lisant le journal\n\n\nUtiliser les articles de presse en continu pour construire un indicateur aidant à prévoir la croissance\n\n\n\n\n\n\n1 mars 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation\n\n\nCollecter sur le web les caractéristiques des produits pour améliorer la prise en compte des effets qualité dans l’indice des prix à la consommation\n\n\n\n\n\n\n1 juin 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_JOCAS/index.html",
    "href": "project/2022_JOCAS/index.html",
    "title": "Jocas, webscraping des offres d’emploi en ligne",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nLes offres d’emploi en ligne, nouvelle source de données sur le marché du travail\n\n\n\n\nDétail du projet\nEn quelques années, Internet est devenu une nouvelle source d’information sur le marché du travail. Selon l’enquête Offre d’emploi et recrutement (Ofer) de la Dares, 95 % des annonces d’offres d’emploi ont fait l’objet d’une publication sur Internet en 2016 contre 53 % en 2005. Forte de ce constat, la Dares a décidé de collecter les offres d’emploi en ligne publiées sur une quinzaine de sites pour en faire une nouvelle base de données sur les offres d’emploi : Jocas (Job offers collection and analysis system). Différents outils sont utilisés pour construire cette nouvelle base de données : webscraping, algorithme de classification automatique de texte, déduplication.  Sur l’année 2019, la base Jocas peut être comparée aux sources usuelles de la statistique publique sur l’offre d’emploi, qu’il s’agisse de sources administratives, comme les offres diffusées par Pôle emploi et les Déclarations préalables à l’embauche (DPAE) des Urssaf, ou bien des données issues d’enquête telles que celle sur les Besoins en main-d’œuvre (BMO) de Pôle emploi, l’enquête Emploi de l’Insee, l’enquête Activité et conditions d’emploi de la main-d’œuvre (Acemo) de la Dares. Il en ressort que les métiers sont inégalement couverts par Jocas. Les domaines professionnels avec une forte proportion de cadres ou effectuant beaucoup de recrutements en ligne ont tendance à être surreprésentés. Au contraire, ceux comptabilisant beaucoup de recrutements multiples ou mobilisant des canaux de recrutement informels sont plutôt sous-représentés.\n\n\nActeurs\nDARES\n\n\nRésultats du projet\nLes données d’offres en ligne ont notamment été intégrées au calcul des tensions sur le marché du travail. Elles ont également été utilisées pour la production du tableau de suivi de la situation du marché du travail en 2020-2021 lors de la crise du Covid-19. les données Jocas sont en accès libre pour les étudiants, les chercheurs et les agents de la fonction publique. L’accès aux données peut également être accordé pour un usage statistique et non commercial, sur demande auprès de la Dares. La base est notamment accessible sur la plateforme SSPCloud de l’Insee, en suivant le chemin d’accès ‘projet-jocas-prod/diffusion/JOCAS’.\n\n\nProduits et documentation du projet\n- Description sur le site de la DARES  - Document de travail  - Hackathon en mars 2023 sur le dédoublement des offres d’emploi  - Actualités du projet  - Formation pour utiliser la base JOCAS\n\n\nCode du projet\n- Le code est disponible sur GitHub  https://github.com/OnlineJobVacanciesESSnetBigData/JobTitleProcessing_FR\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaromètre de la science ouverte\n\n\nPour être en mesure de suivre l’ouverture des publications scientifiques (objectif de la stratégie nationale de science ouverte), le service statistique du Ministère de…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne\n\n\nExploration des apports du webscraping pour mesurer le prix des nuitées hôtelières dans l’IPC\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrévoir la croissance en lisant le journal\n\n\nUtiliser les articles de presse en continu pour construire un indicateur aidant à prévoir la croissance\n\n\n\n\n\n\n1 mars 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique des professions dans la nomenclature PCS 2020\n\n\nCodifier automatiquement les professions dans le cadre de la bascule vers la nouvelle nomenclature PCS (PCS 2020)\n\n\n\n\n\n\n1 janv. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation\n\n\nCollecter sur le web les caractéristiques des produits pour améliorer la prise en compte des effets qualité dans l’indice des prix à la consommation\n\n\n\n\n\n\n1 juin 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité des associations\n\n\nCodification automatique de l’activité des associations à partir de méthodes de machine learning\n\n\n\n\n\n\n1 juin 2019\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_bso/index.html",
    "href": "project/2022_bso/index.html",
    "title": "Baromètre de la science ouverte",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nBaromètre français de la science ouverte\n\n\n\n\nDétail du projet\nLa généralisation de l’accès ouvert aux publications scientifiques est l’un des axes de la stratégie nationale de science ouverte, avec pour objectif un taux d’accès ouvert de 100 % en 2030 pour les articles de revue. En effet, l’accès ouvert facilite, élargit et accélère la diffusion des résultats de la recherche auprès des communautés scientifiques et des acteurs de la société en général : enseignants, étudiants, entreprises, associations, acteurs des politiques publiques, etc.   Le baromètre construit ainsi des indicateurs de la science ouverte pour les publications, données, logiciels et codes afin de mesurer l’atteinte de cet objectif.  Concernant les publications, un maximum de métadonnées sur les publications est collecté. Il est ensuite filtré pour ne garder que les affiliations françaises et puis enrichi pour suivre le statut d’ouverture des publications par discipline identifiée. Les données sont ensuite publiées en open-data. Cette méthode permet ainsi de constituer la base de données des publications françaises la plus exhaustive au monde.  Concernant les données, logiciels et codes, la production scientifique est d’abord répertoriée. Le texte de la publication est ensuite analysé pour détecter la mise à disposition de données, logiciels ou codes en accès ouvert. Des indicateurs de suivi sont enfin produits et publiés.\n\n\nActeurs\nService statistique du Ministère de l’Enseignement Supérieur et de la Recherche (SIES)\n\n\nRésultats du projet\nLe projet est présenté sur le site dédié : https://barometredelascienceouverte.esr.gouv.fr/. Il comprend notamment une méthodologie\n\n\nCode du projet\nLes dossiers de code sont disponibles sur GitHub \n\n\n\n\n\nProjets similaires en lien avec le webscraping\n\n\n\n\n\n\n\n\n\n\nCuriexplore, la plateforme de comparaison des politiques nationales d’enseignement et de recherche\n\n\nVisualisation interactive de l’environnement de l’enseignement et de la recherche dans les différents pays.\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJocas, webscraping des offres d’emploi en ligne\n\n\nLe projet Jocas (Job offers collection and analysis system) permet à la DARES (Service statistique ministériel Travail) de collecter automatique des offres d’emploi en…\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices des prix à la consommation des nuitées hôtelières : l’expérience du webscraping d’une plateforme de réservation en ligne\n\n\nExploration des apports du webscraping pour mesurer le prix des nuitées hôtelières dans l’IPC\n\n\n\n\n\n\n1 juin 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrévoir la croissance en lisant le journal\n\n\nUtiliser les articles de presse en continu pour construire un indicateur aidant à prévoir la croissance\n\n\n\n\n\n\n1 mars 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscrapper les caractéristiques des produits pour améliorer la mesure de l’inflation\n\n\nCollecter sur le web les caractéristiques des produits pour améliorer la prise en compte des effets qualité dans l’indice des prix à la consommation\n\n\n\n\n\n\n1 juin 2020\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2022_codif_ape/cards/cards14.html",
    "href": "project/2022_codif_ape/cards/cards14.html",
    "title": "PyData Paris",
    "section": "",
    "text": "Faciliter l’entraînement mais pas forcément –&gt;"
  },
  {
    "objectID": "project/2022_drees_dataviz/index.html",
    "href": "project/2022_drees_dataviz/index.html",
    "title": "Visualisations des données liées aux tests du SARS-Cov2",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nPublication automatique et data-visualisation des données sur les tests du Covid-19\n\n\n\n\nDétail du projet\nDepuis quelques années, la DREES (service statistique du Ministère de la Santé), met à disposition, dans le cadre de sa stratégie open-data, des visualisations interactives de données sur des thèmes variés (Covid bien-sûr mais aussi effectifs des professionnels de santé, pensions et âges de retraites…). Ces visualisations reposent sur  Shiny et offrent ainsi la liberté à l’utilisateur de jouer avec les données et obtenir une mise à jour de graphiques exploratoires.  Entre octobre 2020 et juin 20233, la DREES a ainsi mis à disposition les données sur le volume et les délais de validation des tests RT-PCR et antigéniques de manière hebdomadaire. Ces résultats sont issus des données du système d’information SI-DEP.\n\n\nActeurs\nDREES\n\n\nRésultats du projet\nLe projet a publié des informations mises à jour automatiquement sur les tests liés au Covid-19 et les délais entre 2020 et mi-2023. Les données sont toujours disponibles sur le site https://drees.shinyapps.io/delais_test_app/ mais ne sont plus mises à jour depuis juin 2023."
  },
  {
    "objectID": "project/2023_doremifasol/index.html",
    "href": "project/2023_doremifasol/index.html",
    "title": "Doremifasol",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nPackage  pour récupérer des données du site de l’Insee\n\n\n\n\nDétail du projet\nLe package doremifasol (Données en R Mises à disposition par l’Insee et Facilement Sollicitables) permet d’importer facilement dans R des données mises à disposition sur le site de l’Insee.   Il offre deux fonctionnalités principales :  - télécharger et importer dans R des fichiers disponibles sur insee.fr (Base Permanente des Équipements, Recensement de Population, Filosofi…), y compris les données mises à disposition via Melodi ;  - requêter l’API Sirene et recupérer les résultats dans R.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLe package DoReMiFaSol est en production et téléchargeable depuis GitHub. La documentation est aussi publiée ici.\n\n\nCode du projet\n- Le code est disponible sur GitHub  https://github.com/InseeFrLab/DoReMIFaSol\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\npynsee, un package Python  pour récupérer les données de l’Insee\n\n\nLe package  pynsee facilite la récupération des données Insee pour les data scientists. La librairie est open source, disponible sur Github.\n\n\n\n\n\n\n1 janv. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation des images satellites pour la statistique publique\n\n\nUtiliser les images satellites pour améliorer le recensement de la population dans les territoire ultra-marins\n\n\n\n\n\n\n1 oct. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodification automatique de l’activité principale des entreprises\n\n\nDévelopper un algorithme de machine learning pour automatiser la classification de l’activité principale des entreprises et mise en production\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "project/2024_cb_mno_tabac/index.html",
    "href": "project/2024_cb_mno_tabac/index.html",
    "title": "Une évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France",
    "section": "",
    "text": "Synthèse du projet\n\n\n\n\nUne évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France\n\n\n\n\nDétail du projet\nLe tabagisme est un problème majeur de santé publique, à l’origine de nombreuses maladies évitables à travers le monde. Au cours des dernières décennies, l’augmentation du prix du tabac s’est imposée comme la principale stratégie des États pour lutter contre le tabagisme. Toutefois, les différences de prix entre certains pays frontaliers sont susceptibles de limiter l’efficacité de cette mesure en permettant à certains consommateurs d’acheter du tabac à un prix inférieur dans un État voisin. Si le problème des achats transfrontaliers n’est pas nouveau, l’ampleur du phénomène reste mal connue et fait encore l’objet de débats réguliers. Cette étude contribue à son évaluation en France en exploitant une expérience naturelle sans précédent : la fermeture des frontières terrestres entre mars 2020 et juin 2020 dans le cadre de la lutte contre la pandémie de Covid-19.\n\n\nActeurs\nInsee\n\n\nRésultats du projet\nLes résultats montrent que la fermeture des frontières a généré un surplus d’achats de tabac de 9,5 % en France métropolitaine, par rapport à la situation contrefactuelle où les frontières seraient restées ouvertes. Il s’agit probablement d’une estimation basse des achats transfrontaliers. En effet, une partie de la consommation de tabac en provenance de l’étranger a pu persister pendant le premier confinement, les frontières n’ayant pas été complètement fermées, notamment aux travailleurs frontaliers. En extrapolant la consommation observée dans le reste du pays aux régions frontalières, à caractéristiques identiques, les recettes générées en France seraient environ 13,5 % plus élevées s’il n’existait pas d’alternatives moins chères à l’étranger.\n\n\nProduits et documentation du projet\n- Une évaluation des achats transfrontaliers de tabac et des pertes fiscales associées en France, Documents de travail de l’Insee n°2024-06, avril 2024\n\n\n\n\n\nProjets similaires\n\n\n\n\n\n\n\n\n\n\nTravaux méthodologiques sur l’enquête Budget de Famille\n\n\n\ncodification automatique\n\nextraction de données\n\ndonnées de caisse\n\n\n\nModernisation de l’enquête budget des familles par utilisation d’outils de classification automatique\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuel effet de la qualité de la nourriture sur notre santé ?\n\n\n\nmachine learning\n\nInsee\n\nCOICOP\n\ndonnées de caisse\n\nappariement\n\nexpérimentation arrêtée\n\ncodification automatique\n\n\n\nEnrichissement de données de caisses à partir de données d’informations nutritionnelles pour analyser l’impact de la consommation sur la santé\n\n\n\n\n\n\n1 janv. 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn 2020, la chute de la consommation a alimenté l’épargne, faisant progresser notamment les hauts patrimoines financiers : quelques résultats de l’exploitation de données bancaires\n\n\n\nInsee\n\nprévisions\n\ndonnées comptes bancaires\n\nexpérimentation\n\n\n\nAnalyse du comportement des ménages pendant la crise sanitaire de 2020 à partir de données de comptes bancaires\n\n\n\n\n\n\n1 avr. 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation de données de cartes bancaires et de téléphonie mobile pour prévoir l’activité économique\n\n\n\nprévisions\n\nexpérimentation\n\nInsee\n\ndonnées CB\n\ndonnées de téléphonie mobile\n\n\n\nLa crise sanitaire de 2020 a nécessité de revoir les processus de prévision pour être plus réactif face aux événements. Dans ce cadre, l’Insee s’est appuyée sur les données…\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nQue disent les données de production et de consommation d’électricité sur l’activité économique en période de confinement ?\n\n\n\nInsee\n\nprévisions\n\ndonnées privées\n\nexpérimentation\n\n\n\nUtilisation des données de production et de consommation d’électricité pour prédire l’activité économique\n\n\n\n\n\n\n1 déc. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouvements de population autour du confinement de mars 2020 grâce aux données de téléphonie mobile\n\n\n\ndatavisualisation\n\nmachine learning\n\nInsee\n\nexpérimentation\n\nopen-data\n\ndonnées de téléphonie mobile\n\n\n\nL’Insee a eu accès à des données de téléphonie mobile dans le cadre du suivi de la crise sanitaire de 2020. Ces données ont permis de produire les statistiques sur les…\n\n\n\n\n\n\n1 nov. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification des données de caisse à partir de machine learning\n\n\n\nPython\n\ncodification automatique\n\ndonnées de caisse\n\nCOICOP\n\nIPC\n\nen production\n\n\n\nClassifier des données de caisse dans la nomenclature COICOP par machine learning pour le calcul de l’IPC\n\n\n\n\n\n\n1 janv. 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSégrégation urbaine : un éclairage par les données de téléphonie mobile\n\n\n\nInsee\n\nexpérimentation\n\ndonnées de téléphonie mobile\n\ndonnées administratives\n\n\n\nCroisement de données administratives et de données de téléphonie pour analyser la ségrégation au niveau local\n\n\n\n\n\n\n1 janv. 2018\n\n\n\n\n\n\nAucun article correspondant"
  }
]